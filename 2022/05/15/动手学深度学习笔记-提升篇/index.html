<!DOCTYPE html>
<html lang="en">

<!-- Head tag -->
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="google-site-verification" content="xBT4GhYoi5qRD5tr338pgPM5OWHHIDR6mNg1a3euekI" />
    <meta name="baidu-site-verification" content="093lY4ziMu" />
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
    <meta name="description" content="A hexo theme">
    <meta name="keyword"  content="BUAADreamer, hexo-theme-snail">
    <link rel="shortcut icon" href="/img/ironman-draw.png">
    <!-- Place this tag in your head or just before your close body tag. -->
    <script async defer src="https://buttons.github.io/buttons.js"></script>
    <!--<link href='http://fonts.googleapis.com/css?family=Montserrat:400,700' rel='stylesheet' type='text/css'>-->
    <title>
        
          动手学深度学习笔记-提升篇 - BUAADreamer&#39;s Blog
        
    </title>

    <link rel="canonical" href="https://BUAADreamer.top/2022/05/15/动手学深度学习笔记-提升篇/">

    <!-- Bootstrap Core CSS -->
    
<link rel="stylesheet" href="/css/bootstrap.min.css">


    <!-- Custom CSS --> 
    
        
<link rel="stylesheet" href="/css/dusign-light.css">

        
<link rel="stylesheet" href="/css/dusign-common-light.css">

        
<link rel="stylesheet" href="/css/font-awesome.css">

        
<link rel="stylesheet" href="/css/toc.css">

        <!-- background effects end -->
    
    
    <!-- Pygments Highlight CSS -->
    
<link rel="stylesheet" href="/css/highlight.css">


    
<link rel="stylesheet" href="/css/widget.css">


    
<link rel="stylesheet" href="/css/rocket.css">


    
<link rel="stylesheet" href="/css/signature.css">


    
<link rel="stylesheet" href="/css/fonts.googleapis.css">


    <link rel="stylesheet" href="//cdn.bootcss.com/font-awesome/4.3.0/css/font-awesome.min.css">

    <!-- photography -->
    
<link rel="stylesheet" href="/css/photography.css">


    <!-- ga & ba script hoook -->
    <script></script>
<meta name="generator" content="Hexo 4.2.1"></head>


<!-- hack iOS CSS :active style -->
<body ontouchstart="">

    <!-- background effects start -->
    
    <!-- background effects end -->

	<!-- Modified by Yu-Hsuan Yen -->
<!-- Post Header -->
<style type="text/css">
    header.intro-header{
        
            
                background-image: linear-gradient(rgba(0, 0, 0, 0.3), rgba(0, 0, 0, 0.3)), url('../../../../img/default.jpg')
                /*post*/
            
        
    }
    
    #signature{
        background-image: url('/img/signature/dusign.png');
    }
    
</style>

<header class="intro-header" >
    <!-- Signature -->
    <div id="signature">
        <div class="container">
            <div class="row">
                <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                
                    <div class="post-heading">
                        <div class="tags">
                            
                              <a class="tag" href="/tags/#DeepLearning" title="DeepLearning">DeepLearning</a>
                            
                        </div>
                        <h1>动手学深度学习笔记-提升篇</h1>
                        <h2 class="subheading"></h2>
                        <span class="meta">
                            Posted by BUAADreamer on
                            2022-05-15
                        </span>

                        
                            <div class="blank_box"></div>
                            <span class="meta">
                                Words <span class="post-count">7.4k</span> and
                                Reading Time <span class="post-count">29</span> Minutes
                            </span>
                            <div class="blank_box"></div>
                            <!-- 不蒜子统计 start -->
                            <span class="meta">
                                Viewed <span id="busuanzi_value_page_pv"><i class="fa fa-spinner fa-spin"></i></span> Times
                            </span>
                            <!-- 不蒜子统计 end -->
                        

                    </div>
                

                </div>
            </div>
        </div>      
    </div>

    
    <div class="waveWrapper">
        <div class="wave wave_before" style="background-image: url('/img/wave-light.png')"></div>
        <div class="wave wave_after" style="background-image: url('/img/wave-light.png')"></div>
    </div>
    
</header>

	
    <!-- Navigation -->
<nav class="navbar navbar-default navbar-custom navbar-fixed-top">
    <div class="container-fluid">
        <!-- Brand and toggle get grouped for better mobile display -->
        <div class="navbar-header page-scroll">
            <button type="button" class="navbar-toggle">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="/">BUAADreamer&#39;s Blog</a>
        </div>

        <!-- Collect the nav links, forms, and other content for toggling -->
        <!-- Known Issue, found by Hux:
            <nav>'s height woule be hold on by its content.
            so, when navbar scale out, the <nav> will cover tags.
            also mask any touch event of tags, unfortunately.
        -->
        <div id="huxblog_navbar">
            <div class="navbar-collapse">
                <ul class="nav navbar-nav navbar-right">
                    <li>
                        <a href="/">Home</a>
                    </li>

                    

                        
                    

                        
                        <li>
                            <a href="/about/">About</a>
                        </li>
                        
                    

                        
                        <li>
                            <a href="/archive/">Archives</a>
                        </li>
                        
                    

                        
                        <li>
                            <a href="/tags/">Tags</a>
                        </li>
                        
                    

                        
                        <li>
                            <a href="/photography/">Photography</a>
                        </li>
                        
                    

                        
                        <li>
                            <a href="/categories/">Categories</a>
                        </li>
                        
                    
                    
                    
                    <li>
                        <a href="https://www.cnblogs.com/BUAADreamer/" target="_blank">Chinese Blog</a>
                    </li>
                    
                </ul>
            </div>
        </div>
        <!-- /.navbar-collapse -->
    </div>
    <!-- /.container -->
</nav>
<script>
    // Drop Bootstarp low-performance Navbar
    // Use customize navbar with high-quality material design animation
    // in high-perf jank-free CSS3 implementation
    var $body   = document.body;
    var $toggle = document.querySelector('.navbar-toggle');
    var $navbar = document.querySelector('#huxblog_navbar');
    var $collapse = document.querySelector('.navbar-collapse');

    $toggle.addEventListener('click', handleMagic)
    function handleMagic(e){
        if ($navbar.className.indexOf('in') > 0) {
        // CLOSE
            $navbar.className = " ";
            // wait until animation end.
            setTimeout(function(){
                // prevent frequently toggle
                if($navbar.className.indexOf('in') < 0) {
                    $collapse.style.height = "0px"
                }
            },400)
        }else{
        // OPEN
            $collapse.style.height = "auto"
            $navbar.className += " in";
        }
    }
</script>


    <!-- Main Content -->
    <!-- Post Content -->
<article>
    <div class="container">
        <div class="row">
            
            <!-- Post Container -->
            <div class="
                col-lg-8 col-lg-offset-2
                col-md-10 col-md-offset-1
                post-container">

                <h1 id="8-循环神经网络（RNN）">8.循环神经网络（RNN）<a class="anchor" href="#8-循环神经网络（RNN）">·</a></h1>
<p>CNN：处理空间信息</p>
<p>RNN-recurrent neural network：处理序列信息</p>
<h2 id="序列模型">序列模型<a class="anchor" href="#序列模型">·</a></h2>
<h3 id="统计工具">统计工具<a class="anchor" href="#统计工具">·</a></h3>
<h4 id="自回归模型">自回归模型<a class="anchor" href="#自回归模型">·</a></h4>
<p>输入数据的数量 $x_{t-1},…,x_1$ 随着 $t$ 而异，而不是不变的</p>
<p>策略1：满足某个长度为 $\tau$ 的时间跨度，即使用观测序列 $x_{t-1},…,x_{t-\tau}$，参数数量不变，可以训练一个网络</p>
<p>策略2：$h_t=g(h_{t-1},x_{t-1})\rightarrow x_t=P(x_t|h_t),$ 更新模型，由于$h_t$没有被观测到，因此称为隐变量自回归模型（latent autoregressive models）</p>
<p>如何生成训练数据？一般是利用历史观测来预测下一个未来观测。常见的一个假设是虽然$x_t$可能会变，序列本身动力学不改变，不变的动力学称为静止的，即整个序列估计值用以下方式获得：$P(x_1,…,x_t)=\prod <em>{t=1}^T P(x_t|x</em>{t-1},…,x_1)$</p>
<h4 id="马尔可夫模型">马尔可夫模型<a class="anchor" href="#马尔可夫模型">·</a></h4>
<p>上文提到的 $x_{t-1},…,x_{t-\tau}$ 来估计 $x_t$ 时如果时近似精确的，则称满足马尔可夫条件，如果$\tau=1$，则可以得到一阶马尔可夫模型</p>
<p>$P(x_1,…,x_T)=\prod_{t=1}^T P(x_t|x_{t-1})$</p>
<p>此时可以得到$P(x_{t+1}|x_{t-1})=\frac {\sum_{x_t} P(x_{t+1},x_t,x_{t-1})} {P(x_{t-1})}=\frac {P(x_{t-1})\sum_{x_t} P(x_{t+1}|x_t)P(x_t|x_{t-1})} {P(x_{t-1})}=\sum_{x_t}P(x_{t+1}|x_t)P(x_t|x_{t-1})$</p>
<h4 id="因果关系">因果关系<a class="anchor" href="#因果关系">·</a></h4>
<p>基于马尔可夫模型我们还可以得到一个反向条件概率分布，不过一般这不好解释</p>
<h3 id="训练">训练<a class="anchor" href="#训练">·</a></h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.utils <span class="keyword">import</span> data</span><br><span class="line">T=<span class="number">1000</span></span><br><span class="line">time = torch.arange(<span class="number">1</span>, T + <span class="number">1</span>, dtype=torch.float32)</span><br><span class="line">x = torch.sin(<span class="number">0.01</span> * time) + torch.normal(<span class="number">0</span>, <span class="number">0.2</span>, (T,))</span><br><span class="line">t=<span class="number">5</span></span><br><span class="line">train_data=torch.zeros(T-t,t)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(t):</span><br><span class="line">    train_data[:,i]=x[i:T-t+i]</span><br><span class="line">train_labels=x[t:].reshape(<span class="number">-1</span>,<span class="number">1</span>)</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">init_weights</span><span class="params">(m)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> type(m) == nn.Linear:</span><br><span class="line">        nn.init.xavier_uniform_(m.weight)</span><br><span class="line">batch_size, n_train,epoch,lr = <span class="number">16</span>, <span class="number">600</span>,<span class="number">100</span>,<span class="number">0.1</span></span><br><span class="line"></span><br><span class="line">net=nn.Sequential(</span><br><span class="line">    nn.Linear(<span class="number">5</span>,<span class="number">10</span>),</span><br><span class="line">    nn.ReLU(),</span><br><span class="line">    nn.Linear(<span class="number">10</span>,<span class="number">1</span>)</span><br><span class="line">)</span><br><span class="line">net.apply(init_weights)</span><br><span class="line">loss=nn.MSELoss(reduction=<span class="string">'none'</span>)</span><br><span class="line">optimizer=torch.optim.Adam(net.parameters(), lr)</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_array</span><span class="params">(data_arrays, batch_size, is_train=True)</span>:</span> </span><br><span class="line">    <span class="string">"""构造一个PyTorch数据迭代器。"""</span></span><br><span class="line">    dataset = data.TensorDataset(*data_arrays)</span><br><span class="line">    <span class="keyword">return</span> data.DataLoader(dataset, batch_size, shuffle=is_train)</span><br><span class="line">train_iter=load_array((train_data[:n_train],train_labels[:n_train]),batch_size)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(epoch):</span><br><span class="line">    <span class="keyword">for</span> X,y <span class="keyword">in</span> train_iter:</span><br><span class="line">        l=loss(net(X),y)</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        l.sum().backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">    cnt=<span class="number">0</span></span><br><span class="line">    loss_sum=<span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> X,y <span class="keyword">in</span> train_iter:</span><br><span class="line">        l=loss(net(X),y)</span><br><span class="line">        loss_sum+=l.sum()</span><br><span class="line">        cnt+=<span class="number">1</span></span><br><span class="line">    print(<span class="string">f'epoch<span class="subst">&#123;i+<span class="number">1</span>&#125;</span> loss: <span class="subst">&#123;float(loss_sum)/cnt&#125;</span>'</span>)</span><br></pre></td></tr></table></figure>
<p>进行下一个时间的预测到下$t$个时间的预测都相对较精准，但$t+1$及之后的时间的预测就不精准了，因为需要不断用预测的数据去预测更之后的数据，会造成误差积累。</p>
<h2 id="文本预处理">文本预处理<a class="anchor" href="#文本预处理">·</a></h2>
<p>最常见的序列数据就是文本，一篇文章可以看为是一串单词序列，甚至是一串字符序列。</p>
<p>常见预处理步骤：</p>
<ul>
<li>文本作为字符串加载到内存中</li>
<li>字符串拆分为词元（单词和字符）</li>
<li>建立一个词表，将拆分的词元映射到数字索引</li>
<li>将文本转换为数字索引序列，方便模型操作</li>
</ul>
<p>此部分在https://github.com/BUAADreamer/nnplayer/tree/master/nlputil处进行了一些实践</p>
<h2 id="语言模型和数据集">语言模型和数据集<a class="anchor" href="#语言模型和数据集">·</a></h2>
<p>语言模型的目标：估计序列的联合概率 $P(x_1,x_2,…,x_T)$</p>
<p>简单统计相对词频，执行某种形式的拉普拉斯平滑</p>
<h3 id="马尔可夫模型与n元语法">马尔可夫模型与<em>n</em>元语法<a class="anchor" href="#马尔可夫模型与n元语法">·</a></h3>
<p>涉及⼀个、两个和三个变量的概率公式分别被称为“⼀元语法”（unigram）、“⼆元语法”（bigram）和“三元语法”（trigram）模型</p>
<h3 id="自然语言统计">自然语言统计<a class="anchor" href="#自然语言统计">·</a></h3>
<p>词频最高的词都是类似 <code>the/i/and</code> 这样的词，被称为停用词，可以被过滤掉</p>
<p>词频衰减迅速，齐普夫定律：第 $i$ 个最常用的单词频率$n_i$为 $n_i\propto \frac 1 {i^\alpha}$</p>
<h2 id="循环神经网络">循环神经网络<a class="anchor" href="#循环神经网络">·</a></h2>
<h3 id="有隐状态的循环神经网络">有隐状态的循环神经网络<a class="anchor" href="#有隐状态的循环神经网络">·</a></h3>
<p>$H_t=\phi(X_tW_{xh}+H_{t-1}W_{hh}+b_h)$</p>
<h3 id="困惑度（Perplexity）">困惑度（<strong>Perplexity</strong>）<a class="anchor" href="#困惑度（Perplexity）">·</a></h3>
<p>$exp(-\frac 1 n \sum_{t=1}^n log P(x_t|x_{t-1},…,x_{1}))$</p>
<h3 id="梯度裁剪">梯度裁剪<a class="anchor" href="#梯度裁剪">·</a></h3>
<p>将梯度映射到$\theta$范围内，比如以下代码</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">grad_clipping</span><span class="params">(net, theta)</span>:</span> <span class="comment">#@save</span></span><br><span class="line">    <span class="string">"""裁剪梯度"""</span></span><br><span class="line">    <span class="keyword">if</span> isinstance(net, nn.Module):</span><br><span class="line">    	params = [p <span class="keyword">for</span> p <span class="keyword">in</span> net.parameters() <span class="keyword">if</span> p.requires_grad]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">    	params = net.params</span><br><span class="line">    norm = torch.sqrt(sum(torch.sum((p.grad ** <span class="number">2</span>)) <span class="keyword">for</span> p <span class="keyword">in</span> params)) <span class="comment">#梯度平方和开根号</span></span><br><span class="line">    <span class="keyword">if</span> norm &gt; theta:</span><br><span class="line">    	<span class="keyword">for</span> param <span class="keyword">in</span> params:</span><br><span class="line">    		param.grad[:] *= theta / norm <span class="comment">#映射到theta范围内</span></span><br></pre></td></tr></table></figure>
<h3 id="简洁实现">简洁实现<a class="anchor" href="#简洁实现">·</a></h3>
<p>参考实现的代码：https://github.com/BUAADreamer/nlpkiller/blob/master/main.py</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RNNModel</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"""循环神经网络模型</span></span><br><span class="line"><span class="string">    Defined in :numref:`sec_rnn-concise`"""</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, rnn_layer, vocab_size, **kwargs)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.rnn = rnn_layer</span><br><span class="line">        self.vocab_size = vocab_size</span><br><span class="line">        self.num_hiddens = self.rnn.hidden_size</span><br><span class="line">        <span class="comment"># 如果RNN是双向的（之后将介绍），num_directions应该是2，否则应该是1</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> self.rnn.bidirectional:</span><br><span class="line">            self.num_directions = <span class="number">1</span></span><br><span class="line">            self.linear = nn.Linear(self.num_hiddens, self.vocab_size)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.num_directions = <span class="number">2</span></span><br><span class="line">            self.linear = nn.Linear(self.num_hiddens * <span class="number">2</span>, self.vocab_size)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, inputs, state)</span>:</span></span><br><span class="line">        X = F.one_hot(inputs.T.long(), self.vocab_size)</span><br><span class="line">        X = X.to(torch.float32)</span><br><span class="line">        Y, state = self.rnn(X, state)</span><br><span class="line">        <span class="comment"># 全连接层首先将Y的形状改为(时间步数*批量大小,隐藏单元数)</span></span><br><span class="line">        <span class="comment"># 它的输出形状是(时间步数*批量大小,词表大小)。</span></span><br><span class="line">        output = self.linear(Y.reshape((<span class="number">-1</span>, Y.shape[<span class="number">-1</span>])))</span><br><span class="line">        <span class="keyword">return</span> output, state</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">begin_state</span><span class="params">(self, device, batch_size=<span class="number">1</span>)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> isinstance(self.rnn, nn.LSTM):</span><br><span class="line">            <span class="comment"># nn.GRU以张量作为隐状态</span></span><br><span class="line">            <span class="keyword">return</span> torch.zeros((self.num_directions * self.rnn.num_layers,</span><br><span class="line">                                batch_size, self.num_hiddens),</span><br><span class="line">                               device=device)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># nn.LSTM以元组作为隐状态</span></span><br><span class="line">            <span class="keyword">return</span> (torch.zeros((</span><br><span class="line">                self.num_directions * self.rnn.num_layers,</span><br><span class="line">                batch_size, self.num_hiddens), device=device),</span><br><span class="line">                    torch.zeros((</span><br><span class="line">                        self.num_directions * self.rnn.num_layers,</span><br><span class="line">                        batch_size, self.num_hiddens), device=device))</span><br></pre></td></tr></table></figure>
<h3 id="通过时间反向传播">通过时间反向传播<a class="anchor" href="#通过时间反向传播">·</a></h3>
<h4 id="数学推导1">数学推导1<a class="anchor" href="#数学推导1">·</a></h4>
<p>链式求导法则应用于 $f(h(x),g(x))$</p>
<p>$\frac {d(f(h(x),g(x)))} {dx}=\frac {d(f(h(x),g(x)))} {d(h(x))}*h’(x)+\frac {d(f(h(x),g(x)))} {d(g(x))}*g’(x)$</p>
<h4 id="数学推导2">数学推导2<a class="anchor" href="#数学推导2">·</a></h4>
<p>$a_3=b_3+c_3a_2=b_3+c_3(b_2+c_2a_1)=b_3+c_3(b_2+c_2b_1)$</p>
<p>数学归纳法证明$a_t=b_t+\sum_{i=1}<sup>{t-1}(\prod_{j=i+1}</sup>tc_j)b_i $</p>
<p>随机截断，常规截断，完整计算（不可行）</p>
<h1 id="9-现代循环神经网络">9.现代循环神经网络<a class="anchor" href="#9-现代循环神经网络">·</a></h1>
<h2 id="门控循环单元（GRU）">门控循环单元（GRU）<a class="anchor" href="#门控循环单元（GRU）">·</a></h2>
<h3 id="重置门和更新门">重置门和更新门<a class="anchor" href="#重置门和更新门">·</a></h3>
<p>R：重置门 接近0时重置隐状态，接近1时是普通RNN</p>
<p>Z：更新门 接近0时接近隐状态，接近1时倾向于保留旧状态，来自X的信息基本被忽略，跳过时间步t</p>
<h3 id="计算重置门和更新门">计算重置门和更新门<a class="anchor" href="#计算重置门和更新门">·</a></h3>
<p><img src="https://umeta-model.oss-cn-beijing.aliyuncs.com/documents/5.png" alt></p>
<h3 id="计算候选隐状态">计算候选隐状态<a class="anchor" href="#计算候选隐状态">·</a></h3>
<p><img src="https://umeta-model.oss-cn-beijing.aliyuncs.com/documents/9-2.png" alt></p>
<p>$\odot$ 是Hardmard积，按元素的乘积</p>
<h3 id="计算隐状态">计算隐状态<a class="anchor" href="#计算隐状态">·</a></h3>
<p><img src="https://umeta-model.oss-cn-beijing.aliyuncs.com/documents/9-3.png" alt></p>
<p>重置门有助于捕捉序列的短期依赖关系</p>
<p>更新门有助于捕捉序列的长期依赖关系</p>
<h3 id="简洁实现-2">简洁实现<a class="anchor" href="#简洁实现-2">·</a></h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">num_inputs = vocab_size</span><br><span class="line">gru_layer = nn.GRU(num_inputs, num_hiddens)</span><br><span class="line">model = RNNModel(gru_layer, len(vocab))</span><br><span class="line">model = model.to(device)</span><br><span class="line">train_rnn(model, train_iter, vocab, lr, num_epochs, device)</span><br></pre></td></tr></table></figure>
<h2 id="长短期记忆网络（LSTM）">长短期记忆网络（LSTM）<a class="anchor" href="#长短期记忆网络（LSTM）">·</a></h2>
<h3 id="输入门，遗忘门，输出门">输入门，遗忘门，输出门<a class="anchor" href="#输入门，遗忘门，输出门">·</a></h3>
<p><img src="https://umeta-model.oss-cn-beijing.aliyuncs.com/documents/9-4.png" alt></p>
<p>值的范围都在[0,1]内</p>
<h3 id="候选记忆元">候选记忆元<a class="anchor" href="#候选记忆元">·</a></h3>
<p><img src="https://umeta-model.oss-cn-beijing.aliyuncs.com/documents/9-5.png" alt></p>
<p>值的范围是[-1,1]</p>
<h3 id="记忆元">记忆元<a class="anchor" href="#记忆元">·</a></h3>
<p><img src="https://umeta-model.oss-cn-beijing.aliyuncs.com/documents/9-6.png" alt></p>
<h3 id="隐状态">隐状态<a class="anchor" href="#隐状态">·</a></h3>
<p><img src="https://umeta-model.oss-cn-beijing.aliyuncs.com/documents/9-7.png" alt></p>
<h3 id="简洁实现-3">简洁实现<a class="anchor" href="#简洁实现-3">·</a></h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">num_inputs = vocab_size</span><br><span class="line">lstm_layer = nn.LSTM(num_inputs, num_hiddens)</span><br><span class="line">model = RNNModel(lstm_layer, len(vocab))</span><br><span class="line">model = model.to(device)</span><br><span class="line">train_rnn(model, train_iter, vocab, lr, num_epochs, device)</span><br></pre></td></tr></table></figure>
<h2 id="深度循环神经网络">深度循环神经网络<a class="anchor" href="#深度循环神经网络">·</a></h2>
<p>堆叠隐藏层来实现</p>
<p><img src="https://umeta-model.oss-cn-beijing.aliyuncs.com/documents/9-8.png" alt></p>
<p><img src="/2022/05/15/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E6%8F%90%E5%8D%87%E7%AF%87/E:%5Ccode%5Cblog%5Csource_posts%5C2022-03-19-%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%5C9-9.png" alt></p>
<p><img src="/2022/05/15/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E6%8F%90%E5%8D%87%E7%AF%87/E:%5Ccode%5Cblog%5Csource_posts%5C2022-03-19-%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%5C9-10.png" alt></p>
<h3 id="简洁实现-4">简洁实现<a class="anchor" href="#简洁实现-4">·</a></h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#和之前的区别在于指定了</span></span><br><span class="line">vocab_size, num_hiddens, num_layers = len(vocab), <span class="number">256</span>, <span class="number">2</span></span><br><span class="line">num_inputs = vocab_size</span><br><span class="line">device = d2l.try_gpu()</span><br><span class="line">lstm_layer = nn.LSTM(num_inputs, num_hiddens, num_layers)</span><br><span class="line">model = RNNModel(lstm_layer, len(vocab))</span><br><span class="line">model = model.to(device)</span><br></pre></td></tr></table></figure>
<p><img src="https://umeta-model.oss-cn-beijing.aliyuncs.com/documents/9-1.png" alt></p>
<h2 id="双向循环神经网络">双向循环神经网络<a class="anchor" href="#双向循环神经网络">·</a></h2>
<p>增加一个从最后的词元开始<strong>从后向前运行</strong>的循环网络设计，添加反向传递信息的隐藏层。和隐马尔可夫模型中的动态规划前向后向递归没有太大区别</p>
<p>现代深度⽹络的设计原则：⾸先使⽤经典统计模型的函数依赖类型，然后将其参数化为<strong>通⽤形式</strong>。</p>
<h2 id="机器翻译与数据集">机器翻译与数据集<a class="anchor" href="#机器翻译与数据集">·</a></h2>
<p>对语言数据集的source（比如英文）和target（比如中文）同时进行统计和建模</p>
<p><strong>英法翻译数据集</strong>：有n条数据，每条数据是一句英文和一句对应法文</p>
<p><strong>数据预处理</strong>：将不间断空格转换为空格，大写转小写</p>
<p><strong>词元化</strong>：单词级词元化。返回两个词元列表：source和target。source[i]是源语言（此处为英语）第i个文本序列的词元列表，target[i]是目标语言（这里为法语）第i个文本序列的词元列表。</p>
<p>单词级词元化的词表大小远远大于字符级词元化大小，因此将低于2次的<strong>低频词元</strong>视为相同的<strong>未知词元</strong><code>&lt;unk&gt;</code>。此外指定额外词元。小批量时将序列填充到相同长度的<strong>填充词元</strong><code>&lt;pad&gt;</code>，序列的<strong>开始词元</strong><code>&lt;bos&gt;</code>，<strong>结束词元</strong><code>&lt;eos&gt;</code></p>
<p><strong>加载数据集</strong>：截断或填充让每个样本的长度保持一致，设置一个统一长度<code>num_steps</code>，小于则填充<code>&lt;pad&gt;</code>，大于则截断。在每个序列的末尾添加<code>&lt;eos&gt;</code>词元表明完成了序列输出工作</p>
<h2 id="编码器-解码器架构">编码器-解码器架构<a class="anchor" href="#编码器-解码器架构">·</a></h2>
<p>将一个长度可变的序列作为输入，转换为有固定形状的编码状态。之后解码，将固定形状的编码状态映射到长度可变的序列。比如，先将某句话编码为一个状态，再将这个状态解码，一个词元一个词元的生成翻译后的序列</p>
<p><img src="https://umeta-model.oss-cn-beijing.aliyuncs.com/documents/image-20220820220446007.png" alt="image-20220820220446007"></p>
<p>解码器新增一个<code>init_state</code>函数，将编码器输出转换为编码后的状态，这一过程可能需要额外的输出，比如输入序列的有效长度。为了生成长度可变的词元序列，解码器在每个时间步都将会将输入（比如在前一时间步生成的词元）和状态映射成当前时间步的输出词元</p>
<h2 id="序列到序列学习（seq2seq）">序列到序列学习（seq2seq）<a class="anchor" href="#序列到序列学习（seq2seq）">·</a></h2>
<p>循环神经网络编码器将长度可变的序列作为输入，转换为固定形状的隐状态，独立的循环神经网络解码器基于<strong>输入序列的编码信息</strong>和输<strong>出序列已经看见的或者生成的词元</strong>来预测下一个词元。一旦输出序列生成了<code>&lt;eos&gt;</code>词元则停止预测</p>
<p><img src="https://umeta-model.oss-cn-beijing.aliyuncs.com/documents/image-20220820223719165.png" alt="image-20220820223719165"></p>
<p><img src="https://umeta-model.oss-cn-beijing.aliyuncs.com/documents/image-20220821101535573.png" alt="image-20220821101535573"></p>
<h3 id="BLEU评估方法">BLEU评估方法<a class="anchor" href="#BLEU评估方法">·</a></h3>
<p>广泛用于测量许多应用的输出序列的质量</p>
<p>$\exp(min(0,1-\frac {len_{label}} {len_{pred}}))\prod_{n=1}<sup>kp_n</sup>{\frac 1 {2^n}}$</p>
<p>$p_n$刻画n元语法的正确率，是预测序列中n元语法与标签序列中匹配的数量除以预测序列中n元语法的数量，k是用于匹配的最长n元语法</p>
<p>比如答案序列abcde，预测序列abcdde</p>
<p>则$p_1=3/5,p_2=2/4,p_3=1/3,p_4=0$</p>
<h2 id="束搜索">束搜索<a class="anchor" href="#束搜索">·</a></h2>
<p>贪心搜索：逐个预测输出序列，每次选择概率最高概率的词元，一直到达最大长度输出完成</p>
<p>穷举搜索：列举所有的可能性</p>
<p>束搜索：改进贪心搜索。超参数为束宽(beam size)k，时间步1选择最高条件概率的k个词元，k个词元分别是k个候选输出序列的第一个词元。之后的每个时间步，基于上一时间步的k个候选输出序列，继续选出最高条件概率的k个候选输出序列。</p>
<p><img src="https://umeta-model.oss-cn-beijing.aliyuncs.com/documents/image-20220821113727477.png" alt="image-20220821113727477"></p>
<p>最后得到了6个候选输出序列 A C AB CE ABD CED</p>
<p>选择其中条件概率最高的序列作为输出序列：</p>
<p><img src="https://umeta-model.oss-cn-beijing.aliyuncs.com/documents/image-20220821114045271.png" alt="image-20220821114045271"></p>
<p>L是最终候选序列的长度，$\alpha$为对长序列的惩罚项，一般为0.75</p>
<h1 id="10-注意力机制">10.注意力机制<a class="anchor" href="#10-注意力机制">·</a></h1>
<h2 id="10-1-注意力提示">10.1 注意力提示<a class="anchor" href="#10-1-注意力提示">·</a></h2>
<p>人类注意力是有限的，有价值，稀缺的</p>
<p>人通过非自主性和自主性提示有选择性地引导注意力，前者基于自然的突出性，比如一个红色杯子和4个灰色物体，很容易注意红色杯子。后者则需要人主观投入更多注意力，比如：我要认真写作业。</p>
<p>注意力机制与全连接层/汇聚层的区别在于增加了自主提示。全连接层/汇聚层可以认为是非自主性提示，注意力机制是自主性提示</p>
<p>注意力机制通过注意力汇聚使得选择偏向于<strong>值</strong>（value 感官输入），其中包含<strong>查询</strong>（query 自主性提示），<strong>键</strong>（key 非自主性提示），<strong>键和值是成对</strong>的，相当于<strong>感官输入的非自主提示</strong>。我们设计注意力汇聚，让给定的<strong>查询（自主性提示）<strong>可以和</strong>键（非自主性提示）<strong>进行匹配，引导得出</strong>最匹配的值（感官输入）</strong></p>
<p><img src="https://umeta-model.oss-cn-beijing.aliyuncs.com/documents/image-20220821193248884.png" alt="image-20220821193248884"></p>
<p>这里的注意力汇聚其实是通过调整不同参数的权重来实现的。</p>
<h2 id="10-2-注意力汇聚：Nadaraya-Watson-核回归">10.2 注意力汇聚：Nadaraya-Watson 核回归<a class="anchor" href="#10-2-注意力汇聚：Nadaraya-Watson-核回归">·</a></h2>
<p>注意力汇聚公式：$f(x)=\sum_{i=1}^n\alpha(x,x_i)y_i$   是$y_i$的加权平均</p>
<p>平均汇聚$\sum_{i=1}^ny_i$可以看成是特殊的注意力汇聚</p>
<p>查询x和键$x_i$之间的关系建模为注意力权重$\alpha(x,x_i)$，并分配给每一个对应值$y_i$，对于<strong>任何查询</strong>，模型在所有键值对注意力权重都是一个有效的概率分布：<strong>非负且总和为1</strong></p>
<p>高斯核：$K(u)=\frac 1 {\sqrt{2\pi}}exp(-\frac {u^2} 2)$</p>
<p>一个键越接近所给的查询x，则分配给这个键对应值$y_i$的注意力权重越大，也就获得了<strong>更多注意力</strong></p>
<p>unsqueeze函数：在指定位置插入一个维度，默认为1</p>
<h3 id="非参数型注意力汇聚">非参数型注意力汇聚<a class="anchor" href="#非参数型注意力汇聚">·</a></h3>
<p><img src="https://umeta-model.oss-cn-beijing.aliyuncs.com/documents/10-1.png" alt></p>
<h3 id="参数型非注意力汇聚（加入一个权重，可以被训练）">参数型非注意力汇聚（加入一个权重，可以被训练）<a class="anchor" href="#参数型非注意力汇聚（加入一个权重，可以被训练）">·</a></h3>
<p>$\omega$仅仅为一个参数，长度为1</p>
<p><img src="https://umeta-model.oss-cn-beijing.aliyuncs.com/documents/10-2.png" alt></p>
<h2 id="10-3-注意力评分函数">10.3 注意力评分函数<a class="anchor" href="#10-3-注意力评分函数">·</a></h2>
<p>假设一个查询$q\in R^q$和m个键值对$(k_1,v_1),…,(k_m,v_m)$，其中$k_i\in R^k,v_i\in R^v$，注意力汇聚函数$f$被表示为值的加权和：</p>
<p>$f(q,(k_1,v_1),…,(k_m,v_m))=\sum_{i=1}^m\alpha (q,k_i)v_i \in R^v$</p>
<p>其中查询$q$和键$k_i$的注意力权重（标量）是通过注意力评分函数$a$将两个向量映射成标量，再经过softmax运算得到。</p>
<p>$\alpha (q,k_i)=softmax(a(q,k_i))=\frac {exp(a(q,k_i))} {\sum_{j=1}^mexp(a(q,k_j))}\in R$</p>
<h3 id="遮掩softmax操作">遮掩softmax操作<a class="anchor" href="#遮掩softmax操作">·</a></h3>
<p>指定一个有效序列长度，即词元个数，从而过滤掉超出指定范围的位置</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">masked_softmax</span><span class="params">(X, valid_lens)</span>:</span></span><br><span class="line">    <span class="string">"""通过在最后⼀个轴上掩蔽元素来执⾏softmax操作"""</span></span><br><span class="line">    <span class="comment"># X:3D张量，valid_lens:1D或2D张量</span></span><br><span class="line">    <span class="keyword">if</span> valid_lens <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">    	<span class="keyword">return</span> nn.functional.softmax(X, dim=<span class="number">-1</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">    	shape = X.shape</span><br><span class="line">    <span class="keyword">if</span> valid_lens.dim() == <span class="number">1</span>:</span><br><span class="line">    	valid_lens = torch.repeat_interleave(valid_lens, shape[<span class="number">1</span>])</span><br><span class="line">        <span class="comment">#repeat_interleave([2,3],2)==&gt;[2,2,3,3]</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">    	valid_lens = valid_lens.reshape(<span class="number">-1</span>) </span><br><span class="line">        <span class="comment"># 最后⼀轴上被掩蔽的元素使⽤⼀个⾮常⼤的负值替换，从⽽其softmax输出为0 </span></span><br><span class="line">        X = d2l.sequence_mask(X.reshape(<span class="number">-1</span>, shape[<span class="number">-1</span>]), valid_lens,</span><br><span class="line">    	value=<span class="number">-1e6</span>)</span><br><span class="line">    <span class="comment">#对处理后的X做softmax运算</span></span><br><span class="line">    <span class="keyword">return</span> nn.functional.softmax(X.reshape(shape), dim=<span class="number">-1</span>)</span><br><span class="line">masked_softmax(torch.rand(<span class="number">2</span>, <span class="number">2</span>, <span class="number">4</span>), torch.tensor([<span class="number">2</span>, <span class="number">3</span>]))</span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">得到结果:</span></span><br><span class="line"><span class="string">tensor([[[0.3412, 0.6588, 0.0000, 0.0000],</span></span><br><span class="line"><span class="string">        [0.4643, 0.5357, 0.0000, 0.0000]],</span></span><br><span class="line"><span class="string">        [[0.1967, 0.3512, 0.4522, 0.0000],</span></span><br><span class="line"><span class="string">        [0.3580, 0.3092, 0.3328, 0.0000]]])</span></span><br><span class="line"><span class="string">'''</span></span><br></pre></td></tr></table></figure>
<h3 id="加性注意力">加性注意力<a class="anchor" href="#加性注意力">·</a></h3>
<p>当<strong>查询</strong>和<strong>键</strong>是<strong>不同长度的矢量</strong>时，可以使用<strong>加性注意力</strong>作为评分函数。给定查询$q\in R^q$和键$k\in R^k$，加性注意力评分函数为
$$
a(q,k)=w_v^Ttanh(W_qq+W_kk)\in R
$$
其中可学习的参数是$W_q\in R^{h\times q},W_k\in R^{h\times k}$和$w_v\in R^h$</p>
<h3 id="缩放点积注意力">缩放点积注意力<a class="anchor" href="#缩放点积注意力">·</a></h3>
<p>查询和键是相同长度矢量时计算效率更高
$$
a(q,k)=q^Tk/\sqrt d
$$</p>
<p>基于n个查询和m个键值对计算注意力，查询和键的长度为d，值的长度为v，查询$Q\in R^{n\times d}$，键$K\in R^{m\times d}$ 和值$V\in R^{m\times v}$的缩放点积注意力为
$$
softmax(\frac {QK^T} {\sqrt d})V\in R^{n\times v}
$$</p>
<h2 id="10-4-Bahdanau注意力">10.4 Bahdanau注意力<a class="anchor" href="#10-4-Bahdanau注意力">·</a></h2>
<p>在预测词元时，如果<strong>不是所有输⼊词元都相关</strong>，模型将<strong>仅对⻬</strong>（或参与）输⼊序列中<strong>与当前预测相关的部分</strong>。这是通过将<strong>上下⽂变量</strong>视为<strong>注意⼒集中的输出</strong>来实现的。</p>
<p>9.7中的上下文变量是注意力集中的输出：</p>
<p>$c_{t’}=\sum_{t=1}^T\alpha (s_{t’-1},h_t)h_t$</p>
<p>其中，时间步$t’-1$时的解码器隐状态$s_{t’-1}$是查询，编码器隐状态$h_t$既是键，也是值，注意⼒权重$\alpha$是使⽤<strong>加性注意⼒打分函数</strong>计算的。</p>
<p><img src="https://umeta-model.oss-cn-beijing.aliyuncs.com/documents/image-20220822094459916.png" alt="image-20220822094459916"></p>
<h2 id="10-5-多头注意力">10.5 多头注意力<a class="anchor" href="#10-5-多头注意力">·</a></h2>
<p>将多个注意力汇聚输出进行连结，每个注意力汇聚都被称为一个头</p>
<p><img src="https://umeta-model.oss-cn-beijing.aliyuncs.com/documents/image-20220822094343539.png" alt="image-20220822094343539"></p>
<h2 id="10-6-自注意力和位置编码">10.6 自注意力和位置编码<a class="anchor" href="#10-6-自注意力和位置编码">·</a></h2>
<p>同⼀组词元同时充当查询、键和值，每个查询都会关注所有的键－值对并⽣成⼀个注意⼒输出</p>
<p><strong>查询、键和值来自同⼀组输入</strong>，因此被称为<strong>⾃注意⼒</strong></p>
<p>自注意力同时具有<strong>最大路径长度短</strong>和<strong>可并行</strong>两个优势，CNN可并行，RNN不可并行</p>
<p>为了学习位置信息，引入<strong>位置编码</strong>：$p_{i,2j}=sin(\frac i {10000^{\frac {2j} d}})$   $p_{i,2j+1}=cos(\frac i {10000^{\frac {2j} d}})$</p>
<h2 id="10-7-Transformer">10.7 Transformer<a class="anchor" href="#10-7-Transformer">·</a></h2>
<p>Transformer编码器和解码器是基于⾃注意⼒的模块叠加⽽成的，源（输⼊）序列和⽬标（输出）序列的嵌⼊（embedding）表⽰将加上位置编码（positional encoding），再分别输⼊到编码器和解码器中。</p>
<p><img src="https://umeta-model.oss-cn-beijing.aliyuncs.com/documents/image-20220822100053869.png" alt="image-20220822100053869"></p>
<p>NLP中一般使用层规范化而不是批量规范化</p>
<h1 id="11-优化算法">11.优化算法<a class="anchor" href="#11-优化算法">·</a></h1>
<h1 id="12-计算性能">12.计算性能<a class="anchor" href="#12-计算性能">·</a></h1>
<h1 id="13-计算机视觉">13.计算机视觉<a class="anchor" href="#13-计算机视觉">·</a></h1>
<h1 id="14-自然语言处理：预训练">14.自然语言处理：预训练<a class="anchor" href="#14-自然语言处理：预训练">·</a></h1>
<p><img src="https://umeta-model.oss-cn-beijing.aliyuncs.com/documents/image-20220822104502404.png" alt="image-20220822104502404"></p>
<h2 id="14-1-词嵌入（Word2Vec）">14.1 词嵌入（Word2Vec）<a class="anchor" href="#14-1-词嵌入（Word2Vec）">·</a></h2>
<p><strong>单词映射到实向量</strong>的<strong>技术</strong>称为<strong>词嵌⼊</strong></p>
<p>独热编码不能刻画词语之间的相似度，需要能够刻画相似度的向量表示</p>
<h3 id="自监督的word2vec">自监督的word2vec<a class="anchor" href="#自监督的word2vec">·</a></h3>
<h4 id="跳元模型（Skip-Gram）">跳元模型（Skip-Gram）<a class="anchor" href="#跳元模型（Skip-Gram）">·</a></h4>
<p>假设<strong>一个词</strong>可以用来在文本序列中<strong>生成其周围的单词</strong></p>
<p>每个词都有<strong>两个d维向量表示</strong>，用于计算条件概率。对于词典中索引为i的任何词，分别用$v_i\in R^d$和$u_i\in R^d$表示其用作<strong>中心词</strong>和<strong>上下文词</strong>时的<strong>两个向量</strong>。</p>
<p><strong>给定中心词</strong>$w_c$（词典中的索引c），<strong>生成任何上下文词</strong>$w_o$（词典中索引o）的条件概率可以通过对<strong>向量点积的softmax操作</strong>来建模：</p>
<p>$P(w_o|w_c)=\frac {exp(u_o^Tv_c)} {\sum_{i\in V}exp(u_i^Tv_c)}$</p>
<p>词表索引集$V={0,1,…,|V|-1}$ 给定长度为T的文本序列，其中时间步t处的词表示为$w^{(t)}$。假设上下文词是在给定任何中心词的情况下独立生成的。对于<strong>上下文窗口m</strong>，跳元模型的似然函数是<strong>给定任何中心词</strong>的情况下<strong>生成所有上下文词</strong>的概率：</p>
<p>$\prod_{t=1}^T\prod_{-m\le j\le m,j\neq 0}P(w<sup>{(t+j)}|w</sup>{(t)})$</p>
<h5 id="训练-2">训练<a class="anchor" href="#训练-2">·</a></h5>
<p>用最大化似然函数（极大似然估计）来学习模型参数</p>
<p>最终使用<strong>中心词向量</strong>作为词表示</p>
<h4 id="连续词袋模型（CBOW）">连续词袋模型（CBOW）<a class="anchor" href="#连续词袋模型（CBOW）">·</a></h4>
<p>基于在文本序列中的<strong>周围上下文词</strong>生成<strong>中心词</strong></p>
<p>也就是P( “loves” | “I” , “you” )</p>
<p>计算条件概率时对<strong>上下文词向量进行平均</strong></p>
<p>给定上下文词$w_{o_1},…,w_{o_m}$生成中心词$w_c$的条件概率由以下公式建模：</p>
<p>$P(w_c|w_{o_1},…,w_{o_m})=\frac {exp(\frac 1 {2m}u_c^T(v_{o_1}+…+v_{o_{2m}}))} {\sum_{i\in V}exp(\frac 1 {2m}u_i^T(v_{o_1}+…+v_{o_{2m}}))}$</p>
<p>通常使用上下文词向量作为词表示</p>
<h2 id="14-2-近似训练">14.2 近似训练<a class="anchor" href="#14-2-近似训练">·</a></h2>
<ul>
<li>负采样。修改目标函数，增加负采样。训练的计算量与每⼀步的噪声词数成线性关系。</li>
<li>层序Softmax。使⽤⼆叉树中从根节点到叶节点的路径构造损失函数。训练的计算成本取决于词表⼤⼩的对数。</li>
</ul>
<h2 id="14-5-全局向量的词嵌入（GloVe）">14.5 全局向量的词嵌入（GloVe）<a class="anchor" href="#14-5-全局向量的词嵌入（GloVe）">·</a></h2>
<h3 id="带全局语料统计的跳元模型">带全局语料统计的跳元模型<a class="anchor" href="#带全局语料统计的跳元模型">·</a></h3>
<p>用$q_{ij}$表示词$w_j$的条件概率$P(w_j|w_i)$，在跳元模型中给定词$w_i$，可以得到：</p>
<p>$q_{ij}=\frac {exp(u_j^Tv_i)} {\sum_{k\in V}exp(u_k^Tv_i)}$</p>
<p>在整个语料库中，词$w_i$可能出现多次，则所有以它为中心词的上下文词形成了一个词索引的多重集$C_i$，允许同一元素的多个实例。实例数称为重数。比如以$w_i$为中心词的上下文词索引为a,b,c,d和a,c,e 那么多重集$C_i={a,a,b,c,c,d,e}$ a,b,c,d,e的重数为2，1，2，1，1</p>
<p>将多重集$C_i$中$j$元素的重数表示为$x_{ij}$，这是词$w_j$作为上下文词和词$w_i$作为中心词在整个语料库中的同一上下文窗口中的全局共现计数，则此时的跳元模型损失函数为：$-\sum_{i\in V}\sum_{j\in V}x_{ij}\log q_{ij}$</p>
<p>用$x_i$表示上下文窗口中所有上下文词的数量，其中$w_i$作为中心词出现，相当于$|C_i|$，设$p_{ij}$为用于生成上下文词$w_{j}$的条件概率$x_{ij}/x_i$，给定中心词，将损失函数重写为：$-\sum_{i\in V}x_i\sum_{j\in V}p_{ij}\log q_{ij}$</p>
<p>计算全局语料统计的条件分布$p_{ij}$和模型预测的条件分布$q_{ij}$的交叉熵，这⼀损失也按$x_i$加权</p>
<p>这个损失函数计算昂贵且给太多罕见事件赋值，需要替换</p>
<h3 id="GloVe模型">GloVe模型<a class="anchor" href="#GloVe模型">·</a></h3>
<p>基于平方损失，修改了跳元模型</p>
<ol>
<li>使用变量$p’<em>{ij}=x</em>{ij}$ 和$q’<em>{ij}=exp(u_j^Tv_i)$ 而非概率分布，并取二者的对数。平方损失项是$(\log(p’</em>{ij})-\log(q’<em>{ij}))<sup>2=(u_j</sup>Tv_i-\log x</em>{ij})^2$</li>
<li>为每个词$w_i$添加两个标量参数模型：中心词偏置$b_i$和上下文偏置$c_i$</li>
<li>用权重函数$h(x_{ij})$替换每个损失项的权重，其中$h(x)$在[0,1]区间单调增</li>
</ol>
<p>整合以下，训练GloVe就是降低以下损失函数</p>
<p>$\sum_{i\in V}\sum_{j\in V}h(x_{ij})(u_j^Tv_i+b_i+c_j-\log x_{ij})^2$</p>
<p>对于权重函数，建议是：当x&lt;c时，$h(x)=(x/c)^\alpha$，否则$h(x)=1$</p>
<p>这样可以省略掉任何$x_{ij}=0$的平方损失项</p>
<p>每次随机抽样一批非零的$x_{ij}$（预先计算的全局语料库统计数据，所以GloVe被叫做全局向量）来计算梯度并更新模型参数</p>
<p>由于$x_{ij}=x_{ji}$，所以任意词的中心词向量和上下文词向量是等价的，实际应用中，同一个词经过训练后，在两个向量中可能得到不同值，因此最后将其相加作为输出向量。</p>
<h2 id="14-6-子词嵌入">14.6 子词嵌入<a class="anchor" href="#14-6-子词嵌入">·</a></h2>
<h3 id="fastText模型">fastText模型<a class="anchor" href="#fastText模型">·</a></h3>
<p>子词级跳元模型</p>
<p>对于where单词，先在开头和末尾添加特殊字符&quot;&lt;“和”&gt;&quot;，从而将前缀和后缀与其他子词分开。之后提取出n-gram。比如n=3时，获得长度为3的所有子词：<code>&lt;wh,whe,her,ere,re&gt;</code>和特殊子词<code>&lt;where&gt;</code></p>
<p>对于任意词w，用$g_w$表示其长度在3和6之间的所有子词与其特殊子词的并集。<strong>词表</strong>是<strong>所有子词的集合</strong>。假设$z_g$是词典中的子词g的向量，则跳元模型中作为中心词的词w的向量$v_w$是其子词向量的和：</p>
<p>$v_w=\sum_{g\in g_w}z_g$</p>
<p>其余部分和跳元模型相同。因此fastText词量更大，模型参数也更多。同时计算一个词表示必须对所有子词向量求和，计算复杂度增高。但是对于具有相似结构的词之间共享来自子词的参数，罕见词甚至词表外的词在fastText中可能获得更好的向量表示</p>
<h3 id="字节对编码（Byte-Pair-Encoding）">字节对编码（Byte Pair Encoding）<a class="anchor" href="#字节对编码（Byte-Pair-Encoding）">·</a></h3>
<p>在fastText中所有子词长度必须指定，词表大小不能预定义</p>
<p>为了在固定大小的词表中允许可变长度的子词，应用一种字节对编码压缩算法来提取子词</p>
<p>字节对编码执⾏训练数据集的统计分析，以发现单词内的公共符号，诸如任意⻓度的连续字符。从⻓度为1的符号开始，字节对编码迭代地合并最频繁的连续符号对以产⽣新的更⻓的符号。请注意，为提⾼效率，不考虑跨越单词边界的对。最后，我们可以使⽤像⼦词这样的符号来切分单词。字节对编码及其变体已经⽤于诸如<strong>GPT-2</strong>和<strong>RoBERTa</strong>等⾃然语⾔处理预训练模型中的输⼊表⽰。</p>
<h2 id="14-7-词的相似性和类比任务">14.7 词的相似性和类比任务<a class="anchor" href="#14-7-词的相似性和类比任务">·</a></h2>
<ul>
<li>在实践中，在⼤型语料库上预先练的词向量可以应⽤于下游的⾃然语⾔处理任务。</li>
<li>预训练的词向量可以应⽤于词的<strong>相似性</strong>和<strong>类⽐任务</strong>。</li>
</ul>
<h2 id="14-8-来⾃Transformers的双向编码器表⽰（BERT）">14.8 来⾃Transformers的双向编码器表⽰（BERT）<a class="anchor" href="#14-8-来⾃Transformers的双向编码器表⽰（BERT）">·</a></h2>
<h3 id="从上下⽂⽆关到上下⽂敏感">从上下⽂⽆关到上下⽂敏感<a class="anchor" href="#从上下⽂⽆关到上下⽂敏感">·</a></h3>
<p>之前的编码方式都采用上下文无关表示，但是很多词语在不同句子中有不同表示，因此需要上下文敏感表示方法。常见的有：TagLM（language-model-augmented sequence tagger，语⾔模型增强的序列标记器），CoVe（Context Vectors，上下⽂向量），ELMo（Embeddings from Language Models，来⾃语⾔模型的嵌⼊）</p>
<p>例如，通过将整个序列作为输⼊，<strong>ELMo</strong>是<strong>为输⼊序列中的每个单词分配⼀个表⽰</strong>的函数。具体来说，ELMo将来⾃预训练的双向⻓短期记忆⽹络的所有中间层表⽰组合为输出表⽰。然后，ELMo的表⽰将作为附加特征添加到下游任务的现有监督模型中，例如通过将ELMo的表⽰和现有模型中词元的原始表⽰（例如GloVe）连结起来。⼀⽅⾯，在加⼊ELMo表⽰后，冻结了预训练的双向LSTM模型中的所有权重。另⼀⽅⾯，现有的监督模型是专⻔为给定的任务定制的。利⽤当时不同任务的不同最佳模型，添加ELMo<strong>改进了六种⾃然语⾔处理任务的技术⽔平</strong>：情感分析、⾃然语⾔推断、语义⻆⾊标注、共指消解、命名实体识别和问答。</p>
<h3 id="从特定于任务到不可知任务">从特定于任务到不可知任务<a class="anchor" href="#从特定于任务到不可知任务">·</a></h3>
<p>尽管ELMo显著改进了各种⾃然语⾔处理任务的解决⽅案，但每个解决⽅案仍然依赖于⼀个特定于任务的架构。然⽽，<strong>为每⼀个⾃然语⾔处理任务设计⼀个特定的架构</strong>实际上并不是⼀件容易的事。<strong>GPT（Generative Pre Training，⽣成式预训练）模型</strong>为上下⽂的敏感表⽰设计了通⽤的<strong>任务⽆关模型</strong> 。GPT建⽴在Transformer解码器的基础上，预训练了⼀个⽤于表⽰⽂本序列的语⾔模型。当将GPT应⽤于下游任务时，语⾔模型的输出将被送到⼀个附加的线性输出层，以<strong>预测任务的标签</strong>。与ELMo冻结预训练模型的参数不同，<strong>GPT在下游任务的监督学习过程中对预训练Transformer解码器中的所有参数进⾏微调</strong>。GPT在⾃然语⾔推断、问答、句⼦相似性和分类等12项任务上进⾏了评估，并在对模型架构进⾏最⼩更改的情况下改善了其中9项任务的最新⽔平。然⽽，由于语⾔模型的⾃回归特性，GPT只能向前看（从左到右）。在“i went to the bank to deposit cash”（我去银⾏存现⾦）和“i went to the bank to sit down”（我去河岸边坐下）的上下⽂中，由于“bank”对其左边的上下⽂敏感，GPT将返回“bank”的相同表⽰，尽管它有不同的含义。</p>
<h3 id="BERT：把两个最好的结合起来">BERT：把两个最好的结合起来<a class="anchor" href="#BERT：把两个最好的结合起来">·</a></h3>
<p>如我们所⻅，<strong>ELMo对上下⽂进⾏双向编码</strong>，但使⽤<strong>特定于任务</strong>的架构；⽽<strong>GPT是任务⽆关</strong>的，但是<strong>从左到右编码</strong>上下⽂。<strong>BERT（来⾃Transformers的双向编码器表⽰）<strong>结合了这两个⽅⾯的优点。它对</strong>上下⽂进⾏双向编码</strong>，并且<strong>对于⼤多数的⾃然语⾔处理任务只需要最少的架构改变</strong>。通过使⽤预训练的Transformer编码器，BERT能够基于其双向上下⽂表⽰任何词元。在下游任务的监督学习过程中，BERT在两个⽅⾯与GPT相似。⾸先，BERT表⽰将被输⼊到⼀个添加的输出层中，根据任务的性质对模型架构进⾏最⼩的更改，例如预测每个词元与预测整个序列。其次，对预训练Transformer编码器的所有参数进⾏微调，⽽额外的输出层将从头开始训练。</p>
<p><img src="https://umeta-model.oss-cn-beijing.aliyuncs.com/documents/image-20220823213359539.png" alt="image-20220823213359539"></p>
<h3 id="输入表示">输入表示<a class="anchor" href="#输入表示">·</a></h3>
<p>有些任务以单文本作为输入（情感分析），有些任务以一对文本序列作为输入（自然语言推断）。BERT输入序列明确表示单个文本和文本对。</p>
<p>输入为单文本的BERT输入序列：特殊类别词元<code>&lt;cls&gt;</code>、文本序列的标记、特殊分割词元<code>&lt;sep&gt;</code>的连结</p>
<p>输入为文本对时的BERT输入序列：<code>&lt;cls&gt;</code>、第一个文本序列的标记、<code>&lt;sep&gt;</code>、第二个文本序列的标记、<code>&lt;sep&gt;</code>的终结</p>
<p>BERT输⼊序列的嵌⼊是词元嵌⼊、⽚段嵌⼊和位置嵌⼊的和。</p>
<p><img src="https://umeta-model.oss-cn-beijing.aliyuncs.com/documents/image-20220824105522963.png" alt="image-20220824105522963"></p>
<h3 id="预训练任务">预训练任务<a class="anchor" href="#预训练任务">·</a></h3>
<h4 id="掩蔽语言模型（Masked-Language-Modeling）">掩蔽语言模型（<strong>Masked Language Modeling</strong>）<a class="anchor" href="#掩蔽语言模型（Masked-Language-Modeling）">·</a></h4>
<p>为了双向编码上下⽂以表⽰每个词元，BERT<strong>随机掩蔽词元</strong>并使⽤来⾃<strong>双向上下⽂的词元</strong>以⾃监督的⽅式**预测掩蔽词元。**此任务称为掩蔽语⾔模型。</p>
<h4 id="下一句预测（Next-Sentence-Prediction）">下一句预测（<strong>Next Sentence Prediction</strong>）<a class="anchor" href="#下一句预测（Next-Sentence-Prediction）">·</a></h4>
<p>为了帮助理解两个⽂本序列之间的关系，BERT在预训练中考虑了⼀个⼆元分类任务——下⼀句预测</p>
<h1 id="15-自然语言处理：应用">15.自然语言处理：应用<a class="anchor" href="#15-自然语言处理：应用">·</a></h1>

                
                <hr>
                <!-- Pager -->
                <ul class="pager">
                    
                    <li class="previous">
                        <a href="/2022/06/04/信号处理与信息推断/" data-toggle="tooltip" data-placement="top" title="信号处理与信息推断">&larr; Previous Post</a>
                    </li>
                    
                    
                    <li class="next">
                        <a href="/2022/04/17/x86-Learning-Note/" data-toggle="tooltip" data-placement="top" title="x86学习笔记">Next Post &rarr;</a>
                    </li>
                    
                </ul>

                <!-- tip start -->
                

                
                <!-- tip end -->

                <!-- Music start-->
                
                <!-- Music end -->

                <!-- Sharing -->
                
                <div class="social-share"  data-wechat-qrcode-helper="" align="center"></div>
                <!--  css & js -->
                <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/social-share.js/1.0.16/css/share.min.css">
                <script src="https://cdnjs.cloudflare.com/ajax/libs/social-share.js/1.0.16/js/social-share.min.js"></script>
                
                <!-- Sharing -->

                <!-- gitment start -->
                
                <!-- gitment end -->

                <!-- 来必力City版安装代码 -->
                
                <!-- City版安装代码已完成 -->

                <!-- disqus comment start -->
                
                <!-- disqus comment end -->
            </div>
            
            <!-- Tabe of Content -->
            <!-- Table of Contents -->

    
      
        <aside id="sidebar">
          <div id="toc" class="toc-article">
          <strong class="toc-title">Contents</strong>
          
            
              <ol class="toc-nav"><li class="toc-nav-item toc-nav-level-1"><a class="toc-nav-link" href="#8-循环神经网络（RNN）"><span class="toc-nav-text">8.循环神经网络（RNN）</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#序列模型"><span class="toc-nav-text">序列模型</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#统计工具"><span class="toc-nav-text">统计工具</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#自回归模型"><span class="toc-nav-text">自回归模型</span></a></li><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#马尔可夫模型"><span class="toc-nav-text">马尔可夫模型</span></a></li><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#因果关系"><span class="toc-nav-text">因果关系</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#训练"><span class="toc-nav-text">训练</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#文本预处理"><span class="toc-nav-text">文本预处理</span></a></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#语言模型和数据集"><span class="toc-nav-text">语言模型和数据集</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#马尔可夫模型与n元语法"><span class="toc-nav-text">马尔可夫模型与n元语法</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#自然语言统计"><span class="toc-nav-text">自然语言统计</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#循环神经网络"><span class="toc-nav-text">循环神经网络</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#有隐状态的循环神经网络"><span class="toc-nav-text">有隐状态的循环神经网络</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#困惑度（Perplexity）"><span class="toc-nav-text">困惑度（Perplexity）</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#梯度裁剪"><span class="toc-nav-text">梯度裁剪</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#简洁实现"><span class="toc-nav-text">简洁实现</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#通过时间反向传播"><span class="toc-nav-text">通过时间反向传播</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#数学推导1"><span class="toc-nav-text">数学推导1</span></a></li><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#数学推导2"><span class="toc-nav-text">数学推导2</span></a></li></ol></li></ol></li></ol></li><li class="toc-nav-item toc-nav-level-1"><a class="toc-nav-link" href="#9-现代循环神经网络"><span class="toc-nav-text">9.现代循环神经网络</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#门控循环单元（GRU）"><span class="toc-nav-text">门控循环单元（GRU）</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#重置门和更新门"><span class="toc-nav-text">重置门和更新门</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#计算重置门和更新门"><span class="toc-nav-text">计算重置门和更新门</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#计算候选隐状态"><span class="toc-nav-text">计算候选隐状态</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#计算隐状态"><span class="toc-nav-text">计算隐状态</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#简洁实现-2"><span class="toc-nav-text">简洁实现</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#长短期记忆网络（LSTM）"><span class="toc-nav-text">长短期记忆网络（LSTM）</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#输入门，遗忘门，输出门"><span class="toc-nav-text">输入门，遗忘门，输出门</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#候选记忆元"><span class="toc-nav-text">候选记忆元</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#记忆元"><span class="toc-nav-text">记忆元</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#隐状态"><span class="toc-nav-text">隐状态</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#简洁实现-3"><span class="toc-nav-text">简洁实现</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#深度循环神经网络"><span class="toc-nav-text">深度循环神经网络</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#简洁实现-4"><span class="toc-nav-text">简洁实现</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#双向循环神经网络"><span class="toc-nav-text">双向循环神经网络</span></a></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#机器翻译与数据集"><span class="toc-nav-text">机器翻译与数据集</span></a></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#编码器-解码器架构"><span class="toc-nav-text">编码器-解码器架构</span></a></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#序列到序列学习（seq2seq）"><span class="toc-nav-text">序列到序列学习（seq2seq）</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#BLEU评估方法"><span class="toc-nav-text">BLEU评估方法</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#束搜索"><span class="toc-nav-text">束搜索</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-1"><a class="toc-nav-link" href="#10-注意力机制"><span class="toc-nav-text">10.注意力机制</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#10-1-注意力提示"><span class="toc-nav-text">10.1 注意力提示</span></a></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#10-2-注意力汇聚：Nadaraya-Watson-核回归"><span class="toc-nav-text">10.2 注意力汇聚：Nadaraya-Watson 核回归</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#非参数型注意力汇聚"><span class="toc-nav-text">非参数型注意力汇聚</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#参数型非注意力汇聚（加入一个权重，可以被训练）"><span class="toc-nav-text">参数型非注意力汇聚（加入一个权重，可以被训练）</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#10-3-注意力评分函数"><span class="toc-nav-text">10.3 注意力评分函数</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#遮掩softmax操作"><span class="toc-nav-text">遮掩softmax操作</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#加性注意力"><span class="toc-nav-text">加性注意力</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#缩放点积注意力"><span class="toc-nav-text">缩放点积注意力</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#10-4-Bahdanau注意力"><span class="toc-nav-text">10.4 Bahdanau注意力</span></a></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#10-5-多头注意力"><span class="toc-nav-text">10.5 多头注意力</span></a></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#10-6-自注意力和位置编码"><span class="toc-nav-text">10.6 自注意力和位置编码</span></a></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#10-7-Transformer"><span class="toc-nav-text">10.7 Transformer</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-1"><a class="toc-nav-link" href="#11-优化算法"><span class="toc-nav-text">11.优化算法</span></a></li><li class="toc-nav-item toc-nav-level-1"><a class="toc-nav-link" href="#12-计算性能"><span class="toc-nav-text">12.计算性能</span></a></li><li class="toc-nav-item toc-nav-level-1"><a class="toc-nav-link" href="#13-计算机视觉"><span class="toc-nav-text">13.计算机视觉</span></a></li><li class="toc-nav-item toc-nav-level-1"><a class="toc-nav-link" href="#14-自然语言处理：预训练"><span class="toc-nav-text">14.自然语言处理：预训练</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#14-1-词嵌入（Word2Vec）"><span class="toc-nav-text">14.1 词嵌入（Word2Vec）</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#自监督的word2vec"><span class="toc-nav-text">自监督的word2vec</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#跳元模型（Skip-Gram）"><span class="toc-nav-text">跳元模型（Skip-Gram）</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-5"><a class="toc-nav-link" href="#训练-2"><span class="toc-nav-text">训练</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#连续词袋模型（CBOW）"><span class="toc-nav-text">连续词袋模型（CBOW）</span></a></li></ol></li></ol></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#14-2-近似训练"><span class="toc-nav-text">14.2 近似训练</span></a></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#14-5-全局向量的词嵌入（GloVe）"><span class="toc-nav-text">14.5 全局向量的词嵌入（GloVe）</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#带全局语料统计的跳元模型"><span class="toc-nav-text">带全局语料统计的跳元模型</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#GloVe模型"><span class="toc-nav-text">GloVe模型</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#14-6-子词嵌入"><span class="toc-nav-text">14.6 子词嵌入</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#fastText模型"><span class="toc-nav-text">fastText模型</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#字节对编码（Byte-Pair-Encoding）"><span class="toc-nav-text">字节对编码（Byte Pair Encoding）</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#14-7-词的相似性和类比任务"><span class="toc-nav-text">14.7 词的相似性和类比任务</span></a></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#14-8-来⾃Transformers的双向编码器表⽰（BERT）"><span class="toc-nav-text">14.8 来⾃Transformers的双向编码器表⽰（BERT）</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#从上下⽂⽆关到上下⽂敏感"><span class="toc-nav-text">从上下⽂⽆关到上下⽂敏感</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#从特定于任务到不可知任务"><span class="toc-nav-text">从特定于任务到不可知任务</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#BERT：把两个最好的结合起来"><span class="toc-nav-text">BERT：把两个最好的结合起来</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#输入表示"><span class="toc-nav-text">输入表示</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#预训练任务"><span class="toc-nav-text">预训练任务</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#掩蔽语言模型（Masked-Language-Modeling）"><span class="toc-nav-text">掩蔽语言模型（Masked Language Modeling）</span></a></li><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#下一句预测（Next-Sentence-Prediction）"><span class="toc-nav-text">下一句预测（Next Sentence Prediction）</span></a></li></ol></li></ol></li></ol></li><li class="toc-nav-item toc-nav-level-1"><a class="toc-nav-link" href="#15-自然语言处理：应用"><span class="toc-nav-text">15.自然语言处理：应用</span></a></li></ol>
            
          
          </div>
        </aside>
      
    

                
            <!-- Sidebar Container -->
            <div class="
                col-lg-8 col-lg-offset-2
                col-md-10 col-md-offset-1
                sidebar-container">

                <!-- Featured Tags -->
                
                <section>
                    <!-- no hr -->
                    <h5><a href="/tags/">FEATURED TAGS</a></h5>
                    <div class="tags">
                       
                          <a class="tag" href="/tags/#DeepLearning" title="DeepLearning">DeepLearning</a>
                        
                    </div>
                </section>
                

                <!-- Friends Blog -->
                
                <hr>
                <h5>FRIENDS</h5>
                <ul class="list-inline">

                    
                        <li><a href="https://roife.github.io" target="_blank">Roife</a></li>
                    
                        <li><a href="https://coekjan.cn" target="_blank">Coekjan</a></li>
                    
                        <li><a href="https://www.dusign.net/" target="_blank">Dusign</a></li>
                    
                </ul>
                
            </div>
        </div>
    </div>
</article>
<script src="https://utteranc.es/client.js"
        repo="buaadreamer/buaadreamer.github.io"
        issue-term="pathname"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script>



<!-- async load function -->
<script>
    function async(u, c) {
      var d = document, t = 'script',
          o = d.createElement(t),
          s = d.getElementsByTagName(t)[0];
      o.src = u;
      if (c) { o.addEventListener('load', function (e) { c(null, e); }, false); }
      s.parentNode.insertBefore(o, s);
    }
</script>
<!-- anchor-js, Doc:http://bryanbraun.github.io/anchorjs/ -->
<script>
    async("https://cdn.bootcss.com/anchor-js/1.1.1/anchor.min.js",function(){
        anchors.options = {
          visible: 'hover',
          placement: 'left',
          icon: 'ℬ'
        };
        anchors.add().remove('.intro-header h1').remove('.subheading').remove('.sidebar-container h5');
    })
</script>


<style  type="text/css">
    /* place left on bigger screen */
    @media all and (min-width: 800px) {
        .anchorjs-link{
            position: absolute;
            left: -0.75em;
            font-size: 1.1em;
            margin-top : -0.1em;
        }
    }
</style>



    <!-- Footer -->
    <!-- Footer -->
<footer>
    <div class="container">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                <ul class="list-inline text-center">

                
                    <li>
                        <a target="_blank"  href="https://github.com/BUAADreamer">
                            <span class="fa-stack fa-lg">
                                <i class="fa fa-circle fa-stack-2x"></i>
                                <i class="fa fa-github fa-stack-1x fa-inverse"></i>
                            </span>
                        </a>
                    </li>
                

                

                

                

                

                

                

                </ul>
                <p class="copyright text-muted">
                    Copyright &copy; BUAADreamer 2024 
                    <br>
                    Powered by 
                    <a href="https://github.com/dusign/hexo-theme-snail" target="_blank" rel="noopener">
                        <i>hexo-theme-snail</i>
                    </a> | 
                    <iframe name="star" style="margin-left: 2px; margin-bottom:-5px;" frameborder="0" scrolling="0"
                        width="100px" height="20px"
                        src="https://ghbtns.com/github-btn.html?user=dusign&repo=hexo-theme-snail&type=star&count=true">
                    </iframe>
                </p>
            </div>
        </div>
    </div>

</footer>

<!-- jQuery -->

<script src="/js/jquery.min.js"></script>


<!-- Bootstrap Core JavaScript -->

<script src="/js/bootstrap.min.js"></script>


<!-- Custom Theme JavaScript -->

<script src="/js/hux-blog.min.js"></script>


<!-- Search -->

<script src="/js/search.js"></script>


<!-- async load function -->
<script>
    function async(u, c) {
      var d = document, t = 'script',
          o = d.createElement(t),
          s = d.getElementsByTagName(t)[0];
      o.src = u;
      if (c) { o.addEventListener('load', function (e) { c(null, e); }, false); }
      s.parentNode.insertBefore(o, s);
    }
</script>


<!-- jquery.tagcloud.js -->
<script>
    // only load tagcloud.js in tag.html
    if($('#tag_cloud').length !== 0){
        async("https://BUAADreamer.top/js/jquery.tagcloud.js",function(){
            $.fn.tagcloud.defaults = {
                //size: {start: 1, end: 1, unit: 'em'},
                color: {start: '#bbbbee', end: '#0085a1'},
            };
            $('#tag_cloud a').tagcloud();
        })
    }
</script>

<!--fastClick.js -->
<script>
    async("https://cdn.bootcss.com/fastclick/1.0.6/fastclick.min.js", function(){
        var $nav = document.querySelector("nav");
        if($nav) FastClick.attach($nav);
    })
</script>


<!-- Google Analytics -->


<script>
    // dynamic User by Hux
    var _gaId = 'UA-XXXXXXXX-X';
    var _gaDomain = 'yoursite';

    // Originial
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

    ga('create', _gaId, _gaDomain);
    ga('send', 'pageview');
</script>




<!-- Baidu Tongji -->


<!-- Search -->

    <script type="text/javascript">      
        var search_path = "search.xml";
        if (search_path.length == 0) {
            search_path = "search.xml";
        }
    var path = "/" + search_path;
    searchFunc(path, 'local-search-input', 'local-search-result');
    </script>


<!-- busuanzi -->
<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>


  <script src='https://unpkg.com/mermaid@7.1.2/dist/mermaid.min.js'></script>
  <script>
    if (window.mermaid) {
      mermaid.initialize({theme: 'forest'});
    }
  </script>








	<a id="rocket" href="#top" class=""></a>
	<script type="text/javascript" src="/js/totop.js?v=1.0.0" async=""></script>
    <script type="text/javascript" src="/js/toc.js?v=1.0.0" async=""></script>

    
        <!-- background effects line -->
        

        
            <script type="text/javascript" src="/js/mouse-click.js" content='[&quot;🌱&quot;,&quot;just do it&quot;,&quot;🍀&quot;]' color='[&quot;rgb(121,93,179)&quot; ,&quot;rgb(76,180,231)&quot; ,&quot;rgb(184,90,154)&quot;]'></script>
        

        <!-- background effects end -->
    

    <!--<script size="50" alpha='0.3' zIndex="-999" src="/js/ribbonStatic.js"></script>-->
    
        <script src="/js/ribbonDynamic.js"></script>
    
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</body>

</html>
