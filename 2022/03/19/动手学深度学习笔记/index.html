<!DOCTYPE html>
<html lang="en">

<!-- Head tag -->
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="google-site-verification" content="xBT4GhYoi5qRD5tr338pgPM5OWHHIDR6mNg1a3euekI" />
    <meta name="baidu-site-verification" content="093lY4ziMu" />
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
    <meta name="description" content="A hexo theme">
    <meta name="keyword"  content="BUAADreamer, hexo-theme-snail">
    <link rel="shortcut icon" href="/img/ironman-draw.png">
    <!-- Place this tag in your head or just before your close body tag. -->
    <script async defer src="https://buttons.github.io/buttons.js"></script>
    <!--<link href='http://fonts.googleapis.com/css?family=Montserrat:400,700' rel='stylesheet' type='text/css'>-->
    <title>
        
          动手学深度学习笔记 - BUAADreamer&#39;s Blog
        
    </title>

    <link rel="canonical" href="https://BUAADreamer.top/2022/03/19/动手学深度学习笔记/">

    <!-- Bootstrap Core CSS -->
    
<link rel="stylesheet" href="/css/bootstrap.min.css">


    <!-- Custom CSS --> 
    
        
<link rel="stylesheet" href="/css/dusign-light.css">

        
<link rel="stylesheet" href="/css/dusign-common-light.css">

        
<link rel="stylesheet" href="/css/font-awesome.css">

        
<link rel="stylesheet" href="/css/toc.css">

        <!-- background effects end -->
    
    
    <!-- Pygments Highlight CSS -->
    
<link rel="stylesheet" href="/css/highlight.css">


    
<link rel="stylesheet" href="/css/widget.css">


    
<link rel="stylesheet" href="/css/rocket.css">


    
<link rel="stylesheet" href="/css/signature.css">


    
<link rel="stylesheet" href="/css/fonts.googleapis.css">


    <link rel="stylesheet" href="//cdn.bootcss.com/font-awesome/4.3.0/css/font-awesome.min.css">

    <!-- photography -->
    
<link rel="stylesheet" href="/css/photography.css">


    <!-- ga & ba script hoook -->
    <script></script>
<meta name="generator" content="Hexo 4.2.1"></head>


<!-- hack iOS CSS :active style -->
<body ontouchstart="">

    <!-- background effects start -->
    
    <!-- background effects end -->

	<!-- Modified by Yu-Hsuan Yen -->
<!-- Post Header -->
<style type="text/css">
    header.intro-header{
        
            
                background-image: linear-gradient(rgba(0, 0, 0, 0.3), rgba(0, 0, 0, 0.3)), url('../../../../img/default.jpg')
                /*post*/
            
        
    }
    
    #signature{
        background-image: url('/img/signature/dusign.png');
    }
    
</style>

<header class="intro-header" >
    <!-- Signature -->
    <div id="signature">
        <div class="container">
            <div class="row">
                <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                
                    <div class="post-heading">
                        <div class="tags">
                            
                              <a class="tag" href="/tags/#DeepLearning" title="DeepLearning">DeepLearning</a>
                            
                        </div>
                        <h1>动手学深度学习笔记</h1>
                        <h2 class="subheading"></h2>
                        <span class="meta">
                            Posted by BUAADreamer on
                            2022-03-19
                        </span>

                        
                            <div class="blank_box"></div>
                            <span class="meta">
                                Words <span class="post-count">6.4k</span> and
                                Reading Time <span class="post-count">29</span> Minutes
                            </span>
                            <div class="blank_box"></div>
                            <!-- 不蒜子统计 start -->
                            <span class="meta">
                                Viewed <span id="busuanzi_value_page_pv"><i class="fa fa-spinner fa-spin"></i></span> Times
                            </span>
                            <!-- 不蒜子统计 end -->
                        

                    </div>
                

                </div>
            </div>
        </div>      
    </div>

    
    <div class="waveWrapper">
        <div class="wave wave_before" style="background-image: url('/img/wave-light.png')"></div>
        <div class="wave wave_after" style="background-image: url('/img/wave-light.png')"></div>
    </div>
    
</header>

	
    <!-- Navigation -->
<nav class="navbar navbar-default navbar-custom navbar-fixed-top">
    <div class="container-fluid">
        <!-- Brand and toggle get grouped for better mobile display -->
        <div class="navbar-header page-scroll">
            <button type="button" class="navbar-toggle">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="/">BUAADreamer&#39;s Blog</a>
        </div>

        <!-- Collect the nav links, forms, and other content for toggling -->
        <!-- Known Issue, found by Hux:
            <nav>'s height woule be hold on by its content.
            so, when navbar scale out, the <nav> will cover tags.
            also mask any touch event of tags, unfortunately.
        -->
        <div id="huxblog_navbar">
            <div class="navbar-collapse">
                <ul class="nav navbar-nav navbar-right">
                    <li>
                        <a href="/">Home</a>
                    </li>

                    

                        
                    

                        
                        <li>
                            <a href="/about/">About</a>
                        </li>
                        
                    

                        
                        <li>
                            <a href="/archive/">Archives</a>
                        </li>
                        
                    

                        
                        <li>
                            <a href="/categories/">Categories</a>
                        </li>
                        
                    

                        
                        <li>
                            <a href="/tags/">Tags</a>
                        </li>
                        
                    

                        
                        <li>
                            <a href="/photography/">Photography</a>
                        </li>
                        
                    
                    
                    
                    <li>
                        <a href="https://www.cnblogs.com/BUAADreamer/" target="_blank">Chinese Blog</a>
                    </li>
                    
                </ul>
            </div>
        </div>
        <!-- /.navbar-collapse -->
    </div>
    <!-- /.container -->
</nav>
<script>
    // Drop Bootstarp low-performance Navbar
    // Use customize navbar with high-quality material design animation
    // in high-perf jank-free CSS3 implementation
    var $body   = document.body;
    var $toggle = document.querySelector('.navbar-toggle');
    var $navbar = document.querySelector('#huxblog_navbar');
    var $collapse = document.querySelector('.navbar-collapse');

    $toggle.addEventListener('click', handleMagic)
    function handleMagic(e){
        if ($navbar.className.indexOf('in') > 0) {
        // CLOSE
            $navbar.className = " ";
            // wait until animation end.
            setTimeout(function(){
                // prevent frequently toggle
                if($navbar.className.indexOf('in') < 0) {
                    $collapse.style.height = "0px"
                }
            },400)
        }else{
        // OPEN
            $collapse.style.height = "auto"
            $navbar.className += " in";
        }
    }
</script>


    <!-- Main Content -->
    <!-- Post Content -->
<article>
    <div class="container">
        <div class="row">
            
            <!-- Post Container -->
            <div class="
                col-lg-8 col-lg-offset-2
                col-md-10 col-md-offset-1
                post-container">

                <h1><span id="1前言">1.前言</span></h1>
<h2><span id="日常生活中的机器学习">日常生活中的机器学习</span></h2>
<h2><span id="关键组件">关键组件</span></h2>
<p><strong>数据</strong>，<strong>转换数据的模型</strong>，<strong>目标函数</strong>，调整模型参数从而优化目标函数的<strong>算法</strong></p>
<h2><span id="各种机器学习问题">各种机器学习问题</span></h2>
<h3><span id="监督学习supervised-learning">监督学习（supervised learning）</span></h3>
<ul>
<li>回归（regression）</li>
<li>分类（classification）
<ul>
<li>层次分类</li>
</ul>
</li>
<li>标记问题
<ul>
<li>二元分类</li>
<li>多元分类 ==&gt; 多标签分类（multi-label classification）</li>
</ul>
</li>
<li>搜索</li>
<li>推荐系统（recommender system）</li>
<li>序列学习
<ul>
<li>标记与解析</li>
<li>自动语音识别</li>
<li>文本到语音</li>
<li>机器翻译</li>
</ul>
</li>
</ul>
<h3><span id="无监督学习unsupervised-learning">无监督学习（unsupervised learning）</span></h3>
<ul>
<li>聚类（clustering）</li>
<li>主成分分析（principal component analysis）</li>
<li>因果关系（causality）和概率图模型（probabilistic graphical models）</li>
<li>生成对抗性网络（generative adversarial networks）</li>
</ul>
<h3><span id="与环境互动">与环境互动</span></h3>
<p>前面两者都是离线学习（offline learning），即不需要和环境交互，先训练好再启动</p>
<h3><span id="强化学习reinforcement-learning">强化学习（reinforcement learning）</span></h3>
<ul>
<li>深度Q网络（Q-network）</li>
<li>AlphaGo</li>
<li>在强化学习问题中，agent在一系列的时间步骤上与环境交互。 在每个特定时间点，agent从环境接收一些观察（observation），并且必须选择一个动作（action），然后通过某种机制（有时称为执行器）将其传输回环境，最后agent从环境中获得奖励（reward）。 此后新一轮循环开始，agent接收后续观察，并选择后续操作，依此类推。</li>
</ul>
<h2><span id="起源">起源</span></h2>
<h2><span id="深度学习之路">深度学习之路</span></h2>
<h2><span id="成功案例">成功案例</span></h2>
<h2><span id="特点">特点</span></h2>
<h1><span id="2预备知识">2.预备知识</span></h1>
<h2><span id="21数据操作">2.1数据操作</span></h2>
<p>张量tensor，<code>pytorch</code> 和 <code>tensorflow</code> 中是 <code>Tensor</code>，其实就是<strong>多维数组</strong></p>
<h3><span id="入门">入门</span></h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">x=torch.arange(<span class="number">12</span>)</span><br><span class="line">x.shape</span><br><span class="line">x.numel <span class="comment">#将多维数组展平成一维数组的长度，即张量的元素个数</span></span><br><span class="line">X=x.reshape(<span class="number">3</span>,<span class="number">4</span>) <span class="comment">#改变张量的形状但不改元素数量和元素值</span></span><br><span class="line">X=x.reshape(<span class="number">-1</span>,<span class="number">4</span>) <span class="comment">#使用-1自动计算对应位置的大小</span></span><br><span class="line">X=torch.ones((<span class="number">2</span>,<span class="number">3</span>))</span><br><span class="line">X=torch.zeros(<span class="number">10</span>) </span><br><span class="line">X=torch.randn(<span class="number">3</span>,<span class="number">4</span>) <span class="comment">#shape为(3,4)的一个随机数张量，元素分布遵循N(0,1)</span></span><br><span class="line">X=torch.tensor([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>]) <span class="comment">#根据列表建立</span></span><br></pre></td></tr></table></figure>
<h3><span id="运算符">运算符</span></h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#按元素运算</span></span><br><span class="line">x=torch.tensor([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>])</span><br><span class="line">y=torch.tensor([<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>])</span><br><span class="line">x+y,x-y,x*y,x/y,x**y <span class="comment">#张量每个对应位置元素计算得到相同形状的张量</span></span><br><span class="line">torch.exp(x) <span class="comment">#求幂</span></span><br><span class="line"><span class="comment">#连结张量</span></span><br><span class="line">X=torch.arange(<span class="number">6</span>,dtype=torch.float32).reshape(<span class="number">2</span>,<span class="number">3</span>)</span><br><span class="line">Y=torch.tensor([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],[<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>]])</span><br><span class="line">torch.cat((X,Y),dim=<span class="number">0</span>),torch.cat((X,Y),dim=<span class="number">1</span>) </span><br><span class="line"><span class="comment">#dim=0 shape的第一个元素，即行上进行cat，会影响shape的第一个元素的长度</span></span><br><span class="line"><span class="comment">#dim=1 shape的第二个元素，即列上进行cat，会影响shape的第二个元素的长度</span></span><br><span class="line">X==Y <span class="comment">#产生一个X.shape的布尔数组</span></span><br><span class="line">X.sum(dim=<span class="literal">None</span>, keepdim=<span class="literal">False</span>, dtype=<span class="literal">None</span>) <span class="comment">#默认求所有元素的和 dim设为0和1解释同上</span></span><br></pre></td></tr></table></figure>
<h3><span id="广播机制">广播机制</span></h3>
<p>先复制元素扩展，再对同样形状的数组进行按元素操作</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">a=torch.arange(<span class="number">3</span>).reshape(<span class="number">3</span>,<span class="number">1</span>)</span><br><span class="line">b=torch.arange(<span class="number">2</span>).reshape(<span class="number">1</span>,<span class="number">2</span>)</span><br><span class="line">a+b</span><br><span class="line"><span class="comment">#先将a复制扩充成[[0,0]，[1,1],[2,2]]</span></span><br><span class="line"><span class="comment">#再将b复制扩充成[[0,1],[0,1],[0,1]]</span></span><br><span class="line"><span class="comment">#再将扩充后的数组相加得到[[0,1],[1,2],[2,3]]</span></span><br></pre></td></tr></table></figure>
<h3><span id="索引和切片">索引和切片</span></h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">X[<span class="number">-1</span>],x[<span class="number">1</span>:<span class="number">3</span>] <span class="comment">#此处和python索引切片一致</span></span><br><span class="line">X[<span class="number">1</span>,<span class="number">2</span>]=<span class="number">3</span></span><br><span class="line">X[<span class="number">0</span>:<span class="number">2</span>,:]=<span class="number">1</span></span><br></pre></td></tr></table></figure>
<h3><span id="节省内存">节省内存</span></h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">before = id(Y)</span><br><span class="line">Y = Y + X</span><br><span class="line">id(Y) == before <span class="comment">#False 重新为上面的计算左侧的Y分配了空间</span></span><br><span class="line">before = id(Y)</span><br><span class="line">Y[:]=Y+X</span><br><span class="line">id(Y) == before <span class="comment">#True 原址改动</span></span><br></pre></td></tr></table></figure>
<h3><span id="转换为其他python对象">转换为其他python对象</span></h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">X=torch.tensor([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>])</span><br><span class="line">A = X.numpy()</span><br><span class="line">B = torch.tensor(A)</span><br><span class="line">B = torch.from_numpy(A)</span><br></pre></td></tr></table></figure>
<h2><span id="22数据预处理">2.2数据预处理</span></h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#pandas基本数据处理</span></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line">os.makedirs(os.path.join(<span class="string">'..'</span>, <span class="string">'data'</span>), exist_ok=<span class="literal">True</span>)</span><br><span class="line">data_file = os.path.join(<span class="string">'..'</span>, <span class="string">'data'</span>, <span class="string">'house_tiny.csv'</span>)</span><br><span class="line"><span class="comment">#写入数据集</span></span><br><span class="line"><span class="keyword">with</span> open(data_file, <span class="string">'w'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    f.write(<span class="string">'NumRooms,Alley,Price\n'</span>)  <span class="comment"># 列名</span></span><br><span class="line">    f.write(<span class="string">'NA,Pave,127500\n'</span>)  <span class="comment"># 每行表示一个数据样本</span></span><br><span class="line">    f.write(<span class="string">'2,NA,106000\n'</span>)</span><br><span class="line">    f.write(<span class="string">'4,NA,178100\n'</span>)</span><br><span class="line">    f.write(<span class="string">'NA,NA,140000\n'</span>)</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">df=pd.read_csv(data_file)</span><br><span class="line">inputs,outputs=data.iloc[:,<span class="number">0</span>:<span class="number">2</span>],data.iloc[:,<span class="number">2</span>]</span><br><span class="line">inputs = inputs.fillna(inputs.mean()) <span class="comment">#填补残缺值</span></span><br><span class="line">inputs = pd.get_dummies(inputs, dummy_na=<span class="literal">True</span>) <span class="comment">#将离散值转换为数值类型</span></span><br><span class="line">X, y = torch.tensor(inputs.values), torch.tensor(outputs.values) <span class="comment">#转换为张量格式</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">drop_col</span><span class="params">(df)</span>:</span></span><br><span class="line">    num = df.isna().sum() <span class="comment">#获得缺失值统计信息</span></span><br><span class="line">    num_dict = num.to_dict() <span class="comment">#转为字典</span></span><br><span class="line">    max_key =max(num_dict,key=num_dict.get) <span class="comment">#取字典中最大值的键</span></span><br><span class="line">    <span class="keyword">del</span> df[max_key] <span class="comment">#删除缺失值最多的列</span></span><br><span class="line">    <span class="keyword">return</span> df</span><br></pre></td></tr></table></figure>
<h2><span id="23线性代数">2.3线性代数</span></h2>
<h3><span id="标量单个数">标量：单个数</span></h3>
<p>x,y,z</p>
<h3><span id="向量标量组成的列表">向量：标量组成的列表</span></h3>
<p><strong>x</strong>,<strong>y</strong>,<strong>z</strong></p>
<h3><span id="矩阵标量组成的二维数组">矩阵：标量组成的二维数组</span></h3>
<p>A,B,C</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">A.T</span><br><span class="line">A==A.T <span class="comment">#看是否是对称矩阵</span></span><br></pre></td></tr></table></figure>
<h3><span id="张量标量组成的多维数组">张量：标量组成的多维数组</span></h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">X=torch.range(<span class="number">24</span>).reshape(<span class="number">3</span>,<span class="number">3</span>,<span class="number">4</span>) <span class="comment">#可认为是三颜色通道，尺寸为3*4的图片数据</span></span><br><span class="line">A = torch.arange(<span class="number">20</span>, dtype=torch.float32).reshape(<span class="number">5</span>, <span class="number">4</span>)</span><br><span class="line">B = A.clone()</span><br><span class="line">A*B <span class="comment">#按元素乘法 Hadamard积（Hadamard product）（数学符号⊙）</span></span><br><span class="line">A+<span class="number">1</span> <span class="comment">#每个元素都+1</span></span><br><span class="line">A*<span class="number">2</span> <span class="comment">#每个元素*2</span></span><br></pre></td></tr></table></figure>
<h3><span id="降维">降维</span></h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x=torch.arange(<span class="number">6</span>,dtype=torch.float32).reshape(<span class="number">2</span>,<span class="number">3</span>)</span><br><span class="line">x.sum(axis=<span class="number">0</span>) <span class="comment">#==&gt;[5,7,9]</span></span><br><span class="line">x.sum(axis=<span class="number">1</span>) <span class="comment">#==&gt;[6,15]</span></span><br><span class="line"><span class="comment">#其他函数比如mean也有类似性质</span></span><br></pre></td></tr></table></figure>
<h3><span id="非降维求和">非降维求和</span></h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">x=torch.arange(<span class="number">6</span>,dtype=torch.float32).reshape(<span class="number">2</span>,<span class="number">3</span>)</span><br><span class="line">sum_x = x.sum(axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line">sum_x <span class="comment">#==&gt;[[6],[15]]</span></span><br><span class="line"><span class="comment">#这样可以直接用x/sum_x进行标准化</span></span><br><span class="line">x/sum_x <span class="comment">#==&gt;[[0.0000, 0.3333, 0.6667],[0.2500, 0.3333, 0.4167]</span></span><br><span class="line">x.cumsum(axis=<span class="number">0</span>) <span class="comment">#==&gt;累计总和:[[0., 1., 2.],[3., 5., 7.]]</span></span><br></pre></td></tr></table></figure>
<h3><span id="点积">点积</span></h3>
<p>$x,y\in R<sup>d,x</sup>Ty=\sum_{i=1}^dx_iy_i$</p>
<p>加权和，计算夹角余弦</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">x=torch.range(<span class="number">4</span>)</span><br><span class="line">y=torch.ones(<span class="number">4</span>,dtype=torch.float32)</span><br><span class="line"><span class="comment">#两种计算方式</span></span><br><span class="line">torch.dot(x,y)</span><br><span class="line">torch.sum(x*y)</span><br></pre></td></tr></table></figure>
<h3><span id="矩阵-向量积">矩阵-向量积</span></h3>
<p>$A\in R^{m\times n},x\in R^n$</p>
<p><img src="/2022/03/19/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1.png" alt></p>
<p>相乘得到$B^m$ 即长度为m的向量</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">A=torch.arange(<span class="number">6</span>).reshape(<span class="number">2</span>,<span class="number">3</span>)</span><br><span class="line">x=torch.tensor([<span class="number">2</span>,<span class="number">4</span>,<span class="number">5</span>])</span><br><span class="line"><span class="comment">#2*3的矩阵与长度为3的向量</span></span><br><span class="line">torch.mv(A,x) <span class="comment">#==&gt;[14,47]</span></span><br></pre></td></tr></table></figure>
<h3><span id="矩阵乘法">矩阵乘法</span></h3>
<p>$A\in R^{n\times k},B\in R^{k\times m}$</p>
<p>$AB\in R^{n\times m}$</p>
<p>可以理解为是A的<strong>n个行向量</strong>与B的<strong>m个列向量</strong>分别相乘</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">A=torch.range(<span class="number">15</span>).reshape(<span class="number">5</span>,<span class="number">3</span>)</span><br><span class="line">B=torch.range(<span class="number">12</span>).reshape(<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line">torch.mm(A,B)</span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">		[[ 20,  23,  26,  29],</span></span><br><span class="line"><span class="string">        [ 56,  68,  80,  92],</span></span><br><span class="line"><span class="string">        [ 92, 113, 134, 155],</span></span><br><span class="line"><span class="string">        [128, 158, 188, 218],</span></span><br><span class="line"><span class="string">        [164, 203, 242, 281]]</span></span><br><span class="line"><span class="string">'''</span></span><br></pre></td></tr></table></figure>
<h3><span id="范数-norm">范数 norm</span></h3>
<p>将向量<strong>x</strong>映射到标量的函数$f$</p>
<p>满足三个性质</p>
<ul>
<li>$f(\alpha x)=|\alpha|f(x)$</li>
<li>$f(x+y)\leq f(x)+f(y)$，三角不等式</li>
<li>$f(x)\ge 0$，当且仅当<strong>x</strong>分量全为0时相等</li>
</ul>
<p>$L_2$范数：$||x||<em>2=\sqrt{(\sum</em>{i=1}<sup>nx_i</sup>2)}$ 一般可以简写为$||x||$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">u=torch.tensor([<span class="number">3.0</span>,<span class="number">4.0</span>])</span><br><span class="line">torch.norm(u) <span class="comment">#==&gt;[5.0]</span></span><br></pre></td></tr></table></figure>
<p>$L_1$范数：$||x||<em>1=\sum</em>{i=1}^n|x_i|$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">u=torch.tensor([<span class="number">3.0</span>,<span class="number">4.0</span>])</span><br><span class="line">torch.abs(u).sum() <span class="comment">#==&gt;[7.0]</span></span><br></pre></td></tr></table></figure>
<p>$L_p$范数：$||x||<em>p=(\sum</em>{i=1}<sup>n|x_i|</sup>p)^{1/p}$</p>
<p>矩阵的$Frobenius$范数（Frobenius norm）是矩阵元素平方和的平方根</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.norm(torch.ones((<span class="number">4</span>, <span class="number">9</span>))) <span class="comment">#==&gt;tensor(6.)</span></span><br></pre></td></tr></table></figure>
<p>深度学习中的目标常表达为范数</p>
<h2><span id="24微积分">2.4微积分</span></h2>
<h2><span id="25自动微分">2.5自动微分</span></h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#标量计算梯度</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">x = torch.arange(<span class="number">4.0</span>)</span><br><span class="line">x.requires_grad_(<span class="literal">True</span>)</span><br><span class="line">y=<span class="number">10</span>*torch.dot(x,x)</span><br><span class="line">y.backward()</span><br><span class="line">x.grad  <span class="comment">#[0.,4.,8.,12.]</span></span><br><span class="line">x.grad==<span class="number">20</span>*x </span><br><span class="line"></span><br><span class="line">x.grad.zero_() <span class="comment">#清除梯度</span></span><br><span class="line">y = x.sum()</span><br><span class="line">y.backward()</span><br><span class="line">x.grad <span class="comment">#[1.,1.,1.,1.]</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#非标量变量的反向传播</span></span><br><span class="line">x.grad.zero_()</span><br><span class="line">y = x * x</span><br><span class="line">y.sum().backward() <span class="comment">#等价于y.backward(torch.ones(len(x)))</span></span><br><span class="line">x.grad <span class="comment">#相当于2x 即[0,2,4,6]</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#分离计算</span></span><br><span class="line">x.grad.zero_()</span><br><span class="line">y = x * x </span><br><span class="line">u = y.detach() <span class="comment">#将u与x进行分离，即作为常数参与之后的运算</span></span><br><span class="line">z = u * x </span><br><span class="line">z.sum().backward() </span><br><span class="line">x.grad == u <span class="comment">#是一样的</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#Python控制流的梯度计算</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span><span class="params">(a)</span>:</span></span><br><span class="line">	b = a * <span class="number">2</span></span><br><span class="line">	<span class="keyword">while</span> b.norm() &lt; <span class="number">1000</span>: </span><br><span class="line">        b = b * <span class="number">2</span></span><br><span class="line">	<span class="keyword">if</span> b.sum() &gt; <span class="number">0</span>: c = b</span><br><span class="line">	<span class="keyword">else</span>:c = <span class="number">100</span> * b</span><br><span class="line">	<span class="keyword">return</span> c</span><br><span class="line">a = torch.randn(size=(), requires_grad=<span class="literal">True</span>) </span><br><span class="line">d = f(a)</span><br><span class="line">d.backward()</span><br></pre></td></tr></table></figure>
<h2><span id="26概率">2.6概率</span></h2>
<h2><span id="27查阅文档">2.7查阅⽂档</span></h2>
<h1><span id="3线性神经网络lnn">3.线性神经网络（LNN）</span></h1>
<h2><span id="线性回归">线性回归</span></h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils <span class="keyword">import</span> data</span><br><span class="line"><span class="comment">#生成原始数据</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">synthetic_data</span><span class="params">(w, b, num_examples)</span>:</span></span><br><span class="line">    <span class="string">"""生成y=Xw+b+噪声</span></span><br><span class="line"><span class="string">    Defined in :numref:`sec_linear_scratch`"""</span></span><br><span class="line">    X = torch.normal(<span class="number">0</span>, <span class="number">1</span>, (num_examples, len(w)))</span><br><span class="line">    y = torch.matmul(X, w) + b</span><br><span class="line">    y += torch.normal(<span class="number">0</span>, <span class="number">0.01</span>, y.shape)</span><br><span class="line">    <span class="keyword">return</span> X, torch.reshape(y, (<span class="number">-1</span>, <span class="number">1</span>))</span><br><span class="line">true_w = torch.tensor([<span class="number">2</span>, <span class="number">-3.4</span>])</span><br><span class="line">true_b = <span class="number">4.2</span></span><br><span class="line">features, labels = synthetic_data(true_w, true_b, <span class="number">1000</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#定义加载数据方法，获得迭代器</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_array</span><span class="params">(data_arrays, batch_size, is_train=True)</span>:</span> </span><br><span class="line">    <span class="string">"""构造一个PyTorch数据迭代器。"""</span></span><br><span class="line">    dataset = data.TensorDataset(*data_arrays)</span><br><span class="line">    <span class="keyword">return</span> data.DataLoader(dataset, batch_size, shuffle=is_train)</span><br><span class="line">batch_size = <span class="number">10</span></span><br><span class="line">data_iter = load_array((features, labels), batch_size)</span><br><span class="line"></span><br><span class="line"><span class="comment">#定义网络</span></span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line">net = nn.Sequential(nn.Linear(<span class="number">2</span>, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">#定义初始权重</span></span><br><span class="line">net[<span class="number">0</span>].weight.data.normal_(<span class="number">0</span>, <span class="number">0.01</span>)</span><br><span class="line">net[<span class="number">0</span>].bias.data.fill_(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#定义损失函数与优化方法</span></span><br><span class="line">loss = nn.MSELoss() <span class="comment">#2范数损失的平均值</span></span><br><span class="line">trainer = torch.optim.SGD(net.parameters(), lr=<span class="number">0.03</span>) <span class="comment">#net.parameters()指定需要优化的参数，同时还需要传入优化算法需要的超参数字典，对于小批量随机梯度下降，这里是lr=0.03</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#开始训练</span></span><br><span class="line">num_epochs = <span class="number">3</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(num_epochs):</span><br><span class="line">  <span class="keyword">for</span> X, y <span class="keyword">in</span> data_iter:</span><br><span class="line">      <span class="comment">#前向计算</span></span><br><span class="line">    l = loss(net(X) ,y)</span><br><span class="line">    <span class="comment">#清除梯度</span></span><br><span class="line">    trainer.zero_grad()</span><br><span class="line">    <span class="comment">#反向传播</span></span><br><span class="line">    l.backward()</span><br><span class="line">    <span class="comment">#优化器根据梯度进行梯度计算更新</span></span><br><span class="line">    trainer.step()</span><br><span class="line">    <span class="comment">#计算打印损失</span></span><br><span class="line">    l = loss(net(features), labels)</span><br><span class="line">  print(<span class="string">f'epoch <span class="subst">&#123;epoch + <span class="number">1</span>&#125;</span>, loss <span class="subst">&#123;l:f&#125;</span>'</span>)</span><br></pre></td></tr></table></figure>
<h2><span id="softmax回归">softmax回归</span></h2>
<p>$softmax$ 运算：$y_j=\frac {exp(o_j)} {\sum_{k=1}^nexp(o_k)}$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torchvision,torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line"></span><br><span class="line"><span class="comment">#获取输入</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_dataloader_workers</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">"""使用4个进程来读取数据"""</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">4</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_data_fashion_mnist</span><span class="params">(batch_size, resize=None)</span>:</span></span><br><span class="line">    <span class="string">"""下载Fashion-MNIST数据集，然后将其加载到内存中"""</span></span><br><span class="line">    trans = [transforms.ToTensor()]</span><br><span class="line">    <span class="keyword">if</span> resize:</span><br><span class="line">        trans.insert(<span class="number">0</span>, transforms.Resize(resize))</span><br><span class="line">    trans = transforms.Compose(trans)</span><br><span class="line">    mnist_train = torchvision.datasets.FashionMNIST(</span><br><span class="line">        root=<span class="string">"../data"</span>, train=<span class="literal">True</span>, transform=trans, download=<span class="literal">True</span>)</span><br><span class="line">    mnist_test = torchvision.datasets.FashionMNIST(</span><br><span class="line">        root=<span class="string">"../data"</span>, train=<span class="literal">False</span>, transform=trans, download=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> (data.DataLoader(mnist_train, batch_size, shuffle=<span class="literal">True</span>,</span><br><span class="line">                            num_workers=get_dataloader_workers()),</span><br><span class="line">            data.DataLoader(mnist_test, batch_size, shuffle=<span class="literal">False</span>,</span><br><span class="line">                            num_workers=get_dataloader_workers()))</span><br><span class="line">batch_size = <span class="number">256</span></span><br><span class="line">train_iter, test_iter = load_data_fashion_mnist(batch_size)</span><br><span class="line"></span><br><span class="line"><span class="comment"># PyTorch不会隐式地调整输⼊的形状。因此，</span></span><br><span class="line"><span class="comment"># 我们在线性层前定义了展平层（flatten），来调整⽹络输⼊的形状</span></span><br><span class="line">net = nn.Sequential(nn.Flatten(), nn.Linear(<span class="number">784</span>, <span class="number">10</span>))</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">init_weights</span><span class="params">(m)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> type(m) == nn.Linear:</span><br><span class="line">    	nn.init.normal_(m.weight, std=<span class="number">0.01</span>)</span><br><span class="line">net.apply(init_weights)</span><br><span class="line"><span class="comment">#损失函数为交叉熵</span></span><br><span class="line">loss = nn.CrossEntropyLoss(reduction=<span class="string">'none'</span>)</span><br><span class="line">trainer = torch.optim.SGD(net.parameters(), lr=<span class="number">0.1</span>)</span><br><span class="line">num_epochs = <span class="number">10</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(num_epochs):</span><br><span class="line">    <span class="keyword">for</span> X,y <span class="keyword">in</span> train_iter:</span><br><span class="line">      <span class="comment">#前向计算</span></span><br><span class="line">      l=loss(net(X),y)</span><br><span class="line">      <span class="comment">#清除梯度</span></span><br><span class="line">      trainer.zero_grad()</span><br><span class="line">      <span class="comment">#反向传播</span></span><br><span class="line">      l.mean().backward()</span><br><span class="line">      <span class="comment">#更新权重</span></span><br><span class="line">      trainer.step()</span><br><span class="line">    loss_sum = <span class="number">0</span></span><br><span class="line">    cnt = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> X,y <span class="keyword">in</span> test_iter:</span><br><span class="line">      loss_sum+=loss(net(X),y).sum()</span><br><span class="line">      cnt+=<span class="number">1</span></span><br><span class="line">    print(<span class="string">"epoch%d"</span>%i,<span class="string">":"</span>,float(loss_sum/cnt))</span><br></pre></td></tr></table></figure>
<h1><span id="4多层感知机mlp">4.多层感知机（MLP）</span></h1>
<h2><span id="多层感知机">多层感知机</span></h2>
<h3><span id="实现">实现</span></h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#此处代码与第3章中softmax回归的代码除了net定义都是一样的</span></span><br><span class="line">net = nn.Sequential(</span><br><span class="line">    nn.Flatten(),</span><br><span class="line">    nn.Linear(<span class="number">784</span>,<span class="number">256</span>),</span><br><span class="line">    nn.ReLU(),</span><br><span class="line">    nn.Linear(<span class="number">256</span>,<span class="number">10</span>)</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<h3><span id="激活函数">激活函数</span></h3>
<h4><span id="relu函数">ReLU函数</span></h4>
<p>$ReLU(x)=max(x,0)$</p>
<h4><span id="sigmoid函数">sigmoid函数</span></h4>
<p>$sigmoid(x)=\frac {1} {1+exp(-x)}$</p>
<h4><span id="tanh函数">tanh函数</span></h4>
<p>$tanh(x)=\frac {1-exp(-2x)} {1+exp(-2x)}$</p>
<h2><span id="模型选择-欠拟合和过拟合">模型选择、欠拟合和过拟合</span></h2>
<h2><span id="权重衰减">权重衰减</span></h2>
<p>给权重加上惩罚项，也称为$L_2$正则化</p>
<p>最简单的方式：直接使用权重向量的某个范数来度量复杂性，比如二范数</p>
<p>即损失变为：$L(w,b)+\frac \lambda 2 ||\omega||^2$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#此处代码与第3章中softmax回归的代码除了trainer定义都是一样的</span></span><br><span class="line">wd=<span class="number">3</span></span><br><span class="line">trainer=torch.optim.SGD([</span><br><span class="line">	&#123;<span class="string">"params"</span>:net[<span class="number">0</span>].weight,<span class="string">'weight_decay'</span>:wd&#125;,</span><br><span class="line">	&#123;<span class="string">"params"</span>:net[<span class="number">0</span>].bias&#125;],lr=lr)</span><br></pre></td></tr></table></figure>
<h2><span id="暂退法">暂退法</span></h2>
<p>扰动或者随机抛弃一些隐藏层的输出（即下一层输入）降低影响</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#此处代码与第3章中softmax回归的代码除了net定义都是一样的</span></span><br><span class="line">net = nn.Sequential(</span><br><span class="line">    nn.Flatten(),</span><br><span class="line">    nn.ReLU(),    </span><br><span class="line">    nn.Dropout(<span class="number">0.2</span>) <span class="comment">#抛弃20%的隐藏层神经元</span></span><br><span class="line">    nn.Linear(<span class="number">784</span>,<span class="number">256</span>),</span><br><span class="line">    nn.ReLU(),</span><br><span class="line">    nn.Dropout(<span class="number">0.2</span>) <span class="comment">#抛弃20%的隐藏层神经元</span></span><br><span class="line">    nn.Linear(<span class="number">256</span>,<span class="number">10</span>)</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<h2><span id="前向传播反向传播与计算图">前向传播，反向传播与计算图</span></h2>
<h2><span id="数值稳定性和模型初始化">数值稳定性和模型初始化</span></h2>
<h2><span id="环境与分布偏移">环境与分布偏移</span></h2>
<h1><span id="5深度学习计算">5.深度学习计算</span></h1>
<h2><span id="层和块">层和块</span></h2>
<h3><span id="自定义块">自定义块</span></h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.nn.modules.flatten <span class="keyword">import</span> Flatten</span><br><span class="line"><span class="keyword">import</span> torchvision,torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> functional <span class="keyword">as</span> F</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MLP</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.hidden=nn.Linear(<span class="number">20</span>,<span class="number">256</span>)</span><br><span class="line">        self.output=nn.Linear(<span class="number">256</span>,<span class="number">10</span>)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self,X)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.output(F.relu(self.hidden(X)))</span><br><span class="line">mlp=MLP()</span><br><span class="line">X=torch.arange(<span class="number">20</span>,dtype=torch.float32)</span><br><span class="line">mlp(X)</span><br></pre></td></tr></table></figure>
<h3><span id="顺序块">顺序块</span></h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MySequential</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, *args)</span>:</span></span><br><span class="line">		super().__init__()</span><br><span class="line">		<span class="keyword">for</span> idx, module <span class="keyword">in</span> enumerate(args):</span><br><span class="line">            <span class="comment"># 这⾥，module是Module⼦类的⼀个实例。我们把它保存在'Module'类的成员</span></span><br><span class="line">            <span class="comment"># 变量_modules中。module的类型是OrderedDict</span></span><br><span class="line">            self._modules[str(idx)] = module</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, X)</span>:</span></span><br><span class="line">        <span class="comment"># OrderedDict保证了按照成员添加的顺序遍历它们</span></span><br><span class="line">        <span class="keyword">for</span> block <span class="keyword">in</span> self._modules.values():</span><br><span class="line">        	X = block(X)</span><br><span class="line">        <span class="keyword">return</span> X</span><br><span class="line">net = MySequential(nn.Linear(<span class="number">20</span>, <span class="number">256</span>), nn.ReLU(), nn.Linear(<span class="number">256</span>, <span class="number">10</span>))</span><br><span class="line">net(X)</span><br></pre></td></tr></table></figure>
<h3><span id="前向传播函数中执行代码">前向传播函数中执行代码</span></h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyForwardSequential</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, *args)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        <span class="comment"># 不计算梯度的随机权重参数。因此其在训练期间保持不变</span></span><br><span class="line">        self.rand_weight = torch.rand((<span class="number">20</span>, <span class="number">20</span>), requires_grad=<span class="literal">False</span>)</span><br><span class="line">        self.linear = nn.Linear(<span class="number">20</span>, <span class="number">20</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, X)</span>:</span></span><br><span class="line">        <span class="comment"># OrderedDict保证了按照成员添加的顺序遍历它们</span></span><br><span class="line">        X=self.linear(X)</span><br><span class="line">        X=F.relu(X)</span><br><span class="line">        X=self.linear(X)</span><br><span class="line">        <span class="comment">#控制流</span></span><br><span class="line">        <span class="keyword">while</span> X.sum()&gt;<span class="number">1</span>:</span><br><span class="line">            X/=<span class="number">2</span></span><br><span class="line">        <span class="keyword">return</span> X.sum()</span><br><span class="line">net = MyForwardSequential()</span><br><span class="line">net(X)</span><br></pre></td></tr></table></figure>
<h2><span id="参数管理">参数管理</span></h2>
<h3><span id="参数访问">参数访问</span></h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">net = nn.Sequential(nn.Linear(<span class="number">4</span>, <span class="number">8</span>), nn.ReLU(), nn.Linear(<span class="number">8</span>, <span class="number">1</span>))</span><br><span class="line">net[<span class="number">2</span>].state_dict()</span><br><span class="line">net[<span class="number">2</span>].bias</span><br><span class="line">net[<span class="number">2</span>].bias.data</span><br><span class="line">net[<span class="number">2</span>].bias.grad</span><br><span class="line">net[<span class="number">2</span>].weight</span><br><span class="line">net[<span class="number">2</span>].weight.data</span><br><span class="line">net[<span class="number">2</span>].weight.grad</span><br><span class="line"><span class="comment">#新的获取参数的方式</span></span><br><span class="line">print(*[(name, param.shape) <span class="keyword">for</span> name, param <span class="keyword">in</span> net[<span class="number">2</span>].named_parameters()])</span><br><span class="line">print(*[(name, param.shape) <span class="keyword">for</span> name, param <span class="keyword">in</span> net.named_parameters()])</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">('weight', torch.Size([1, 8])) ('bias', torch.Size([1]))</span></span><br><span class="line"><span class="string">('0.weight', torch.Size([8, 4])) ('0.bias', torch.Size([8])) ('2.weight', torch.Size([1, 8])) ('2.bias', torch.Size([1]))</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure>
<h3><span id="参数初始化">参数初始化</span></h3>
<h4><span id="内置初始化">内置初始化</span></h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">init_normal</span><span class="params">(m)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> type(m) == nn.Linear:</span><br><span class="line">        nn.init.normal_(m.weight, mean=<span class="number">0</span>, std=<span class="number">0.01</span>)</span><br><span class="line">        nn.init.zeros_(m.bias)</span><br><span class="line">        nn.init.constant_(m.weight,<span class="number">1</span>) <span class="comment">#设置为一个常数</span></span><br><span class="line">        nn.init.xavier_uniform_(m.weight) <span class="comment">#Xavier初始化⽅法 防止梯度消失或者地图爆炸</span></span><br><span class="line">net.apply(init_normal)  </span><br><span class="line"><span class="comment">#不同层使用不同初始化方法</span></span><br><span class="line">net[<span class="number">0</span>].apply(xavier)</span><br><span class="line">net[<span class="number">2</span>].apply(init_42)</span><br><span class="line">print(net[<span class="number">0</span>].weight.data[<span class="number">0</span>])</span><br><span class="line">print(net[<span class="number">2</span>].weight.data)</span><br></pre></td></tr></table></figure>
<h2><span id="延后初始化">延后初始化</span></h2>
<p>输入维度与前一层输出维度都延后到第一次计算时自动进行推断</p>
<h2><span id="自定义层">自定义层</span></h2>
<h3><span id="不带参数">不带参数</span></h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CenteredLayer</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, X)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> X - X.mean()</span><br><span class="line">net=nn.Sequential(</span><br><span class="line">    nn.Linear(<span class="number">20</span>,<span class="number">12</span>),</span><br><span class="line">    CenteredLayer(),</span><br><span class="line">    nn.Dropout(<span class="number">0.8</span>),</span><br><span class="line">    nn.ReLU()</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<h3><span id="带参数">带参数</span></h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyLinear</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,innum,outnum)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.weight=nn.Parameter(torch.randn(innum,outnum))</span><br><span class="line">        self.bias=nn.Parameter(torch.randn(outnum))</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self,X)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> torch.matmul(X,self.weight.data)+self.bias.data <span class="comment">#此处不能用mm！！</span></span><br></pre></td></tr></table></figure>
<h2><span id="读写文件">读写文件</span></h2>
<h3><span id="加载和保存张量">加载和保存张量</span></h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">x=torch.arange(<span class="number">24</span>)</span><br><span class="line">torch.save(x,<span class="string">'x-tensor'</span>)</span><br><span class="line">x=torch.load(<span class="string">'x-tensor'</span>)</span><br><span class="line">torch.save([x,y],<span class="string">'x-tensor'</span>)</span><br><span class="line">x,y=torch.load(<span class="string">'x-tensor'</span>)</span><br><span class="line">torch.save(&#123;<span class="string">'x'</span>:x,<span class="string">'y'</span>:y&#125;,<span class="string">'dict'</span>)</span><br><span class="line">mydict=torch.load(<span class="string">'dict'</span>)</span><br></pre></td></tr></table></figure>
<h3><span id="加载和保存模型参数">加载和保存模型参数</span></h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">net=MLP()</span><br><span class="line">torch.save(net.state_dict(),<span class="string">'mlp.params'</span>)</span><br><span class="line">net.load_state_dict(torch.load(<span class="string">'mlp.params'</span>))</span><br><span class="line">net.eval() <span class="comment">#不更新梯度，只计算</span></span><br></pre></td></tr></table></figure>
<h2><span id="gpu">GPU</span></h2>
<h3><span id="计算设备">计算设备</span></h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line">torch.device(<span class="string">'cpu'</span>)</span><br><span class="line">torch.device(<span class="string">'cuda'</span>)</span><br><span class="line">torch.cuda.device_count()</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">try_all_gpus</span><span class="params">()</span>:</span></span><br><span class="line">    devices=[torch.device(<span class="string">f'cuda:<span class="subst">&#123;i&#125;</span>'</span>) <span class="keyword">for</span> i <span class="keyword">in</span> range(torch.cuda.device_count())]</span><br><span class="line">    <span class="keyword">return</span> devices <span class="keyword">if</span> devices <span class="keyword">else</span> [torch.device(<span class="string">'cpu'</span>)]</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">try_gpu</span><span class="params">(i=<span class="number">0</span>)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> torch.device(<span class="string">'cpu'</span>) <span class="keyword">if</span> torch.cuda.device_count()&lt;i+<span class="number">1</span> <span class="keyword">else</span> torch.device(<span class="string">f'cuda:<span class="subst">&#123;i&#125;</span>'</span>)</span><br></pre></td></tr></table></figure>
<h3><span id="张量与gpu">张量与GPU</span></h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#存储在GPU上</span></span><br><span class="line">x=torch.tensor([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>])</span><br><span class="line">x.device <span class="comment">#device(type='cpu')</span></span><br><span class="line">x=torch.ones(<span class="number">2</span>,<span class="number">3</span>,device=try_gpu())</span><br><span class="line"><span class="comment">#复制</span></span><br><span class="line">x=torch.ones(<span class="number">2</span>,<span class="number">3</span>,device=try_gpu(<span class="number">0</span>))</span><br><span class="line">x=torch.ones(<span class="number">2</span>,<span class="number">3</span>,device=try_gpu(<span class="number">1</span>))</span><br><span class="line">Z=X.cuda(<span class="number">1</span>) <span class="comment">#将X的数据复制到第0块GPU上</span></span><br></pre></td></tr></table></figure>
<h3><span id="神经网络与gpu">神经网络与GPU</span></h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">net=nn.Sequential(nn.Linear(<span class="number">3</span>,<span class="number">1</span>))</span><br><span class="line">net=net.to(device=(try_gpu()))</span><br><span class="line"><span class="comment">#之后net的所有计算都会在GPU上</span></span><br></pre></td></tr></table></figure>
<h1><span id="6卷积神经网络cnn">6.卷积神经网络（CNN）</span></h1>
<h2><span id="卷积基础">卷积基础</span></h2>
<h3><span id="卷积核">卷积核</span></h3>
<p>沃尔多检测器—空间不变性！</p>
<p>从二维图像输入，即二维张量$m*n$开始，则如果隐藏层是全连接层，就会需要有$m<sup>2n</sup>2$个参数，此时的计算公式为：</p>
<p>$H_{i,j}=U_{i,j}+\sum_k\sum_lW_{i,j,k,l}X_{k,l}=U_{i,j}+\sum_{a}\sum_bV_{i,j,a,b}X_{i+a,j+b}$</p>
<p>此时利用图像的不变性原则，即对于同一个物体在不同处检测结果应该一致，所以有$V_{i,j,a,b}=V_{a,b}$</p>
<p>且$U$为常数，设为$u$</p>
<p>所以可以简化为$H_{i,j}=u+\sum_{a}\sum_bV_{a,b}X_{i+a,j+b}$，相当于对其$X_{i,j}$周边的像素进行加权得到$H_{i,j}$</p>
<p>同时，又考虑到局部性，因此最终的$H_{i,j}=u+\sum_{-\Delta}<sup>{\Delta}\sum_{-\Delta}</sup>{\Delta}V_{a,b}X_{i+a,j+b}$</p>
<p>这里的$V$是卷积核(convolution kernel)，或者说是滤波器(filter)</p>
<h3><span id="通道">通道</span></h3>
<p>输入的图像数据一般是三通道的，即红绿蓝，而维数则是3维张量。隐藏层一般也设置成3维张量，但由于需要学习更多的隐藏表示，每个隐藏表示是一系列具有二维张量的通道，比如纹理，边缘等等，有时也称为特征映射，被称为feature maps，为了表示多个隐藏表示，所以需要给卷积核增加一个维度，即</p>
<p>$H_{i,j,d}=\sum_{-\Delta}<sup>{\Delta}\sum_{-\Delta}</sup>{\Delta}\sum_cV_{a,b,c,d}X_{i+a,j+b,c}$</p>
<h2><span id="图像卷积">图像卷积</span></h2>
<h3><span id="互相关运算">互相关运算</span></h3>
<p>比如以下的卷积计算结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#input:</span></span><br><span class="line">[[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],</span><br><span class="line"> [<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>],</span><br><span class="line"> [<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span>]]</span><br><span class="line"><span class="comment">#卷积核:</span></span><br><span class="line">[[<span class="number">0</span>,<span class="number">1</span>],</span><br><span class="line"> [<span class="number">2</span>,<span class="number">3</span>]]</span><br><span class="line"><span class="comment">#卷积结果:</span></span><br><span class="line">[[<span class="number">25</span>,<span class="number">31</span>],</span><br><span class="line"> [<span class="number">43</span>,<span class="number">49</span>]]</span><br></pre></td></tr></table></figure>
<p>输入为$n<em>m$，卷积核为$h</em>w$，则卷积相当于是一个正方块在输入的正方形进行移动计算，并到达自己能够到达的每个位置，最后的卷积结果为$(n-h+1)*(m-w+1)$</p>
<h3><span id="卷积层">卷积层</span></h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 构造⼀个⼆维卷积层，它具有1个输入通道，1个输出通道和形状为（1，2）的卷积核</span></span><br><span class="line">conv2d = nn.Conv2d(<span class="number">1</span>,<span class="number">1</span>, kernel_size=(<span class="number">1</span>, <span class="number">2</span>), bias=<span class="literal">False</span>) </span><br><span class="line"><span class="comment"># 这个⼆维卷积层使⽤四维输⼊和输出格式（批量⼤⼩、通道、⾼度、宽度），</span></span><br><span class="line"><span class="comment"># 其中批量⼤⼩和通道数都为1 </span></span><br><span class="line">X = X.reshape((<span class="number">1</span>, <span class="number">1</span>, <span class="number">6</span>, <span class="number">8</span>))</span><br><span class="line">Y = Y.reshape((<span class="number">1</span>, <span class="number">1</span>, <span class="number">6</span>, <span class="number">7</span>))</span><br><span class="line">lr = <span class="number">3e-2</span> <span class="comment"># 学习率</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">	Y_hat = conv2d(X)</span><br><span class="line">    l = (Y_hat - Y) ** <span class="number">2</span></span><br><span class="line">    conv2d.zero_grad()</span><br><span class="line">    l.sum().backward()</span><br><span class="line">    <span class="comment"># 迭代卷积核</span></span><br><span class="line">    conv2d.weight.data[:] -= lr * conv2d.weight.grad</span><br><span class="line">    <span class="keyword">if</span> (i + <span class="number">1</span>) % <span class="number">2</span> == <span class="number">0</span>:</span><br><span class="line">    	print(<span class="string">f'epoch <span class="subst">&#123;i+<span class="number">1</span>&#125;</span>, loss <span class="subst">&#123;l.sum():<span class="number">.3</span>f&#125;</span>'</span>)</span><br></pre></td></tr></table></figure>
<h3><span id="特征映射与感受野">特征映射与感受野</span></h3>
<p>对于某⼀层的任意元素x，其感受野（receptive field）是指在前向传播期间可能影响x计算的所有元素（来⾃所有先前层）</p>
<h2><span id="填充与步幅">填充与步幅</span></h2>
<h3><span id="填充">填充</span></h3>
<p>对输入做填充处理从而捕捉边缘信息</p>
<img src="/2022/03/19/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/2.png" style="zoom:67%;">
<p>假设添加$p_h$行填充和$p_w$列填充，填充的内容都是上下差不多各一半，左右差不多各一半，则输出形状为 $(n_h-k_h+p_h+1)\times (n_w-k_w+p_w+1)$</p>
<p>所以一般设置 $p_h=k_h-1,p_w=k_w-1$ 这样就可以让输入输出形状相同</p>
<p>同时，如果满足：</p>
<ul>
<li>卷积核大小为奇数</li>
<li>所有边的填充行数与列数相同</li>
<li>输出与输入具有相同高度和宽度</li>
</ul>
<p>则可以得出：输出$Y[i,j]$是通过输入$X[i,j]$为中心，与卷积核进行互相关计算得到的。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">conv2d = nn.Conv2d(<span class="number">1</span>, <span class="number">1</span>, kernel_size=(<span class="number">5</span>, <span class="number">3</span>), padding=(<span class="number">2</span>, <span class="number">1</span>))</span><br><span class="line">conv2d = nn.Conv2d(<span class="number">1</span>, <span class="number">1</span>, kernel_size=(<span class="number">5</span>, <span class="number">3</span>), padding=<span class="number">3</span>)</span><br></pre></td></tr></table></figure>
<h3><span id="步幅">步幅</span></h3>
<p>按照一定步幅进行卷积，从而输入的宽度高度减少一定比例</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">conv2d = nn.Conv2d(<span class="number">1</span>, <span class="number">1</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>, stride=<span class="number">2</span>)</span><br><span class="line">conv2d = nn.Conv2d(<span class="number">1</span>, <span class="number">1</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>, stride=(<span class="number">2</span>,<span class="number">3</span>))</span><br></pre></td></tr></table></figure>
<p>变为：$\lfloor(n_k-k_h+p_h+s_h)/s_h\rfloor\times \lfloor(n_w-k_w+p_w+s_w)/s_w\rfloor$</p>
<p>后面的池化也是类似的</p>
<h2><span id="多输入多输出通道">多输⼊多输出通道</span></h2>
<h3><span id="多输入通道">多输入通道</span></h3>
<p>每个通道计算完成之后将每个通道卷积得到的结果相加</p>
<img src="/2022/03/19/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/3.png" style="zoom: 67%;">
<h3><span id="多输出通道">多输出通道</span></h3>
<p>在之前多输入基础上，加上一个维度来产生多通道的输出，相当于每一个通道自己有一组核函数</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">K = torch.stack((K, K + <span class="number">1</span>, K + <span class="number">2</span>), <span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<h2><span id="汇聚层">汇聚层</span></h2>
<p>汇聚层目的：降低卷积层对<strong>位置</strong>的敏感性，降低对<strong>空间降采样</strong>表示的敏感性</p>
<h3><span id="最大汇聚层与平均汇聚层">最大汇聚层与平均汇聚层</span></h3>
<p>不包含参数，具有确定性，即直接计算汇聚窗口中的所有元素的最大值或平均值</p>
<p><img src="/2022/03/19/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/4.png" alt></p>
<p>同样，汇聚层也具有<strong>填充和步幅</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pool2d = nn.MaxPool2d(<span class="number">3</span>,padding=<span class="number">1</span>, stride=<span class="number">2</span>) <span class="comment">#这里的padding相当于是上下各加1行，左右各加一列</span></span><br><span class="line">pool2d = nn.MaxPool2d(<span class="number">3</span>,padding=(<span class="number">1</span>,<span class="number">2</span>), stride=(<span class="number">2</span>,<span class="number">3</span>))</span><br></pre></td></tr></table></figure>
<p>多通道时，每个输入通道单独运算而不会对结果汇总，因此输出的通道数量与输入通道数相同</p>
<h2><span id="卷积神经网络lenet">卷积神经网络（LeNet）</span></h2>
<p>Lenet包含两个部分：</p>
<ul>
<li>卷积编码器：两个卷积块组成
<ul>
<li>每个卷积块包含1个卷积层，1个sigmoid激活函数，还有平均汇聚层</li>
</ul>
</li>
<li>全连接层密集块：三个全连接层组成</li>
</ul>
<h3><span id="网络结构">网络结构</span></h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#输入为28*28</span></span><br><span class="line">net = nn.Sequential(</span><br><span class="line">    nn.Conv2d(<span class="number">1</span>, <span class="number">6</span>, kernel_size=<span class="number">5</span>, padding=<span class="number">2</span>), nn.Sigmoid(), <span class="comment">#==&gt;6*28*28</span></span><br><span class="line">    nn.AvgPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>), <span class="comment">#==&gt;(28-2+2)/2=14 6*14*14</span></span><br><span class="line">    nn.Conv2d(<span class="number">6</span>, <span class="number">16</span>, kernel_size=<span class="number">5</span>), nn.Sigmoid(), <span class="comment">#==&gt;16*10*10</span></span><br><span class="line">    nn.AvgPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>), <span class="comment">#==&gt;16*5*5</span></span><br><span class="line">    nn.Flatten(), <span class="comment">#==&gt;400</span></span><br><span class="line">    nn.Linear(<span class="number">16</span> * <span class="number">5</span> * <span class="number">5</span>, <span class="number">120</span>), nn.Sigmoid(),</span><br><span class="line">    nn.Linear(<span class="number">120</span>, <span class="number">84</span>), nn.Sigmoid(),</span><br><span class="line">    nn.Linear(<span class="number">84</span>, <span class="number">10</span>))</span><br></pre></td></tr></table></figure>
<h1><span id="7现代卷积神经网络">7.现代卷积神经网络</span></h1>
<ul>
<li>AlexNet。它是第⼀个在⼤规模视觉竞赛中击败传统计算机视觉模型的⼤型神经⽹络；</li>
<li>使⽤重复块的⽹络（VGG）。它利⽤许多重复的神经⽹络块；</li>
<li>⽹络中的⽹络（NiN）。它重复使⽤由卷积层和1 <em>×</em> 1卷积层（⽤来代替全连接层）来构建深层⽹络;</li>
<li>含并⾏连结的⽹络（GoogLeNet）。它使⽤并⾏连结的⽹络，通过不同窗⼝⼤⼩的卷积层和最⼤汇聚层来并⾏抽取信息；</li>
<li>残差⽹络（ResNet）。它通过残差块构建跨层的数据通道，是计算机视觉中最流⾏的体系架构；</li>
<li>稠密连接⽹络（DenseNet）。它的计算成本很⾼，但给我们带来了更好的效果。</li>
</ul>
<h1><span id="8循环神经网络rnn">8.循环神经网络（RNN）</span></h1>
<p>CNN：处理空间信息</p>
<p>RNN-recurrent neural network：处理序列信息</p>
<h2><span id="序列模型">序列模型</span></h2>
<h3><span id="统计工具">统计工具</span></h3>
<h4><span id="自回归模型">自回归模型</span></h4>
<p>输入数据的数量 $x_{t-1},…,x_1$ 随着 $t$ 而异，而不是不变的</p>
<p>策略1：满足某个长度为 $\tau$ 的时间跨度，即使用观测序列 $x_{t-1},…,x_{t-\tau}$，参数数量不变，可以训练一个网络</p>
<p>策略2：$h_t=g(h_{t-1},x_{t-1})\rightarrow x_t=P(x_t|h_t),$ 更新模型，由于$h_t$没有被观测到，因此称为隐变量自回归模型（latent autoregressive models）</p>
<p>如何生成训练数据？一般是利用历史观测来预测下一个未来观测。常见的一个假设是虽然$x_t$可能会变，序列本身动力学不改变，不变的动力学称为静止的，即整个序列估计值用以下方式获得：$P(x_1,…,x_t)=\prod <em>{t=1}^T P(x_t|x</em>{t-1},…,x_1)$</p>
<h4><span id="马尔可夫模型">马尔可夫模型</span></h4>
<p>上文提到的 $x_{t-1},…,x_{t-\tau}$ 来估计 $x_t$ 时如果时近似精确的，则称满足马尔可夫条件，如果$\tau=1$，则可以得到一阶马尔可夫模型</p>
<p>$P(x_1,…,x_T)=\prod_{t=1}^T P(x_t|x_{t-1})$</p>
<p>此时可以得到$P(x_{t+1}|x_{t-1})=\frac {\sum_{x_t} P(x_{t+1},x_t,x_{t-1})} {P(x_{t-1})}=\frac {P(x_{t-1})\sum_{x_t} P(x_{t+1}|x_t)P(x_t|x_{t-1})} {P(x_{t-1})}=\sum_{x_t}P(x_{t+1}|x_t)P(x_t|x_{t-1})$</p>
<h4><span id="因果关系">因果关系</span></h4>
<p>基于马尔可夫模型我们还可以得到一个反向条件概率分布，不过一般这不好解释</p>
<h3><span id="训练">训练</span></h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.utils <span class="keyword">import</span> data</span><br><span class="line">T=<span class="number">1000</span></span><br><span class="line">time = torch.arange(<span class="number">1</span>, T + <span class="number">1</span>, dtype=torch.float32)</span><br><span class="line">x = torch.sin(<span class="number">0.01</span> * time) + torch.normal(<span class="number">0</span>, <span class="number">0.2</span>, (T,))</span><br><span class="line">t=<span class="number">5</span></span><br><span class="line">train_data=torch.zeros(T-t,t)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(t):</span><br><span class="line">    train_data[:,i]=x[i:T-t+i]</span><br><span class="line">train_labels=x[t:].reshape(<span class="number">-1</span>,<span class="number">1</span>)</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">init_weights</span><span class="params">(m)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> type(m) == nn.Linear:</span><br><span class="line">        nn.init.xavier_uniform_(m.weight)</span><br><span class="line">batch_size, n_train,epoch,lr = <span class="number">16</span>, <span class="number">600</span>,<span class="number">100</span>,<span class="number">0.1</span></span><br><span class="line"></span><br><span class="line">net=nn.Sequential(</span><br><span class="line">    nn.Linear(<span class="number">5</span>,<span class="number">10</span>),</span><br><span class="line">    nn.ReLU(),</span><br><span class="line">    nn.Linear(<span class="number">10</span>,<span class="number">1</span>)</span><br><span class="line">)</span><br><span class="line">net.apply(init_weights)</span><br><span class="line">loss=nn.MSELoss(reduction=<span class="string">'none'</span>)</span><br><span class="line">optimizer=torch.optim.Adam(net.parameters(), lr)</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_array</span><span class="params">(data_arrays, batch_size, is_train=True)</span>:</span> </span><br><span class="line">    <span class="string">"""构造一个PyTorch数据迭代器。"""</span></span><br><span class="line">    dataset = data.TensorDataset(*data_arrays)</span><br><span class="line">    <span class="keyword">return</span> data.DataLoader(dataset, batch_size, shuffle=is_train)</span><br><span class="line">train_iter=load_array((train_data[:n_train],train_labels[:n_train]),batch_size)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(epoch):</span><br><span class="line">    <span class="keyword">for</span> X,y <span class="keyword">in</span> train_iter:</span><br><span class="line">        l=loss(net(X),y)</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        l.sum().backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">    cnt=<span class="number">0</span></span><br><span class="line">    loss_sum=<span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> X,y <span class="keyword">in</span> train_iter:</span><br><span class="line">        l=loss(net(X),y)</span><br><span class="line">        loss_sum+=l.sum()</span><br><span class="line">        cnt+=<span class="number">1</span></span><br><span class="line">    print(<span class="string">f'epoch<span class="subst">&#123;i+<span class="number">1</span>&#125;</span> loss: <span class="subst">&#123;float(loss_sum)/cnt&#125;</span>'</span>)</span><br></pre></td></tr></table></figure>
<p>进行下一个时间的预测到下$t$个时间的预测都相对较精准，但$t+1$及之后的时间的预测就不精准了，因为需要不断用预测的数据去预测更之后的数据，会造成误差积累。</p>
<h2><span id="文本预处理">文本预处理</span></h2>
<p>最常见的序列数据就是文本，一篇文章可以看为是一串单词序列，甚至是一串字符序列。</p>
<p>常见预处理步骤：</p>
<ul>
<li>文本作为字符串加载到内存中</li>
<li>字符串拆分为词元（单词和字符）</li>
<li>建立一个词表，将拆分的词元映射到数字索引</li>
<li>将文本转换为数字索引序列，方便模型操作</li>
</ul>
<p>此部分在https://github.com/BUAADreamer/nnplayer/tree/master/nlputil处进行了一些实践</p>
<h2><span id="语言模型和数据集">语言模型和数据集</span></h2>
<p>语言模型的目标：估计序列的联合概率 $P(x_1,x_2,…,x_T)$</p>
<p>简单统计相对词频，执行某种形式的拉普拉斯平滑</p>
<h3><span id="马尔可夫模型与n元语法">马尔可夫模型与<em>n</em>元语法</span></h3>
<p>涉及⼀个、两个和三个变量的概率公式分别被称为“⼀元语法”（unigram）、“⼆元语法”（bigram）和“三元语法”（trigram）模型</p>
<h3><span id="自然语言统计">自然语言统计</span></h3>
<p>词频最高的词都是类似 <code>the/i/and</code> 这样的词，被称为停用词，可以被过滤掉</p>
<p>词频衰减迅速，齐普夫定律：第 $i$ 个最常用的单词频率$n_i$为 $n_i\propto \frac 1 {i^\alpha}$</p>
<h2><span id="循环神经网络">循环神经网络</span></h2>
<h3><span id="有隐状态的循环神经网络">有隐状态的循环神经网络</span></h3>
<p>$H_t=\phi(X_tW_{xh}+H_{t-1}W_{hh}+b_h)$</p>
<h3><span id="困惑度perplexity">困惑度（<strong>Perplexity</strong>）</span></h3>
<p>$exp(-\frac 1 n \sum_{t=1}^n log P(x_t|x_{t-1},…,x_{1}))$</p>
<h3><span id="梯度裁剪">梯度裁剪</span></h3>
<p>将梯度映射到$\theta$范围内，比如以下代码</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">grad_clipping</span><span class="params">(net, theta)</span>:</span> <span class="comment">#@save</span></span><br><span class="line">    <span class="string">"""裁剪梯度"""</span></span><br><span class="line">    <span class="keyword">if</span> isinstance(net, nn.Module):</span><br><span class="line">    	params = [p <span class="keyword">for</span> p <span class="keyword">in</span> net.parameters() <span class="keyword">if</span> p.requires_grad]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">    	params = net.params</span><br><span class="line">    norm = torch.sqrt(sum(torch.sum((p.grad ** <span class="number">2</span>)) <span class="keyword">for</span> p <span class="keyword">in</span> params)) <span class="comment">#梯度平方和开根号</span></span><br><span class="line">    <span class="keyword">if</span> norm &gt; theta:</span><br><span class="line">    	<span class="keyword">for</span> param <span class="keyword">in</span> params:</span><br><span class="line">    		param.grad[:] *= theta / norm <span class="comment">#映射到theta范围内</span></span><br></pre></td></tr></table></figure>
<h2><span id="简洁实现">简洁实现</span></h2>
<p>参考实现的代码：https://github.com/BUAADreamer/nlpkiller/blob/master/main.py</p>
<h1><span id="9现代循环神经网络">9.现代循环神经网络</span></h1>
<h2><span id="门控循环单元gru">门控循环单元（GRU）</span></h2>
<h2><span id="长短期记忆网络lstm">长短期记忆网络（LSTM）</span></h2>
<h2><span id="深度循环神经网络">深度循环神经⽹络</span></h2>
<h2><span id="双向循环神经网络">双向循环神经⽹络</span></h2>
<h2><span id="机器翻译与数据集">机器翻译与数据集</span></h2>
<h2><span id="编码器-解码器架构">编码器-解码器架构</span></h2>
<h2><span id="序列到序列学习seq2seq">序列到序列学习（seq2seq）</span></h2>
<h2><span id="束搜索">束搜索</span></h2>
<h1><span id="10注意力机制">10.注意力机制</span></h1>
<h1><span id="11优化算法">11.优化算法</span></h1>
<h1><span id="12计算性能">12.计算性能</span></h1>
<h1><span id="13计算机视觉">13.计算机视觉</span></h1>
<h1><span id="14自然语言处理预训练">14.自然语言处理：预训练</span></h1>
<h1><span id="15自然语言处理应用">15.自然语言处理：应用</span></h1>

                
                <hr>
                <!-- Pager -->
                <ul class="pager">
                    
                    <li class="previous">
                        <a href="/2022/04/04/结对编程实录/" data-toggle="tooltip" data-placement="top" title="结对编程实录">&larr; Previous Post</a>
                    </li>
                    
                    
                    <li class="next">
                        <a href="/2022/03/14/软件案例分析-音乐软件界的卧龙凤雏-QQ音乐与网易云音乐/" data-toggle="tooltip" data-placement="top" title="软件案例分析-音乐软件界的卧龙凤雏-酷狗音乐与网易云音乐">Next Post &rarr;</a>
                    </li>
                    
                </ul>

                <!-- tip start -->
                

                
                <!-- tip end -->

                <!-- Music start-->
                
                <!-- Music end -->

                <!-- Sharing -->
                
                <div class="social-share"  data-wechat-qrcode-helper="" align="center"></div>
                <!--  css & js -->
                <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/social-share.js/1.0.16/css/share.min.css">
                <script src="https://cdnjs.cloudflare.com/ajax/libs/social-share.js/1.0.16/js/social-share.min.js"></script>
                
                <!-- Sharing -->

                <!-- gitment start -->
                
                <!-- gitment end -->

                <!-- 来必力City版安装代码 -->
                
                <!-- City版安装代码已完成 -->

                <!-- disqus comment start -->
                
                <!-- disqus comment end -->
            </div>
            
            <!-- Tabe of Content -->
            <!-- Table of Contents -->

    
      
        <aside id="sidebar">
          <div id="toc" class="toc-article">
          <strong class="toc-title">Contents</strong>
          
            
              <ol class="toc-nav"><li class="toc-nav-item toc-nav-level-1"><a class="toc-nav-link" href="#"><span class="toc-nav-text">1.前言</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#"><span class="toc-nav-text">日常生活中的机器学习</span></a></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#"><span class="toc-nav-text">关键组件</span></a></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#"><span class="toc-nav-text">各种机器学习问题</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#"><span class="toc-nav-text">监督学习（supervised learning）</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#"><span class="toc-nav-text">无监督学习（unsupervised learning）</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#"><span class="toc-nav-text">与环境互动</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#"><span class="toc-nav-text">强化学习（reinforcement learning）</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#"><span class="toc-nav-text">起源</span></a></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#"><span class="toc-nav-text">深度学习之路</span></a></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#"><span class="toc-nav-text">成功案例</span></a></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#"><span class="toc-nav-text">特点</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-1"><a class="toc-nav-link" href="#"><span class="toc-nav-text">2.预备知识</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#"><span class="toc-nav-text">2.1数据操作</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#"><span class="toc-nav-text">入门</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#"><span class="toc-nav-text">运算符</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#"><span class="toc-nav-text">广播机制</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#"><span class="toc-nav-text">索引和切片</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#"><span class="toc-nav-text">节省内存</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#"><span class="toc-nav-text">转换为其他python对象</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#"><span class="toc-nav-text">2.2数据预处理</span></a></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#"><span class="toc-nav-text">2.3线性代数</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#"><span class="toc-nav-text">标量：单个数</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#"><span class="toc-nav-text">向量：标量组成的列表</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#"><span class="toc-nav-text">矩阵：标量组成的二维数组</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#"><span class="toc-nav-text">张量：标量组成的多维数组</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#"><span class="toc-nav-text">降维</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#"><span class="toc-nav-text">非降维求和</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#"><span class="toc-nav-text">点积</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#"><span class="toc-nav-text">矩阵-向量积</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#"><span class="toc-nav-text">矩阵乘法</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#"><span class="toc-nav-text">范数 norm</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#"><span class="toc-nav-text">2.4微积分</span></a></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#"><span class="toc-nav-text">2.5自动微分</span></a></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#"><span class="toc-nav-text">2.6概率</span></a></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#"><span class="toc-nav-text">2.7查阅⽂档</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-1"><a class="toc-nav-link" href="#"><span class="toc-nav-text">3.线性神经网络（LNN）</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#"><span class="toc-nav-text">线性回归</span></a></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#"><span class="toc-nav-text">softmax回归</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-1"><a class="toc-nav-link" href="#"><span class="toc-nav-text">4.多层感知机（MLP）</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#"><span class="toc-nav-text">多层感知机</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#"><span class="toc-nav-text">实现</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#"><span class="toc-nav-text">激活函数</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#"><span class="toc-nav-text">ReLU函数</span></a></li><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#"><span class="toc-nav-text">sigmoid函数</span></a></li><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#"><span class="toc-nav-text">tanh函数</span></a></li></ol></li></ol></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#"><span class="toc-nav-text">模型选择、欠拟合和过拟合</span></a></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#"><span class="toc-nav-text">权重衰减</span></a></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#"><span class="toc-nav-text">暂退法</span></a></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#"><span class="toc-nav-text">前向传播，反向传播与计算图</span></a></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#"><span class="toc-nav-text">数值稳定性和模型初始化</span></a></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#"><span class="toc-nav-text">环境与分布偏移</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-1"><a class="toc-nav-link" href="#"><span class="toc-nav-text">5.深度学习计算</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#"><span class="toc-nav-text">层和块</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#"><span class="toc-nav-text">自定义块</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#"><span class="toc-nav-text">顺序块</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#"><span class="toc-nav-text">前向传播函数中执行代码</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#"><span class="toc-nav-text">参数管理</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#"><span class="toc-nav-text">参数访问</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#"><span class="toc-nav-text">参数初始化</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#"><span class="toc-nav-text">内置初始化</span></a></li></ol></li></ol></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#"><span class="toc-nav-text">延后初始化</span></a></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#"><span class="toc-nav-text">自定义层</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#"><span class="toc-nav-text">不带参数</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#"><span class="toc-nav-text">带参数</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#"><span class="toc-nav-text">读写文件</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#"><span class="toc-nav-text">加载和保存张量</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#"><span class="toc-nav-text">加载和保存模型参数</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#"><span class="toc-nav-text">GPU</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#"><span class="toc-nav-text">计算设备</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#"><span class="toc-nav-text">张量与GPU</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#"><span class="toc-nav-text">神经网络与GPU</span></a></li></ol></li></ol></li><li class="toc-nav-item toc-nav-level-1"><a class="toc-nav-link" href="#"><span class="toc-nav-text">6.卷积神经网络（CNN）</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#"><span class="toc-nav-text">卷积基础</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#"><span class="toc-nav-text">卷积核</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#"><span class="toc-nav-text">通道</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#"><span class="toc-nav-text">图像卷积</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#"><span class="toc-nav-text">互相关运算</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#"><span class="toc-nav-text">卷积层</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#"><span class="toc-nav-text">特征映射与感受野</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#"><span class="toc-nav-text">填充与步幅</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#"><span class="toc-nav-text">填充</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#"><span class="toc-nav-text">步幅</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#"><span class="toc-nav-text">多输⼊多输出通道</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#"><span class="toc-nav-text">多输入通道</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#"><span class="toc-nav-text">多输出通道</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#"><span class="toc-nav-text">汇聚层</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#"><span class="toc-nav-text">最大汇聚层与平均汇聚层</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#"><span class="toc-nav-text">卷积神经网络（LeNet）</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#"><span class="toc-nav-text">网络结构</span></a></li></ol></li></ol></li><li class="toc-nav-item toc-nav-level-1"><a class="toc-nav-link" href="#"><span class="toc-nav-text">7.现代卷积神经网络</span></a></li><li class="toc-nav-item toc-nav-level-1"><a class="toc-nav-link" href="#"><span class="toc-nav-text">8.循环神经网络（RNN）</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#"><span class="toc-nav-text">序列模型</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#"><span class="toc-nav-text">统计工具</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#"><span class="toc-nav-text">自回归模型</span></a></li><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#"><span class="toc-nav-text">马尔可夫模型</span></a></li><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#"><span class="toc-nav-text">因果关系</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#"><span class="toc-nav-text">训练</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#"><span class="toc-nav-text">文本预处理</span></a></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#"><span class="toc-nav-text">语言模型和数据集</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#"><span class="toc-nav-text">马尔可夫模型与n元语法</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#"><span class="toc-nav-text">自然语言统计</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#"><span class="toc-nav-text">循环神经网络</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#"><span class="toc-nav-text">有隐状态的循环神经网络</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#"><span class="toc-nav-text">困惑度（Perplexity）</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#"><span class="toc-nav-text">梯度裁剪</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#"><span class="toc-nav-text">简洁实现</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-1"><a class="toc-nav-link" href="#"><span class="toc-nav-text">9.现代循环神经网络</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#"><span class="toc-nav-text">门控循环单元（GRU）</span></a></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#"><span class="toc-nav-text">长短期记忆网络（LSTM）</span></a></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#"><span class="toc-nav-text">深度循环神经⽹络</span></a></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#"><span class="toc-nav-text">双向循环神经⽹络</span></a></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#"><span class="toc-nav-text">机器翻译与数据集</span></a></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#"><span class="toc-nav-text">编码器-解码器架构</span></a></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#"><span class="toc-nav-text">序列到序列学习（seq2seq）</span></a></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#"><span class="toc-nav-text">束搜索</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-1"><a class="toc-nav-link" href="#"><span class="toc-nav-text">10.注意力机制</span></a></li><li class="toc-nav-item toc-nav-level-1"><a class="toc-nav-link" href="#"><span class="toc-nav-text">11.优化算法</span></a></li><li class="toc-nav-item toc-nav-level-1"><a class="toc-nav-link" href="#"><span class="toc-nav-text">12.计算性能</span></a></li><li class="toc-nav-item toc-nav-level-1"><a class="toc-nav-link" href="#"><span class="toc-nav-text">13.计算机视觉</span></a></li><li class="toc-nav-item toc-nav-level-1"><a class="toc-nav-link" href="#"><span class="toc-nav-text">14.自然语言处理：预训练</span></a></li><li class="toc-nav-item toc-nav-level-1"><a class="toc-nav-link" href="#"><span class="toc-nav-text">15.自然语言处理：应用</span></a></li></ol>
            
          
          </div>
        </aside>
      
    

                
            <!-- Sidebar Container -->
            <div class="
                col-lg-8 col-lg-offset-2
                col-md-10 col-md-offset-1
                sidebar-container">

                <!-- Featured Tags -->
                
                <section>
                    <!-- no hr -->
                    <h5><a href="/tags/">FEATURED TAGS</a></h5>
                    <div class="tags">
                       
                          <a class="tag" href="/tags/#DeepLearning" title="DeepLearning">DeepLearning</a>
                        
                    </div>
                </section>
                

                <!-- Friends Blog -->
                
                <hr>
                <h5>FRIENDS</h5>
                <ul class="list-inline">

                    
                        <li><a href="https://roife.github.io" target="_blank">Roife</a></li>
                    
                        <li><a href="https://coekjan.cn" target="_blank">Coekjan</a></li>
                    
                        <li><a href="https://www.dusign.net/" target="_blank">Dusign</a></li>
                    
                </ul>
                
            </div>
        </div>
    </div>
</article>
<script src="https://utteranc.es/client.js"
        repo="buaadreamer/buaadreamer.github.io"
        issue-term="pathname"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script>



<!-- async load function -->
<script>
    function async(u, c) {
      var d = document, t = 'script',
          o = d.createElement(t),
          s = d.getElementsByTagName(t)[0];
      o.src = u;
      if (c) { o.addEventListener('load', function (e) { c(null, e); }, false); }
      s.parentNode.insertBefore(o, s);
    }
</script>
<!-- anchor-js, Doc:http://bryanbraun.github.io/anchorjs/ -->
<script>
    async("https://cdn.bootcss.com/anchor-js/1.1.1/anchor.min.js",function(){
        anchors.options = {
          visible: 'hover',
          placement: 'left',
          icon: 'ℬ'
        };
        anchors.add().remove('.intro-header h1').remove('.subheading').remove('.sidebar-container h5');
    })
</script>


<style  type="text/css">
    /* place left on bigger screen */
    @media all and (min-width: 800px) {
        .anchorjs-link{
            position: absolute;
            left: -0.75em;
            font-size: 1.1em;
            margin-top : -0.1em;
        }
    }
</style>



    <!-- Footer -->
    <!-- Footer -->
<footer>
    <div class="container">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                <ul class="list-inline text-center">

                
                    <li>
                        <a target="_blank"  href="https://github.com/BUAADreamer">
                            <span class="fa-stack fa-lg">
                                <i class="fa fa-circle fa-stack-2x"></i>
                                <i class="fa fa-github fa-stack-1x fa-inverse"></i>
                            </span>
                        </a>
                    </li>
                

                

                

                

                

                

                

                </ul>
                <p class="copyright text-muted">
                    Copyright &copy; BUAADreamer 2022 
                    <br>
                    Powered by 
                    <a href="https://github.com/dusign/hexo-theme-snail" target="_blank" rel="noopener">
                        <i>hexo-theme-snail</i>
                    </a> | 
                    <iframe name="star" style="margin-left: 2px; margin-bottom:-5px;" frameborder="0" scrolling="0"
                        width="100px" height="20px"
                        src="https://ghbtns.com/github-btn.html?user=dusign&repo=hexo-theme-snail&type=star&count=true">
                    </iframe>
                </p>
            </div>
        </div>
    </div>

</footer>

<!-- jQuery -->

<script src="/js/jquery.min.js"></script>


<!-- Bootstrap Core JavaScript -->

<script src="/js/bootstrap.min.js"></script>


<!-- Custom Theme JavaScript -->

<script src="/js/hux-blog.min.js"></script>


<!-- Search -->

<script src="/js/search.js"></script>


<!-- async load function -->
<script>
    function async(u, c) {
      var d = document, t = 'script',
          o = d.createElement(t),
          s = d.getElementsByTagName(t)[0];
      o.src = u;
      if (c) { o.addEventListener('load', function (e) { c(null, e); }, false); }
      s.parentNode.insertBefore(o, s);
    }
</script>


<!-- jquery.tagcloud.js -->
<script>
    // only load tagcloud.js in tag.html
    if($('#tag_cloud').length !== 0){
        async("https://BUAADreamer.top/js/jquery.tagcloud.js",function(){
            $.fn.tagcloud.defaults = {
                //size: {start: 1, end: 1, unit: 'em'},
                color: {start: '#bbbbee', end: '#0085a1'},
            };
            $('#tag_cloud a').tagcloud();
        })
    }
</script>

<!--fastClick.js -->
<script>
    async("https://cdn.bootcss.com/fastclick/1.0.6/fastclick.min.js", function(){
        var $nav = document.querySelector("nav");
        if($nav) FastClick.attach($nav);
    })
</script>


<!-- Google Analytics -->


<script>
    // dynamic User by Hux
    var _gaId = 'UA-XXXXXXXX-X';
    var _gaDomain = 'yoursite';

    // Originial
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

    ga('create', _gaId, _gaDomain);
    ga('send', 'pageview');
</script>




<!-- Baidu Tongji -->


<!-- Search -->

    <script type="text/javascript">      
        var search_path = "search.xml";
        if (search_path.length == 0) {
            search_path = "search.xml";
        }
    var path = "/" + search_path;
    searchFunc(path, 'local-search-input', 'local-search-result');
    </script>


<!-- busuanzi -->
<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>


  <script src='https://unpkg.com/mermaid@7.1.2/dist/mermaid.min.js'></script>
  <script>
    if (window.mermaid) {
      mermaid.initialize({theme: 'forest'});
    }
  </script>








	<a id="rocket" href="#top" class=""></a>
	<script type="text/javascript" src="/js/totop.js?v=1.0.0" async=""></script>
    <script type="text/javascript" src="/js/toc.js?v=1.0.0" async=""></script>

    
        <!-- background effects line -->
        

        
            <script type="text/javascript" src="/js/mouse-click.js" content='[&quot;🌱&quot;,&quot;just do it&quot;,&quot;🍀&quot;]' color='[&quot;rgb(121,93,179)&quot; ,&quot;rgb(76,180,231)&quot; ,&quot;rgb(184,90,154)&quot;]'></script>
        

        <!-- background effects end -->
    

    <!--<script size="50" alpha='0.3' zIndex="-999" src="/js/ribbonStatic.js"></script>-->
    
        <script src="/js/ribbonDynamic.js"></script>
    
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</body>

</html>
