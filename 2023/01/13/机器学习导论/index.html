<!DOCTYPE html>
<html lang="en">

<!-- Head tag -->
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="google-site-verification" content="xBT4GhYoi5qRD5tr338pgPM5OWHHIDR6mNg1a3euekI" />
    <meta name="baidu-site-verification" content="093lY4ziMu" />
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
    <meta name="description" content="A hexo theme">
    <meta name="keyword"  content="BUAADreamer, hexo-theme-snail">
    <link rel="shortcut icon" href="/img/ironman-draw.png">
    <!-- Place this tag in your head or just before your close body tag. -->
    <script async defer src="https://buttons.github.io/buttons.js"></script>
    <!--<link href='http://fonts.googleapis.com/css?family=Montserrat:400,700' rel='stylesheet' type='text/css'>-->
    <title>
        
          机器学习导论 - BUAADreamer&#39;s Blog
        
    </title>

    <link rel="canonical" href="https://BUAADreamer.top/2023/01/13/机器学习导论/">

    <!-- Bootstrap Core CSS -->
    
<link rel="stylesheet" href="/css/bootstrap.min.css">


    <!-- Custom CSS --> 
    
        
<link rel="stylesheet" href="/css/dusign-light.css">

        
<link rel="stylesheet" href="/css/dusign-common-light.css">

        
<link rel="stylesheet" href="/css/font-awesome.css">

        
<link rel="stylesheet" href="/css/toc.css">

        <!-- background effects end -->
    
    
    <!-- Pygments Highlight CSS -->
    
<link rel="stylesheet" href="/css/highlight.css">


    
<link rel="stylesheet" href="/css/widget.css">


    
<link rel="stylesheet" href="/css/rocket.css">


    
<link rel="stylesheet" href="/css/signature.css">


    
<link rel="stylesheet" href="/css/fonts.googleapis.css">


    <link rel="stylesheet" href="//cdn.bootcss.com/font-awesome/4.3.0/css/font-awesome.min.css">

    <!-- photography -->
    
<link rel="stylesheet" href="/css/photography.css">


    <!-- ga & ba script hoook -->
    <script></script>
<meta name="generator" content="Hexo 4.2.1"></head>


<!-- hack iOS CSS :active style -->
<body ontouchstart="">

    <!-- background effects start -->
    
    <!-- background effects end -->

	<!-- Modified by Yu-Hsuan Yen -->
<!-- Post Header -->
<style type="text/css">
    header.intro-header{
        
            
                background-image: linear-gradient(rgba(0, 0, 0, 0.3), rgba(0, 0, 0, 0.3)), url('../../../../img/default.jpg')
                /*post*/
            
        
    }
    
    #signature{
        background-image: url('/img/signature/dusign.png');
    }
    
</style>

<header class="intro-header" >
    <!-- Signature -->
    <div id="signature">
        <div class="container">
            <div class="row">
                <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                
                    <div class="post-heading">
                        <div class="tags">
                            
                              <a class="tag" href="/tags/#DeepLearning" title="DeepLearning">DeepLearning</a>
                            
                        </div>
                        <h1>机器学习导论</h1>
                        <h2 class="subheading"></h2>
                        <span class="meta">
                            Posted by BUAADreamer on
                            2023-01-13
                        </span>

                        
                            <div class="blank_box"></div>
                            <span class="meta">
                                Words <span class="post-count">4.3k</span> and
                                Reading Time <span class="post-count">15</span> Minutes
                            </span>
                            <div class="blank_box"></div>
                            <!-- 不蒜子统计 start -->
                            <span class="meta">
                                Viewed <span id="busuanzi_value_page_pv"><i class="fa fa-spinner fa-spin"></i></span> Times
                            </span>
                            <!-- 不蒜子统计 end -->
                        

                    </div>
                

                </div>
            </div>
        </div>      
    </div>

    
    <div class="waveWrapper">
        <div class="wave wave_before" style="background-image: url('/img/wave-light.png')"></div>
        <div class="wave wave_after" style="background-image: url('/img/wave-light.png')"></div>
    </div>
    
</header>

	
    <!-- Navigation -->
<nav class="navbar navbar-default navbar-custom navbar-fixed-top">
    <div class="container-fluid">
        <!-- Brand and toggle get grouped for better mobile display -->
        <div class="navbar-header page-scroll">
            <button type="button" class="navbar-toggle">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="/">BUAADreamer&#39;s Blog</a>
        </div>

        <!-- Collect the nav links, forms, and other content for toggling -->
        <!-- Known Issue, found by Hux:
            <nav>'s height woule be hold on by its content.
            so, when navbar scale out, the <nav> will cover tags.
            also mask any touch event of tags, unfortunately.
        -->
        <div id="huxblog_navbar">
            <div class="navbar-collapse">
                <ul class="nav navbar-nav navbar-right">
                    <li>
                        <a href="/">Home</a>
                    </li>

                    

                        
                    

                        
                        <li>
                            <a href="/about/">About</a>
                        </li>
                        
                    

                        
                        <li>
                            <a href="/archive/">Archives</a>
                        </li>
                        
                    

                        
                        <li>
                            <a href="/tags/">Tags</a>
                        </li>
                        
                    

                        
                        <li>
                            <a href="/photography/">Photography</a>
                        </li>
                        
                    

                        
                        <li>
                            <a href="/categories/">Categories</a>
                        </li>
                        
                    
                    
                    
                    <li>
                        <a href="https://www.cnblogs.com/BUAADreamer/" target="_blank">Chinese Blog</a>
                    </li>
                    
                </ul>
            </div>
        </div>
        <!-- /.navbar-collapse -->
    </div>
    <!-- /.container -->
</nav>
<script>
    // Drop Bootstarp low-performance Navbar
    // Use customize navbar with high-quality material design animation
    // in high-perf jank-free CSS3 implementation
    var $body   = document.body;
    var $toggle = document.querySelector('.navbar-toggle');
    var $navbar = document.querySelector('#huxblog_navbar');
    var $collapse = document.querySelector('.navbar-collapse');

    $toggle.addEventListener('click', handleMagic)
    function handleMagic(e){
        if ($navbar.className.indexOf('in') > 0) {
        // CLOSE
            $navbar.className = " ";
            // wait until animation end.
            setTimeout(function(){
                // prevent frequently toggle
                if($navbar.className.indexOf('in') < 0) {
                    $collapse.style.height = "0px"
                }
            },400)
        }else{
        // OPEN
            $collapse.style.height = "auto"
            $navbar.className += " in";
        }
    }
</script>


    <!-- Main Content -->
    <!-- Post Content -->
<article>
    <div class="container">
        <div class="row">
            
            <!-- Post Container -->
            <div class="
                col-lg-8 col-lg-offset-2
                col-md-10 col-md-offset-1
                post-container">

                <h1 id="一、引论">一、引论<a class="anchor" href="#一、引论">·</a></h1>
<h3 id="机器学习定义">机器学习定义<a class="anchor" href="#机器学习定义">·</a></h3>
<p>Tom Mitchell：Well-posed Learning Problem: A computer program is said to learn from <strong>experience E</strong> with respect to some <strong>task T</strong> and some <strong>performance measure P</strong>, if its performance on T, as measured by P, improves with experience E (1998).</p>
<h3 id="机器学习研究问题">机器学习研究问题<a class="anchor" href="#机器学习研究问题">·</a></h3>
<img src="/2023/01/13/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AF%BC%E8%AE%BA/1.png" style="zoom:50%;">
<h1 id="二、模型评估与选择">二、模型评估与选择<a class="anchor" href="#二、模型评估与选择">·</a></h1>
<h2 id="误差">误差<a class="anchor" href="#误差">·</a></h2>
<p>误差 训练误差/经验误差 测试误差/泛化误差</p>
<h2 id="数据集划分">数据集划分<a class="anchor" href="#数据集划分">·</a></h2>
<p>划分为训练集与测试集，两者尽可能互斥。</p>
<h3 id="保持-留出法-hold-out">保持/留出法**(hold-out)**<a class="anchor" href="#保持-留出法-hold-out">·</a></h3>
<p>随即划分两个集合，一版2/3为训练，1/3为测试</p>
<h3 id="随机子抽样-random-sub-sampling">随机子抽样(random sub-sampling)<a class="anchor" href="#随机子抽样-random-sub-sampling">·</a></h3>
<p>随机选择，保持方法重复K次，总准确率取平均</p>
<h3 id="K折交叉验证-k-fold-cross-validation">K折交叉验证(k-fold cross-validation)<a class="anchor" href="#K折交叉验证-k-fold-cross-validation">·</a></h3>
<p>开始时分为k个大小相似的折，训练测试k次，第i次迭代时，第i折用于测试，其余用于训练。</p>
<h3 id="留一法-leave-one-out">留一法(leave-one-out)<a class="anchor" href="#留一法-leave-one-out">·</a></h3>
<p>K折交叉的特殊情况，只给检验集留<strong>1个样本</strong></p>
<h3 id="自助法">自助法<a class="anchor" href="#自助法">·</a></h3>
<p>有放回等可能地从初始样本D中均匀抽样，采样D次即可产生|D|长度数据集。对于小数据集和集成学习有较好效果。</p>
<h2 id="性能度量">性能度量<a class="anchor" href="#性能度量">·</a></h2>
<img src="/2023/01/13/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AF%BC%E8%AE%BA/6.png" style="zoom:50%;">
<p><strong>分类任务</strong>中，虽然错误率和精度计算简单，但是当<strong>数据类别不均衡</strong>时，<strong>占比大类</strong>成为影响结果的主要因素，不能提供更详细评估。</p>
<h3 id="性能度量-混淆矩阵">性能度量-混淆矩阵<a class="anchor" href="#性能度量-混淆矩阵">·</a></h3>
<p>包括每一类的正确和错误的样本个数，包括正确和错误的分类。主对角线上的是正确预测的，其他的是预测错误的。</p>
<h4 id="混淆矩阵-两类">混淆矩阵-两类<a class="anchor" href="#混淆矩阵-两类">·</a></h4>
<p>仅有正、负样本2类。用T和F（或1和0）来表征正确错误，P表示正例，N表示反例</p>
<img src="/2023/01/13/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AF%BC%E8%AE%BA/7.png" style="zoom:50%;">
<ul>
<li>TP ：被分类器<strong>正确分类</strong>的<strong>正元组</strong>；期望为P，分类为P：称为<strong>真正</strong></li>
<li>TN：被分类器<strong>正确分类</strong>的<strong>负元组</strong>； 期望为N，分类为N：称为<strong>真负</strong></li>
<li>FP：被<strong>错误标记为正元组</strong>的<strong>负元组</strong>； 期望为N，分类为P：称为<strong>假正</strong></li>
<li>FN：被<strong>错误标记为负元组</strong>的<strong>正元组</strong>。期望为P，分类为N：称为<strong>假负</strong></li>
</ul>
<img src="/2023/01/13/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AF%BC%E8%AE%BA/21.png" style="zoom:50%;">
<img src="/2023/01/13/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AF%BC%E8%AE%BA/22.png" style="zoom: 50%;">
<h4 id="P-R曲线">P-R曲线<a class="anchor" href="#P-R曲线">·</a></h4>
<img src="/2023/01/13/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AF%BC%E8%AE%BA/23.png" style="zoom:50%;">
<p>查全率/召回率其实是看所有的正样本中，有多少被成功标记成正的。</p>
<p>查准率则是，判断为正的样本中，有多少是正确的。</p>
<p>查准率与查全率<strong>相互矛盾</strong></p>
<h3 id="性能度量-F分数">性能度量-F分数<a class="anchor" href="#性能度量-F分数">·</a></h3>
<img src="/2023/01/13/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AF%BC%E8%AE%BA/24.png" style="zoom:50%;">
<h3 id="代价敏感性能度量">代价敏感性能度量<a class="anchor" href="#代价敏感性能度量">·</a></h3>
<img src="/2023/01/13/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AF%BC%E8%AE%BA/25.png" style="zoom:50%;">
<h1 id="三、贝叶斯决策理论">三、贝叶斯决策理论<a class="anchor" href="#三、贝叶斯决策理论">·</a></h1>
<p>统计决策理论</p>
<h2 id="基本概念">基本概念<a class="anchor" href="#基本概念">·</a></h2>
<p>样本 每个人的身高体重等信息</p>
<p>类别/状态 例如：男女性别</p>
<p>先验概率 类别比例/出现概率 —— 以往历史数据得到的概率</p>
<p>样本分布密度</p>
<p>类条件概率密度</p>
<p>后验概率 —— 利用最新输入数据对先验概率进行修正后的概率</p>
<p>错误概率</p>
<p>平均错误率</p>
<p>正确率</p>
<h2 id="贝叶斯决策">贝叶斯决策<a class="anchor" href="#贝叶斯决策">·</a></h2>
<h3 id="基于最大后验概率进行决策（即最小错误率）">基于最大后验概率进行决策（即最小错误率）<a class="anchor" href="#基于最大后验概率进行决策（即最小错误率）">·</a></h3>
<img src="/2023/01/13/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AF%BC%E8%AE%BA/2.png" style="zoom: 33%;">
<p>$P(x|\omega_i)$：类条件概率密度</p>
<p>$P(\omega_i)$：先验概率</p>
<p>$P(\omega_i|x)$：后验概率</p>
<p><strong>解决分类问题</strong></p>
<h4 id="最小错误率贝叶斯决策">最小错误率贝叶斯决策<a class="anchor" href="#最小错误率贝叶斯决策">·</a></h4>
<p>对于每一个样本后验概率均选择错误率较小的结果</p>
<h2 id="最小风险贝叶斯决策">最小风险贝叶斯决策<a class="anchor" href="#最小风险贝叶斯决策">·</a></h2>
<p>损失函数：对特定的x采取特定的期望损失</p>
<img src="/2023/01/13/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AF%BC%E8%AE%BA/31.png" style="zoom:50%;">
<h4 id="朴素贝叶斯决策">朴素贝叶斯决策<a class="anchor" href="#朴素贝叶斯决策">·</a></h4>
<p>直接根据样本的各个属性发生概率之积来计算</p>
<h3 id="概率密度估计">概率密度估计<a class="anchor" href="#概率密度估计">·</a></h3>
<h4 id="参数化方法">参数化方法<a class="anchor" href="#参数化方法">·</a></h4>
<p>最大似然估计</p>
<p>贝叶斯估计</p>
<h4 id="非参数化方法">非参数化方法<a class="anchor" href="#非参数化方法">·</a></h4>
<p>$Parzen$窗法</p>
<p>$k_n$近邻法</p>
<h1 id="四、线性模型">四、线性模型<a class="anchor" href="#四、线性模型">·</a></h1>
<h3 id="线性回归">线性回归<a class="anchor" href="#线性回归">·</a></h3>
<h4 id="梯度下降法">梯度下降法<a class="anchor" href="#梯度下降法">·</a></h4>
<p><strong>梯度下降</strong>—利用所有数据</p>
<p><strong>批处理梯度下降(Batch Gradient Descent)</strong>—每次选取一个mini-batch进行梯度下降，一个循环称为一个epoch</p>
<p><strong>随机梯度下降（Stochastic Gradient Descent）</strong>，又称Online Learning，每次看一个样本</p>
<p>样本量较大时使用</p>
<h4 id="标准方程组法">标准方程组法<a class="anchor" href="#标准方程组法">·</a></h4>
<p>直接对损失函数求导得到标准解</p>
<p>样本量较小时使用</p>
<h3 id="线性判别函数">线性判别函数<a class="anchor" href="#线性判别函数">·</a></h3>
<p>线性判别函数$g(x)=\omega ^T x+\omega_0$</p>
<img src="/2023/01/13/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AF%BC%E8%AE%BA/41.png" style="zoom:50%;">
<p>广义线性判别函数容易带来维数灾难。因此使用线性判别函数的简单性解决问题。利用齐次简化后的线性判别函数</p>
<img src="/2023/01/13/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AF%BC%E8%AE%BA/42.png" style="zoom:50%;">
<p>设计分类函数，准则函数，之后求准则函数极值</p>
<h4 id="Fisher准则">Fisher准则<a class="anchor" href="#Fisher准则">·</a></h4>
<p>寻找投影方向最好的方向</p>
<img src="/2023/01/13/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AF%BC%E8%AE%BA/43.png" style="zoom:67%;">
<p>上面的<code>m1/m2</code>是某一类的均值，下面的<code>S1/S2</code>是类内的离散度，即类之间的差别越大，类内差别越小</p>
<img src="/2023/01/13/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AF%BC%E8%AE%BA/46.png" style="zoom:50%;">
<img src="/2023/01/13/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AF%BC%E8%AE%BA/44.png" style="zoom: 67%;">
<img src="/2023/01/13/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AF%BC%E8%AE%BA/45.png" style="zoom:50%;">
<h4 id="感知机准则">感知机准则<a class="anchor" href="#感知机准则">·</a></h4>
<img src="/2023/01/13/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AF%BC%E8%AE%BA/47.png" style="zoom:50%;">
<img src="/2023/01/13/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AF%BC%E8%AE%BA/48.png" style="zoom:50%;">
<h1 id="五、支持向量机">五、支持向量机<a class="anchor" href="#五、支持向量机">·</a></h1>
<h2 id="概念">概念<a class="anchor" href="#概念">·</a></h2>
<p>SVM从线性可分情况下的<strong>最优分类面</strong>发展而来。最优分类面就是要求分类线不但能<strong>将两类正确分开</strong>(训练错误率为0)，且使<strong>分类间隔最大</strong>。寻找一个<strong>满足分类要求</strong>的超平面，并且使训练集中的<strong>点距离分类面</strong>尽可能的<strong>远</strong>，也就是寻找一个分类面使它两侧的空白区域(<strong>Margin</strong>)最大</p>
<h2 id="线性支持向量机">线性支持向量机<a class="anchor" href="#线性支持向量机">·</a></h2>
<img src="/2023/01/13/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AF%BC%E8%AE%BA/54.png" style="zoom:50%;">
<img src="/2023/01/13/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AF%BC%E8%AE%BA/55.png" style="zoom:50%;">
<h3 id="KKT条件">KKT条件<a class="anchor" href="#KKT条件">·</a></h3>
<img src="/2023/01/13/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AF%BC%E8%AE%BA/53.png" style="zoom:50%;">
<p>处理噪声和离群点</p>
<h2 id="非线性支持向量机">非线性支持向量机<a class="anchor" href="#非线性支持向量机">·</a></h2>
<p>用一个固定的非线性映射将特征空间学习的线性分类器等价于基于原始数据学习的非线性分类器</p>
<p>这个非线性映射即为<strong>核函数</strong></p>
<h3 id="常用核函数">常用核函数<a class="anchor" href="#常用核函数">·</a></h3>
<img src="/2023/01/13/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AF%BC%E8%AE%BA/51.png" style="zoom:50%;">
<h4 id="判断是否可以作为核函数">判断是否可以作为核函数<a class="anchor" href="#判断是否可以作为核函数">·</a></h4>
<img src="/2023/01/13/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AF%BC%E8%AE%BA/52.png" style="zoom:50%;">
<h3 id="序列最小优化算法">序列最小优化算法<a class="anchor" href="#序列最小优化算法">·</a></h3>
<p><strong>序列最小优化算法(Sequential Minimal Optimization, SMO)<strong>是一种启发式算法。基本思想是：如果所有变量都满足此优化问题的</strong>KKT</strong>条件，那么这个问题的解就得到了。</p>
<h3 id="支持向量机工具">支持向量机工具<a class="anchor" href="#支持向量机工具">·</a></h3>
<p>LibSVM: http://www.csie.ntu.edu.tw/~cjlin/libsvm/</p>
<h1 id="六、决策树">六、决策树<a class="anchor" href="#六、决策树">·</a></h1>
<p>树型结构，由<strong>结点</strong>和<strong>有向边</strong>组成</p>
<img src="/2023/01/13/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AF%BC%E8%AE%BA/61.png" style="zoom:50%;">
<p>采用<strong>自顶向下</strong>的递归方法，以<strong>信息熵为度量</strong>构造一棵<strong>熵值下降最快</strong>的树，到叶子结点处的熵值为0，此时叶结点中的实例属于同一类。决策树可以看成一堆<code>if-else</code>的规则集合</p>
<h2 id="算法流程">算法流程<a class="anchor" href="#算法流程">·</a></h2>
<img src="/2023/01/13/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AF%BC%E8%AE%BA/62.png" style="zoom:50%;">
<h2 id="主要算法">主要算法<a class="anchor" href="#主要算法">·</a></h2>
<p>每个分支结点的样本<strong>尽可能属于同一类别</strong>，即结点的<strong>纯度</strong>越来越高</p>
<p>不同目标函数建立决策树主要有以下三种算法：</p>
<ul>
<li>ID3：信息增益</li>
<li>C4.5：信息增益率</li>
<li>CART：基尼指数</li>
</ul>
<h3 id="ID3算法">ID3算法<a class="anchor" href="#ID3算法">·</a></h3>
<p>以<strong>信息熵</strong>为度量，优先选择<strong>熵值下降最快</strong>的决策树，即熵值最小的属性</p>
<h4 id="信息熵-Entropy">信息熵(Entropy)<a class="anchor" href="#信息熵-Entropy">·</a></h4>
<img src="/2023/01/13/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AF%BC%E8%AE%BA/63.png" style="zoom:50%;">
<p>不确定性越大，熵值越大。</p>
<h4 id="经验-信息-熵">经验(信息)熵<a class="anchor" href="#经验-信息-熵">·</a></h4>
<img src="/2023/01/13/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AF%BC%E8%AE%BA/64.png" style="zoom:50%;">
<h4 id="条件熵-Conditional-Entropy">条件熵(Conditional Entropy)<a class="anchor" href="#条件熵-Conditional-Entropy">·</a></h4>
<img src="/2023/01/13/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AF%BC%E8%AE%BA/65.png" style="zoom:50%;">
<h4 id="信息增益">信息增益<a class="anchor" href="#信息增益">·</a></h4>
<img src="/2023/01/13/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AF%BC%E8%AE%BA/66.png" style="zoom:50%;">
<h4 id="ID3算法-决策树生成算法">ID3算法-决策树生成算法<a class="anchor" href="#ID3算法-决策树生成算法">·</a></h4>
<img src="/2023/01/13/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AF%BC%E8%AE%BA/67.png" style="zoom:50%;">
<h4 id="算法特点">算法特点<a class="anchor" href="#算法特点">·</a></h4>
<p><strong>优点</strong>：只要有较好的标注就可以进行学习，可从无序无规则事物中推理出分类规则。分类模型为树状，简单直观容易理解。</p>
<p><strong>缺点</strong>：偏好<strong>取值多</strong>的属性，极限趋近于均匀分布，可能受到<strong>噪声或小样本</strong>影响，容易出现<strong>过拟合</strong>问题。无法处理<strong>连续值</strong>，<strong>属性值不完整</strong>，<strong>不同代价</strong>等情况的属性。</p>
<h3 id="属性筛选度量标准">属性筛选度量标准<a class="anchor" href="#属性筛选度量标准">·</a></h3>
<h4 id="信息增益率">信息增益率<a class="anchor" href="#信息增益率">·</a></h4>
<img src="/2023/01/13/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AF%BC%E8%AE%BA/68.png" style="zoom: 50%;">
<p>对可取值数目N较多的属性有所偏好。使用如下的<strong>信息增益率</strong>，可以<strong>缓解</strong>信息增益准则对<strong>可取值数目较多</strong>的属性的偏好</p>
<img src="/2023/01/13/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AF%BC%E8%AE%BA/69.png" style="zoom:50%;">
<p><strong>C4.5算法</strong>采用了这种方式替代了ID3的信息增益。</p>
<h4 id="基尼指数-Gini-Index">基尼指数(Gini Index)<a class="anchor" href="#基尼指数-Gini-Index">·</a></h4>
<img src="/2023/01/13/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AF%BC%E8%AE%BA/610.png" style="zoom:50%;">
<p><strong>CART算法</strong>就采用<strong>基尼指数</strong>替代了ID3算法的信息增益</p>
<h2 id="剪枝处理-Pruning">剪枝处理(Pruning)<a class="anchor" href="#剪枝处理-Pruning">·</a></h2>
<p><strong>问题</strong>：<strong>过拟合</strong></p>
<p><strong>基本策略</strong>：</p>
<ul>
<li><strong>预剪枝策略(Pre-pruning)</strong>：决策树生成过程中，对每个结点在划分前进行估计，如果<strong>不能</strong>带来决策树<strong>泛化性能提升</strong>，<strong>停止划分</strong>并作为叶结点。
<ul>
<li>剪掉很多没有必要的分支，<strong>降低过拟合风险</strong>，但是由于有些划分可能之后会提高泛化性能，所以可能导致<strong>欠拟合</strong>。</li>
</ul>
</li>
<li><strong>后剪枝策略(Post-pruning)</strong>：先利用训练集生成决策树，自底向上考察非叶结点，如果将这个结点对应子树<strong>替换成叶结点</strong>可以<strong>提升泛化性能</strong>，则替换该子树为叶结点。
<ul>
<li>比预剪枝决策树保留了更多分支，降低欠拟合风险，泛化性能更好，但训练开销也更大。</li>
</ul>
</li>
</ul>
<h2 id="连续值处理">连续值处理<a class="anchor" href="#连续值处理">·</a></h2>
<p>采用**二分法(Bi-Partition)**进行离散化。</p>
<p>具体来说，是把从小到大排列的数据每两个相邻的数值的平均作为划分依据，计算相应的信息增益。</p>
<h2 id="缺失值处理">缺失值处理<a class="anchor" href="#缺失值处理">·</a></h2>
<p>让样本以不同概率划分到不同的子结点去。即只计算无缺失样本的信息增益，并乘一个无缺失样本占比的系数。</p>
<h2 id="不同代价属性的处理">不同代价属性的处理<a class="anchor" href="#不同代价属性的处理">·</a></h2>
<img src="/2023/01/13/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AF%BC%E8%AE%BA/611.png" style="zoom:50%;">
<h2 id="延申">延申<a class="anchor" href="#延申">·</a></h2>
<p><strong>概念学习系统</strong>(Concept Learning System, CLS) 1966年Hunt等人提出</p>
<p><strong>分类回归树</strong>(Classification And Regression Tree, <strong>CART</strong>)算法 1984年Breiman等人提出，一种二分递归分割技术</p>
<p><strong>J. R. Quinlan</strong>：1979年 ID3/1993年 C4.5算法</p>
<p><strong>多变量决策树</strong></p>
<h1 id="七、人工神经网络">七、人工神经网络<a class="anchor" href="#七、人工神经网络">·</a></h1>
<h2 id="MP模型">MP模型<a class="anchor" href="#MP模型">·</a></h2>
<p>一种人工神经元的数学模型。</p>
<img src="/2023/01/13/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AF%BC%E8%AE%BA/71.png" style="zoom: 50%;">
<img src="/2023/01/13/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AF%BC%E8%AE%BA/72.png" style="zoom:50%;">
<img src="/2023/01/13/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AF%BC%E8%AE%BA/73.png" style="zoom:80%;">
<p>MP模型采用阙值（阶跃）函数作为激活函数。</p>
<img src="/2023/01/13/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AF%BC%E8%AE%BA/74.png" style="zoom:50%;">
<p>神经元此时可以看成一个<strong>线性分类器</strong></p>
<p>激活函数：线性函数，非线性斜面函数，Sigmoid输出函数（S型函数）</p>
<img src="/2023/01/13/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AF%BC%E8%AE%BA/75.png" style="zoom:50%;">
<h2 id="感知器">感知器<a class="anchor" href="#感知器">·</a></h2>
<p>实际上是一种MP模型，监督学习</p>
<img src="/2023/01/13/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AF%BC%E8%AE%BA/76.png" style="zoom:50%;">
<p><strong>三层感知器</strong>可以<strong>实现任意的逻辑运算</strong>，在<strong>激活函数为Sigmoid函数</strong>的情况下，可以<strong>逼近</strong>任何非线性多元函数。</p>
<h3 id="前馈神经网络">前馈神经网络<a class="anchor" href="#前馈神经网络">·</a></h3>
<p>前向传播</p>
<h3 id="反向传播-BP-算法">反向传播(BP)算法<a class="anchor" href="#反向传播-BP-算法">·</a></h3>
<p>以一个三层感知器为例</p>
<p>有训练集<code>D={(x1,t1),(x2,t2),...,(xN,tN)}</code></p>
<p>隐藏层有<code>M</code>个神经元，激活函数为<code>f1</code>。输出层有<code>K</code>个神经元，激活函数为<code>f2</code>，最终的损失函数为$E(\omega)=\frac 1 2 \sum_{k=1}^K (y_k-t_k)^2$</p>
<p>$z_j=f1(\sum_{i=1}^{N}a_{ji}*x_i)=f1(c_j)$</p>
<p>$y_k=f2(\sum_{j=1}^M b_{kj}*z_j)=f2(d_k)$</p>
<p>方便起见，有如下标记</p>
<p>$c_j=\sum_{i=1}^{N}a_{ji}*x_i$</p>
<p>$d_k=\sum_{j=1}^M b_{kj}*z_j$</p>
<p>则如果需要更新参数，以更新$a_{mn}$和$b_{km}$为例。</p>
<p>先计算$b_{km}$梯度</p>
<p>$\Delta(b_{km})=(y_k-t_k)*f2’(d_k)*z_m$</p>
<p>再计算$a_{mn}$梯度</p>
<p>$\Delta(a_{mn})=\sum_{k=1}^K(y_k-t_k)*f2’(d_k)*b_{km}*f1’(c_m)*x_n$</p>
<h1 id="八、聚类">八、聚类<a class="anchor" href="#八、聚类">·</a></h1>
<p>一种非监督学习</p>
<h2 id="性能度量-2">性能度量<a class="anchor" href="#性能度量-2">·</a></h2>
<h3 id="外部指标">外部指标<a class="anchor" href="#外部指标">·</a></h3>
<img src="/2023/01/13/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AF%BC%E8%AE%BA/81.png" style="zoom:50%;">
<h3 id="内部指标">内部指标<a class="anchor" href="#内部指标">·</a></h3>
<img src="/2023/01/13/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AF%BC%E8%AE%BA/82.png" style="zoom:50%;">
<img src="/2023/01/13/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AF%BC%E8%AE%BA/83.png" style="zoom:50%;">
<h2 id="聚类方法">聚类方法<a class="anchor" href="#聚类方法">·</a></h2>
<h3 id="K均值-K-Means">K均值(K-Means)<a class="anchor" href="#K均值-K-Means">·</a></h3>
<img src="/2023/01/13/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AF%BC%E8%AE%BA/84.png" style="zoom: 50%;">
<h3 id="K-means">K-means++<a class="anchor" href="#K-means">·</a></h3>
<p>已经选取了n个初始聚类中心，在选取第n+1个中心时，距离当前n个聚类中心越远的点会有更高概率被选为第n+1个聚类中心。</p>
<h3 id="Kernel-K-means">Kernel K-means<a class="anchor" href="#Kernel-K-means">·</a></h3>
<p>参照<strong>核函数</strong>思想，将所有样本映射到另一个<strong>特征空间</strong>中再进行聚类。</p>
<h3 id="ISODATA">ISODATA<a class="anchor" href="#ISODATA">·</a></h3>
<p><strong>迭代自组织数据分析算法 Iterative Self-Organizing Data Analysis Technique Algorithm</strong></p>
<p>当某<strong>两个聚类中心距离小于某阙值</strong>时将它们合并为一类，当某类<strong>标准差大于某一阙值</strong>时，将其分裂为两类。某类<strong>样本数目少于某阙值取消</strong>这个过程。</p>
<h3 id="层次聚类算法">层次聚类算法<a class="anchor" href="#层次聚类算法">·</a></h3>
<p>分为自底向上和自顶向下</p>
<h4 id="AGNES算法（Agglomerative-Nesting）">AGNES算法（Agglomerative Nesting）<a class="anchor" href="#AGNES算法（Agglomerative-Nesting）">·</a></h4>
<p><strong>自底向上</strong></p>
<p>初始时，每个样本看作一个初始聚类簇，之后每一步找出距离最近的两个聚类簇进行合并，并不断重复。直到达到<strong>预设的聚类簇个数</strong>。度量两个聚类簇之间的距离，常用度量方式：</p>
<img src="/2023/01/13/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AF%BC%E8%AE%BA/85.png" style="zoom:50%;">
<img src="/2023/01/13/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AF%BC%E8%AE%BA/86.png" style="zoom:50%;">
<h2 id="聚类应用">聚类应用<a class="anchor" href="#聚类应用">·</a></h2>
<p>医学图像-组织分类</p>
<p>遥感图像-地貌分类</p>
<p>BOW模型——词袋模型</p>
<p>网络用户分类</p>
<h1 id="九、数据降维">九、数据降维<a class="anchor" href="#九、数据降维">·</a></h1>
<h2 id="数据维度">数据维度<a class="anchor" href="#数据维度">·</a></h2>
<p>点0 线1 面2 体3</p>
<h2 id="数据降维">数据降维<a class="anchor" href="#数据降维">·</a></h2>
<p>高维空间中有很多冗余信息和噪声信息，会在实际应用中引入误差，影响准确率。</p>
<p>降维可以提取数据内部本质结构，减少冗余信息和噪声信息造成的误差，提高应用精度。</p>
<p><strong>降维</strong>：利用<strong>某种映射</strong>将原高维度空间的数据点投射到低维度空间。</p>
<h2 id="降维方法">降维方法<a class="anchor" href="#降维方法">·</a></h2>
<h3 id="主成分分析-PCA">主成分分析 PCA<a class="anchor" href="#主成分分析-PCA">·</a></h3>
<p>将原有的众多具有一定相关性的指标重新组合成一组少量相互无关的综合指标。</p>
<p>使得降维后方差尽可能大，均方误差尽可能小</p>
<p><strong>最大方差思想</strong>：使用较少的数据维度保留住较多的原数据特性</p>
<p><strong>最小均方误差思想</strong>：使原数据与降维后的数据(在原空间中的重建)的误差最小</p>
<h4 id="应用">应用<a class="anchor" href="#应用">·</a></h4>
<h5 id="利用PCA处理高维数据">利用PCA处理高维数据<a class="anchor" href="#利用PCA处理高维数据">·</a></h5>
<h4 id="LDA">LDA<a class="anchor" href="#LDA">·</a></h4>
<p>LDA是一种监督学习的降维技术，也就是说它的数据集的每个样本是有类别输出的。这点和PCA不同。PCA是不考虑样本类别输出的无监督降维技术。LDA的思想可以用一句话概括，就是“投影后类内方差最小，类间方差最大”。什么意思呢？ 我们要将数据在低维度上进行投影，投影后希望每一种类别数据的投影点尽可能的接近，而不同类别的数据的类别中心之间的距离尽可能的大。</p>
<h4 id="PCA和LDA区别">PCA和LDA区别<a class="anchor" href="#PCA和LDA区别">·</a></h4>
<p>PCA追求降维后能够<strong>最大化保持数据内在信息</strong>，并通过衡量在<strong>投影方向上的数据方差</strong>来判断其重要性。但这对数据的区分作用并不大，反而可能使得数据点混杂在一起。</p>
<p>LDA所追求的目标与PCA不同，<strong>不是希望</strong>保持数据最多的信息，而是<strong>希望数据在降维后能够很容易地被区分开</strong>。</p>
<h4 id="Kernel-PCA">Kernel PCA<a class="anchor" href="#Kernel-PCA">·</a></h4>
<p>线性假设一般化，引入核函数。</p>
<h3 id="等距映射">等距映射<a class="anchor" href="#等距映射">·</a></h3>
<h3 id="局部线性嵌入">局部线性嵌入<a class="anchor" href="#局部线性嵌入">·</a></h3>
<h1 id="十、集成学习">十、集成学习<a class="anchor" href="#十、集成学习">·</a></h1>
<h2 id="基本概念-2">基本概念<a class="anchor" href="#基本概念-2">·</a></h2>
<p>通过构建并结合<strong>多个分类器</strong>完成学习任务</p>
<p>又称为<strong>多分类器系统</strong>，<strong>基于委员会的学习</strong></p>
<p><strong>弱分类器</strong>：准确率仅比随机猜测略高的分类器</p>
<p><strong>强分类器</strong>：准确率高并能在多项式时间内完成的分类器</p>
<p>个体学习器生成方式不同，可以分为两大类方法</p>
<ul>
<li>串行化方法：个体学习器间<strong>存在强依赖</strong>关系
<ul>
<li>典型算法：<strong>Boosting(Adaboost)</strong></li>
</ul>
</li>
<li>并行化方法：个体学习器间<strong>不存在强依赖</strong>关系
<ul>
<li>典型算法：<strong>Bagging/随机森林(Random Forest)</strong></li>
</ul>
</li>
</ul>
<h2 id="串行化方法-Boosting">串行化方法 Boosting<a class="anchor" href="#串行化方法-Boosting">·</a></h2>
<img src="/2023/01/13/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AF%BC%E8%AE%BA/101.png" style="zoom:50%;">
<h3 id="Adaboost算法">Adaboost算法<a class="anchor" href="#Adaboost算法">·</a></h3>
<img src="/2023/01/13/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AF%BC%E8%AE%BA/102.png" style="zoom:40%;">
<h3 id="Boosting算法特点">Boosting算法特点<a class="anchor" href="#Boosting算法特点">·</a></h3>
<h4 id="基学习器能学习特定的数据分布">基学习器能学习特定的数据分布<a class="anchor" href="#基学习器能学习特定的数据分布">·</a></h4>
<p>重赋权法(Re-weighting)</p>
<p>重采样法(Re-sampling)</p>
<h4 id="特点总结">特点总结<a class="anchor" href="#特点总结">·</a></h4>
<p>主要关注降低偏差，每个模型是<strong>弱模型，偏差高，方差低</strong></p>
<p><strong>贪心法</strong>最小化损失函数</p>
<h2 id="并行化方法-Bagging算法">并行化方法 Bagging算法<a class="anchor" href="#并行化方法-Bagging算法">·</a></h2>
<p>基于<strong>自助法采样 (Bootstrap Sampling)</strong></p>
<img src="/2023/01/13/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AF%BC%E8%AE%BA/103.png" style="zoom:50%;">
<h3 id="基本思想">基本思想<a class="anchor" href="#基本思想">·</a></h3>
<p>利用自助法采样可构造T个含m个训练样本的采样集，基于每个采样集训练出一个基学习器，再将它们进行结合(在对预测输出结合时，通常对分类任务使用简单投票法，对回归任务使用简单平均法)。</p>
<img src="/2023/01/13/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AF%BC%E8%AE%BA/104.png" style="zoom:50%;">
<h3 id="特点">特点<a class="anchor" href="#特点">·</a></h3>
<p>主要关注降低方差，在易受扰动的学习器上效用更加明显。是强模型，偏差低，方差高</p>
<h2 id="并行化方法-随机森林算法">并行化方法 随机森林算法<a class="anchor" href="#并行化方法-随机森林算法">·</a></h2>
<p>Bagging方法的一种扩展变体</p>
<p><strong>Random Forest</strong>，简称<strong>RF</strong></p>
<p>以<strong>决策树</strong>为基学习器</p>
<p>训练过程引入<strong>随机属性选择</strong></p>
<img src="/2023/01/13/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AF%BC%E8%AE%BA/105.png" style="zoom: 67%;">
<img src="/2023/01/13/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AF%BC%E8%AE%BA/1015.png" style="zoom:50%;">
<h2 id="结合策略">结合策略<a class="anchor" href="#结合策略">·</a></h2>
<h3 id="平均法">平均法<a class="anchor" href="#平均法">·</a></h3>
<h4 id="简单平均法">简单平均法<a class="anchor" href="#简单平均法">·</a></h4>
<p>个体学习器性能相近时适用</p>
<h4 id="加权平均法">加权平均法<a class="anchor" href="#加权平均法">·</a></h4>
<p>个体学习器性能迥异时适用</p>
<h3 id="结合策略—投票法">结合策略—投票法<a class="anchor" href="#结合策略—投票法">·</a></h3>
<img src="/2023/01/13/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AF%BC%E8%AE%BA/106.png" style="zoom:40%;">
<h3 id="结合策略—学习法">结合策略—学习法<a class="anchor" href="#结合策略—学习法">·</a></h3>
<p>从初始数据集训练初始学习器，初级学习器的输出被当作样例输入特征，继承初始样本标记，从次级数据集训练次级学习器</p>
<h2 id="多样性">多样性<a class="anchor" href="#多样性">·</a></h2>
<p>度量集成中个体学习器的<strong>多样性</strong>，考虑<strong>个体学习器</strong>的两两<strong>相似</strong>/<strong>不相似</strong>性</p>
<h3 id="多样性度量">多样性度量<a class="anchor" href="#多样性度量">·</a></h3>
<img src="/2023/01/13/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AF%BC%E8%AE%BA/107.png" style="zoom:40%;">
<img src="/2023/01/13/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AF%BC%E8%AE%BA/108.png" style="zoom:40%;">
<img src="/2023/01/13/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AF%BC%E8%AE%BA/109.png" style="zoom:40%;">
<img src="/2023/01/13/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AF%BC%E8%AE%BA/1010.png" style="zoom:40%;">
<h3 id="多样性增强">多样性增强<a class="anchor" href="#多样性增强">·</a></h3>
<ul>
<li>数据样本扰动
<ul>
<li>
<img src="/2023/01/13/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AF%BC%E8%AE%BA/1011.png" style="zoom:40%;">
</li>
</ul>
</li>
<li>输入属性扰动
<ul>
<li>
<img src="/2023/01/13/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AF%BC%E8%AE%BA/1012.png" style="zoom:40%;">
</li>
</ul>
</li>
<li>输出表示扰动
<ul>
<li>
<img src="/2023/01/13/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AF%BC%E8%AE%BA/1013.png" style="zoom:40%;">
</li>
</ul>
</li>
<li>算法参数扰动
<ul>
<li>
<img src="/2023/01/13/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AF%BC%E8%AE%BA/1014.png" style="zoom:40%;">
</li>
</ul>
</li>
<li>不同的多样性增强机制也可一起使用
<ul>
<li>Adaboost：加入了<strong>数据样本扰动</strong></li>
<li>随机森林：同时加入了<strong>数据样本扰动</strong>和<strong>输入属性扰动</strong></li>
</ul>
</li>
</ul>
<h1 id="十一、半监督学习">十一、半监督学习<a class="anchor" href="#十一、半监督学习">·</a></h1>
<p>如何有效利用<strong>已标记</strong>和<strong>未标记</strong>的样本集</p>
<h2 id="基本假设">基本假设<a class="anchor" href="#基本假设">·</a></h2>
<img src="/2023/01/13/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AF%BC%E8%AE%BA/111.png" style="zoom:50%;">
<h2 id="自学习方法-Self-Training-Methods">自学习方法 (Self-Training Methods)<a class="anchor" href="#自学习方法-Self-Training-Methods">·</a></h2>
<img src="/2023/01/13/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AF%BC%E8%AE%BA/112.png" style="zoom:50%;">
<h3 id="典型代表-最近邻自学习算法">典型代表-最近邻自学习算法<a class="anchor" href="#典型代表-最近邻自学习算法">·</a></h3>
<img src="/2023/01/13/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AF%BC%E8%AE%BA/113.png" style="zoom:40%;">
<h3 id="典型代表-半监督SVM">典型代表-半监督SVM<a class="anchor" href="#典型代表-半监督SVM">·</a></h3>
<h4 id="直推式支持向量机T-SVM">直推式支持向量机T-SVM<a class="anchor" href="#直推式支持向量机T-SVM">·</a></h4>
<p>针对二分类问题，同时利用标记和未标记样本，通过<strong>尝试将每个未标记样本分别作为正例和反例</strong>来寻找<strong>最优分类边界</strong>，来得到原始数据中两类样本的最大分类间隔</p>
<h3 id="典型代表-半监督聚类">典型代表-半监督聚类<a class="anchor" href="#典型代表-半监督聚类">·</a></h3>
<ul>
<li><strong>必连</strong>与<strong>勿连</strong>约束
<ul>
<li>利用这样的关系进行约束K均值</li>
<li></li>
</ul>
</li>
<li>具有少量标记样本
<ul>
<li>直接将初始的有标记样本作为种子，初始化Kmeans的K个聚类中心</li>
</ul>
</li>
</ul>

                
                <hr>
                <!-- Pager -->
                <ul class="pager">
                    
                    <li class="previous">
                        <a href="/2024/01/15/RAG理论-概述/" data-toggle="tooltip" data-placement="top" title="RAG理论-第一篇-概述">&larr; Previous Post</a>
                    </li>
                    
                    
                    <li class="next">
                        <a href="/2022/09/19/编译技术理论课笔记/" data-toggle="tooltip" data-placement="top" title="编译技术理论课笔记">Next Post &rarr;</a>
                    </li>
                    
                </ul>

                <!-- tip start -->
                

                
                <!-- tip end -->

                <!-- Music start-->
                
                <!-- Music end -->

                <!-- Sharing -->
                
                <div class="social-share"  data-wechat-qrcode-helper="" align="center"></div>
                <!--  css & js -->
                <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/social-share.js/1.0.16/css/share.min.css">
                <script src="https://cdnjs.cloudflare.com/ajax/libs/social-share.js/1.0.16/js/social-share.min.js"></script>
                
                <!-- Sharing -->

                <!-- gitment start -->
                
                <!-- gitment end -->

                <!-- 来必力City版安装代码 -->
                
                <!-- City版安装代码已完成 -->

                <!-- disqus comment start -->
                
                <!-- disqus comment end -->
            </div>
            
            <!-- Tabe of Content -->
            <!-- Table of Contents -->

    
      
        <aside id="sidebar">
          <div id="toc" class="toc-article">
          <strong class="toc-title">Contents</strong>
          
            
              <ol class="toc-nav"><li class="toc-nav-item toc-nav-level-1"><a class="toc-nav-link" href="#一、引论"><span class="toc-nav-text">一、引论</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#机器学习定义"><span class="toc-nav-text">机器学习定义</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#机器学习研究问题"><span class="toc-nav-text">机器学习研究问题</span></a></li></ol></li></ol></li><li class="toc-nav-item toc-nav-level-1"><a class="toc-nav-link" href="#二、模型评估与选择"><span class="toc-nav-text">二、模型评估与选择</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#误差"><span class="toc-nav-text">误差</span></a></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#数据集划分"><span class="toc-nav-text">数据集划分</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#保持-留出法-hold-out"><span class="toc-nav-text">保持&#x2F;留出法**(hold-out)**</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#随机子抽样-random-sub-sampling"><span class="toc-nav-text">随机子抽样(random sub-sampling)</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#K折交叉验证-k-fold-cross-validation"><span class="toc-nav-text">K折交叉验证(k-fold cross-validation)</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#留一法-leave-one-out"><span class="toc-nav-text">留一法(leave-one-out)</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#自助法"><span class="toc-nav-text">自助法</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#性能度量"><span class="toc-nav-text">性能度量</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#性能度量-混淆矩阵"><span class="toc-nav-text">性能度量-混淆矩阵</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#混淆矩阵-两类"><span class="toc-nav-text">混淆矩阵-两类</span></a></li><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#P-R曲线"><span class="toc-nav-text">P-R曲线</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#性能度量-F分数"><span class="toc-nav-text">性能度量-F分数</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#代价敏感性能度量"><span class="toc-nav-text">代价敏感性能度量</span></a></li></ol></li></ol></li><li class="toc-nav-item toc-nav-level-1"><a class="toc-nav-link" href="#三、贝叶斯决策理论"><span class="toc-nav-text">三、贝叶斯决策理论</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#基本概念"><span class="toc-nav-text">基本概念</span></a></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#贝叶斯决策"><span class="toc-nav-text">贝叶斯决策</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#基于最大后验概率进行决策（即最小错误率）"><span class="toc-nav-text">基于最大后验概率进行决策（即最小错误率）</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#最小错误率贝叶斯决策"><span class="toc-nav-text">最小错误率贝叶斯决策</span></a></li></ol></li></ol></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#最小风险贝叶斯决策"><span class="toc-nav-text">最小风险贝叶斯决策</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#朴素贝叶斯决策"><span class="toc-nav-text">朴素贝叶斯决策</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#概率密度估计"><span class="toc-nav-text">概率密度估计</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#参数化方法"><span class="toc-nav-text">参数化方法</span></a></li><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#非参数化方法"><span class="toc-nav-text">非参数化方法</span></a></li></ol></li></ol></li></ol></li><li class="toc-nav-item toc-nav-level-1"><a class="toc-nav-link" href="#四、线性模型"><span class="toc-nav-text">四、线性模型</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#线性回归"><span class="toc-nav-text">线性回归</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#梯度下降法"><span class="toc-nav-text">梯度下降法</span></a></li><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#标准方程组法"><span class="toc-nav-text">标准方程组法</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#线性判别函数"><span class="toc-nav-text">线性判别函数</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#Fisher准则"><span class="toc-nav-text">Fisher准则</span></a></li><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#感知机准则"><span class="toc-nav-text">感知机准则</span></a></li></ol></li></ol></li></ol></li><li class="toc-nav-item toc-nav-level-1"><a class="toc-nav-link" href="#五、支持向量机"><span class="toc-nav-text">五、支持向量机</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#概念"><span class="toc-nav-text">概念</span></a></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#线性支持向量机"><span class="toc-nav-text">线性支持向量机</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#KKT条件"><span class="toc-nav-text">KKT条件</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#非线性支持向量机"><span class="toc-nav-text">非线性支持向量机</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#常用核函数"><span class="toc-nav-text">常用核函数</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#判断是否可以作为核函数"><span class="toc-nav-text">判断是否可以作为核函数</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#序列最小优化算法"><span class="toc-nav-text">序列最小优化算法</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#支持向量机工具"><span class="toc-nav-text">支持向量机工具</span></a></li></ol></li></ol></li><li class="toc-nav-item toc-nav-level-1"><a class="toc-nav-link" href="#六、决策树"><span class="toc-nav-text">六、决策树</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#算法流程"><span class="toc-nav-text">算法流程</span></a></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#主要算法"><span class="toc-nav-text">主要算法</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#ID3算法"><span class="toc-nav-text">ID3算法</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#信息熵-Entropy"><span class="toc-nav-text">信息熵(Entropy)</span></a></li><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#经验-信息-熵"><span class="toc-nav-text">经验(信息)熵</span></a></li><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#条件熵-Conditional-Entropy"><span class="toc-nav-text">条件熵(Conditional Entropy)</span></a></li><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#信息增益"><span class="toc-nav-text">信息增益</span></a></li><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#ID3算法-决策树生成算法"><span class="toc-nav-text">ID3算法-决策树生成算法</span></a></li><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#算法特点"><span class="toc-nav-text">算法特点</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#属性筛选度量标准"><span class="toc-nav-text">属性筛选度量标准</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#信息增益率"><span class="toc-nav-text">信息增益率</span></a></li><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#基尼指数-Gini-Index"><span class="toc-nav-text">基尼指数(Gini Index)</span></a></li></ol></li></ol></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#剪枝处理-Pruning"><span class="toc-nav-text">剪枝处理(Pruning)</span></a></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#连续值处理"><span class="toc-nav-text">连续值处理</span></a></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#缺失值处理"><span class="toc-nav-text">缺失值处理</span></a></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#不同代价属性的处理"><span class="toc-nav-text">不同代价属性的处理</span></a></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#延申"><span class="toc-nav-text">延申</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-1"><a class="toc-nav-link" href="#七、人工神经网络"><span class="toc-nav-text">七、人工神经网络</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#MP模型"><span class="toc-nav-text">MP模型</span></a></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#感知器"><span class="toc-nav-text">感知器</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#前馈神经网络"><span class="toc-nav-text">前馈神经网络</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#反向传播-BP-算法"><span class="toc-nav-text">反向传播(BP)算法</span></a></li></ol></li></ol></li><li class="toc-nav-item toc-nav-level-1"><a class="toc-nav-link" href="#八、聚类"><span class="toc-nav-text">八、聚类</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#性能度量-2"><span class="toc-nav-text">性能度量</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#外部指标"><span class="toc-nav-text">外部指标</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#内部指标"><span class="toc-nav-text">内部指标</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#聚类方法"><span class="toc-nav-text">聚类方法</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#K均值-K-Means"><span class="toc-nav-text">K均值(K-Means)</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#K-means"><span class="toc-nav-text">K-means++</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#Kernel-K-means"><span class="toc-nav-text">Kernel K-means</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#ISODATA"><span class="toc-nav-text">ISODATA</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#层次聚类算法"><span class="toc-nav-text">层次聚类算法</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#AGNES算法（Agglomerative-Nesting）"><span class="toc-nav-text">AGNES算法（Agglomerative Nesting）</span></a></li></ol></li></ol></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#聚类应用"><span class="toc-nav-text">聚类应用</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-1"><a class="toc-nav-link" href="#九、数据降维"><span class="toc-nav-text">九、数据降维</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#数据维度"><span class="toc-nav-text">数据维度</span></a></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#数据降维"><span class="toc-nav-text">数据降维</span></a></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#降维方法"><span class="toc-nav-text">降维方法</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#主成分分析-PCA"><span class="toc-nav-text">主成分分析 PCA</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#应用"><span class="toc-nav-text">应用</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-5"><a class="toc-nav-link" href="#利用PCA处理高维数据"><span class="toc-nav-text">利用PCA处理高维数据</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#LDA"><span class="toc-nav-text">LDA</span></a></li><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#PCA和LDA区别"><span class="toc-nav-text">PCA和LDA区别</span></a></li><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#Kernel-PCA"><span class="toc-nav-text">Kernel PCA</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#等距映射"><span class="toc-nav-text">等距映射</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#局部线性嵌入"><span class="toc-nav-text">局部线性嵌入</span></a></li></ol></li></ol></li><li class="toc-nav-item toc-nav-level-1"><a class="toc-nav-link" href="#十、集成学习"><span class="toc-nav-text">十、集成学习</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#基本概念-2"><span class="toc-nav-text">基本概念</span></a></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#串行化方法-Boosting"><span class="toc-nav-text">串行化方法 Boosting</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#Adaboost算法"><span class="toc-nav-text">Adaboost算法</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#Boosting算法特点"><span class="toc-nav-text">Boosting算法特点</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#基学习器能学习特定的数据分布"><span class="toc-nav-text">基学习器能学习特定的数据分布</span></a></li><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#特点总结"><span class="toc-nav-text">特点总结</span></a></li></ol></li></ol></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#并行化方法-Bagging算法"><span class="toc-nav-text">并行化方法 Bagging算法</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#基本思想"><span class="toc-nav-text">基本思想</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#特点"><span class="toc-nav-text">特点</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#并行化方法-随机森林算法"><span class="toc-nav-text">并行化方法 随机森林算法</span></a></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#结合策略"><span class="toc-nav-text">结合策略</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#平均法"><span class="toc-nav-text">平均法</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#简单平均法"><span class="toc-nav-text">简单平均法</span></a></li><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#加权平均法"><span class="toc-nav-text">加权平均法</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#结合策略—投票法"><span class="toc-nav-text">结合策略—投票法</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#结合策略—学习法"><span class="toc-nav-text">结合策略—学习法</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#多样性"><span class="toc-nav-text">多样性</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#多样性度量"><span class="toc-nav-text">多样性度量</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#多样性增强"><span class="toc-nav-text">多样性增强</span></a></li></ol></li></ol></li><li class="toc-nav-item toc-nav-level-1"><a class="toc-nav-link" href="#十一、半监督学习"><span class="toc-nav-text">十一、半监督学习</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#基本假设"><span class="toc-nav-text">基本假设</span></a></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#自学习方法-Self-Training-Methods"><span class="toc-nav-text">自学习方法 (Self-Training Methods)</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#典型代表-最近邻自学习算法"><span class="toc-nav-text">典型代表-最近邻自学习算法</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#典型代表-半监督SVM"><span class="toc-nav-text">典型代表-半监督SVM</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#直推式支持向量机T-SVM"><span class="toc-nav-text">直推式支持向量机T-SVM</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#典型代表-半监督聚类"><span class="toc-nav-text">典型代表-半监督聚类</span></a></li></ol></li></ol></li></ol>
            
          
          </div>
        </aside>
      
    

                
            <!-- Sidebar Container -->
            <div class="
                col-lg-8 col-lg-offset-2
                col-md-10 col-md-offset-1
                sidebar-container">

                <!-- Featured Tags -->
                
                <section>
                    <!-- no hr -->
                    <h5><a href="/tags/">FEATURED TAGS</a></h5>
                    <div class="tags">
                       
                          <a class="tag" href="/tags/#DeepLearning" title="DeepLearning">DeepLearning</a>
                        
                    </div>
                </section>
                

                <!-- Friends Blog -->
                
                <hr>
                <h5>FRIENDS</h5>
                <ul class="list-inline">

                    
                        <li><a href="https://roife.github.io" target="_blank">Roife</a></li>
                    
                        <li><a href="https://coekjan.cn" target="_blank">Coekjan</a></li>
                    
                        <li><a href="https://www.dusign.net/" target="_blank">Dusign</a></li>
                    
                </ul>
                
            </div>
        </div>
    </div>
</article>
<script src="https://utteranc.es/client.js"
        repo="buaadreamer/buaadreamer.github.io"
        issue-term="pathname"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script>



<!-- async load function -->
<script>
    function async(u, c) {
      var d = document, t = 'script',
          o = d.createElement(t),
          s = d.getElementsByTagName(t)[0];
      o.src = u;
      if (c) { o.addEventListener('load', function (e) { c(null, e); }, false); }
      s.parentNode.insertBefore(o, s);
    }
</script>
<!-- anchor-js, Doc:http://bryanbraun.github.io/anchorjs/ -->
<script>
    async("https://cdn.bootcss.com/anchor-js/1.1.1/anchor.min.js",function(){
        anchors.options = {
          visible: 'hover',
          placement: 'left',
          icon: 'ℬ'
        };
        anchors.add().remove('.intro-header h1').remove('.subheading').remove('.sidebar-container h5');
    })
</script>


<style  type="text/css">
    /* place left on bigger screen */
    @media all and (min-width: 800px) {
        .anchorjs-link{
            position: absolute;
            left: -0.75em;
            font-size: 1.1em;
            margin-top : -0.1em;
        }
    }
</style>



    <!-- Footer -->
    <!-- Footer -->
<footer>
    <div class="container">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                <ul class="list-inline text-center">

                
                    <li>
                        <a target="_blank"  href="https://github.com/BUAADreamer">
                            <span class="fa-stack fa-lg">
                                <i class="fa fa-circle fa-stack-2x"></i>
                                <i class="fa fa-github fa-stack-1x fa-inverse"></i>
                            </span>
                        </a>
                    </li>
                

                

                

                

                

                

                

                </ul>
                <p class="copyright text-muted">
                    Copyright &copy; BUAADreamer 2024 
                    <br>
                    Powered by 
                    <a href="https://github.com/dusign/hexo-theme-snail" target="_blank" rel="noopener">
                        <i>hexo-theme-snail</i>
                    </a> | 
                    <iframe name="star" style="margin-left: 2px; margin-bottom:-5px;" frameborder="0" scrolling="0"
                        width="100px" height="20px"
                        src="https://ghbtns.com/github-btn.html?user=dusign&repo=hexo-theme-snail&type=star&count=true">
                    </iframe>
                </p>
            </div>
        </div>
    </div>

</footer>

<!-- jQuery -->

<script src="/js/jquery.min.js"></script>


<!-- Bootstrap Core JavaScript -->

<script src="/js/bootstrap.min.js"></script>


<!-- Custom Theme JavaScript -->

<script src="/js/hux-blog.min.js"></script>


<!-- Search -->

<script src="/js/search.js"></script>


<!-- async load function -->
<script>
    function async(u, c) {
      var d = document, t = 'script',
          o = d.createElement(t),
          s = d.getElementsByTagName(t)[0];
      o.src = u;
      if (c) { o.addEventListener('load', function (e) { c(null, e); }, false); }
      s.parentNode.insertBefore(o, s);
    }
</script>


<!-- jquery.tagcloud.js -->
<script>
    // only load tagcloud.js in tag.html
    if($('#tag_cloud').length !== 0){
        async("https://BUAADreamer.top/js/jquery.tagcloud.js",function(){
            $.fn.tagcloud.defaults = {
                //size: {start: 1, end: 1, unit: 'em'},
                color: {start: '#bbbbee', end: '#0085a1'},
            };
            $('#tag_cloud a').tagcloud();
        })
    }
</script>

<!--fastClick.js -->
<script>
    async("https://cdn.bootcss.com/fastclick/1.0.6/fastclick.min.js", function(){
        var $nav = document.querySelector("nav");
        if($nav) FastClick.attach($nav);
    })
</script>


<!-- Google Analytics -->


<script>
    // dynamic User by Hux
    var _gaId = 'UA-XXXXXXXX-X';
    var _gaDomain = 'yoursite';

    // Originial
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

    ga('create', _gaId, _gaDomain);
    ga('send', 'pageview');
</script>




<!-- Baidu Tongji -->


<!-- Search -->

    <script type="text/javascript">      
        var search_path = "search.xml";
        if (search_path.length == 0) {
            search_path = "search.xml";
        }
    var path = "/" + search_path;
    searchFunc(path, 'local-search-input', 'local-search-result');
    </script>


<!-- busuanzi -->
<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>


  <script src='https://unpkg.com/mermaid@7.1.2/dist/mermaid.min.js'></script>
  <script>
    if (window.mermaid) {
      mermaid.initialize({theme: 'forest'});
    }
  </script>








	<a id="rocket" href="#top" class=""></a>
	<script type="text/javascript" src="/js/totop.js?v=1.0.0" async=""></script>
    <script type="text/javascript" src="/js/toc.js?v=1.0.0" async=""></script>

    
        <!-- background effects line -->
        

        
            <script type="text/javascript" src="/js/mouse-click.js" content='[&quot;🌱&quot;,&quot;just do it&quot;,&quot;🍀&quot;]' color='[&quot;rgb(121,93,179)&quot; ,&quot;rgb(76,180,231)&quot; ,&quot;rgb(184,90,154)&quot;]'></script>
        

        <!-- background effects end -->
    

    <!--<script size="50" alpha='0.3' zIndex="-999" src="/js/ribbonStatic.js"></script>-->
    
        <script src="/js/ribbonDynamic.js"></script>
    
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</body>

</html>
